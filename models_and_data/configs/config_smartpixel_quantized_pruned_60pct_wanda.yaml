pruning_parameters:
  disable_pruning_for_layers: # Disable pruning for these layers, even if enable_pruning is true
    -
  enable_pruning: true
  M: null
  N: null
  pruning_method: wanda
  threshold_decay: 0.
  sparsity: 0.90
  t_delta: 5
  t_start_collecting: 50000
quantization_parameters:
  default_integer_bits: 0.
  default_fractional_bits: 7.
  enable_quantization: true
  hgq_gamma: 0.0003
  layer_specific:
    sep_conv.depthwise:
      weight:
        integer_bits: 0
        fractional_bits: 3
      bias: # Not used, bias only in pointwise
        integer_bits: 0
        fractional_bits: 3
    sep_conv.pointwise:
      weight:
        integer_bits: 0
        fractional_bits: 3
      bias:
        integer_bits: 0
        fractional_bits: 3
    conv:
      weight:
        integer_bits: 0
        fractional_bits: 3
      bias:
        integer_bits: 0
        fractional_bits: 3
    tanh:
      bits: 4
    tanh_1:
      bits: 4
  use_high_granularity_quantization: false
  use_real_tanh: false
  use_symmetric_quantization: false
training_parameters:
  epochs: 500
  fine_tuning_epochs: 100
  pretraining_epochs: 20
  pruning_first: false # Quantize first, then prune. Seems to work better
  rewind: never
  rounds: 1
  save_weights_epoch: -1
  threshold_decay: 0.
batch_size: 5000
cosine_tmax: 420
gamma: 0.1
l2_decay: 0.01
label_smoothing: 0.0
lr: 0.001
lr_schedule: multistep
milestones: # Never drop
- 1000
- 1000
momentum: 0.9
optimizer: adam
plot_frequency: 100
