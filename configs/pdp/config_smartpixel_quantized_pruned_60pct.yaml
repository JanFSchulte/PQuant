pruning_parameters:
  enable_pruning: true
  epsilon: 0.005
  pruning_method: pdp
  sparsity: 0.4
  temperature: 1.0e-05
  structured_pruning: false
quantization_parameters:
  default_integer_bits: 0.
  default_fractional_bits: 7.
  enable_quantization: true
  hgq_gamma: 0.0003
  layer_specific:
    sep_conv.depthwise:
      integer_bits: 0
      fractional_bits: 3
    sep_conv.pointwise:
      integer_bits: 0
      fractional_bits: 3
    conv:
      integer_bits: 0
      fractional_bits: 3
  use_high_granularity_quantization: false
training_parameters:
  batch_size: 256 #Dataset has inbuilt batch size of 5000
  cosine_tmax: 500
  epochs: 500
  fine_tuning_epochs: 100
  gamma: 0.1
  l2_decay: 0.01
  label_smoothing: 0.0
  lr: 0.001
  lr_schedule: cosine
  milestones: # Never drop
  - 1000
  - 1000
  momentum: 0.9
  optimizer: adam
  plot_frequency: 100
  pretraining_epochs: 20
  pruning_first: false # Quantize first, then prune. Seems to work better
  rewind: never
  rounds: 1
  save_weights_epoch: -1
  threshold_decay: 0.
