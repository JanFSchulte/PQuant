{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5676e100-c255-4871-b167-01a788309112",
   "metadata": {},
   "source": [
    "## In this tutorial we create a CNN and dataloaders, and train / prune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf8b22-eca4-48eb-8343-1dc144ee5737",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27197caf-85a2-48b7-af76-a5ff943408ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\" # Needs to be set, some pruning layers as well as the quantizers are Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3eff9-178f-432c-95d2-392f89181e96",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's define a ResNet20 using a slighty modified version of https://github.com/akamaster/pytorch_resnet_cifar10/blob/master/resnet.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5a763-a029-495d-a03a-390048d749f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weights_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = LambdaLayer(lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.apply(_weights_init)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def resnet20():\n",
    "    return ResNet(BasicBlock, [3, 3, 3])\n",
    "\n",
    "\n",
    "model = resnet20()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd71c9-86b2-4911-aa71-bd18b1e75aa1",
   "metadata": {},
   "source": [
    "## Add pruning and quantization\n",
    "To add pruning and quantization, we need a config file that defines how to do that. Let's load a config file from pquant/configs/configs_pdp.yaml. The training function we use later will add the pruning layers and quantized activations automatically using this config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec145f1-502c-4fd0-84ed-e87b84a27374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pquant.core.utils import get_default_config\n",
    "\n",
    "# pruning_methods: \"autosparse, cl, cs, dst, pdp, wanda\"\n",
    "pruning_method = \"pdp\"\n",
    "config = get_default_config(pruning_method)\n",
    "# Set target sparsity to 50%. This parameter exists only for some pruning methods\n",
    "config[\"pruning_parameters\"][\"sparsity\"] = 0.5\n",
    "# Check what's inside the config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef3115-2f3d-43e1-a199-4a19d667f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace layers with compressed layers\n",
    "from pquant.core.compressed_layers import add_pruning_and_quantization\n",
    "model = add_pruning_and_quantization(model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7241c-0ebd-492c-b4d4-3b269e5afc4d",
   "metadata": {},
   "source": [
    "## Pruning and quantization in the config\n",
    "From the config we see that we are using the PDP pruning method, unstructured version. We aim for 50% weights pruned (sparsity 0.4), and we quantize the model to 8 bits (1 bit goes to sign). \n",
    "By default, all convolutional and linear layers, as well as activations will be quantized using the default values ```default_integer_bits``` and ```default_fractional_bits```. Similarly, by default all convolutional and linear layers will be pruned.\n",
    "\n",
    "We can disable pruning and/or quantization by setting the enable_pruning / enable_quantization to False. To change quantization bits for a specific layer, add the layers name to the list found in ```layer_specific```, followed by number of bits. To disable pruning for a single layer, add its name to the ```disable_pruning_for_layers``` list. \n",
    "\n",
    "We'll show later how to create a custom quantization / pruning config file from an existing config for a given model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf6ec9-9fb3-4f56-bffc-8676341cde32",
   "metadata": {},
   "source": [
    "## About the different epochs\n",
    "\n",
    "The config defines 20 ```pretraining_epochs```, 100 ```epochs``` and 20 ```fine_tuning_epochs```. What happens during each of these training steps is algorithm specific. \n",
    "\n",
    "In PDP, the pretraining phase consists of training without pruning, followed by calculation of layerwise pruning budgets. After pretraining is finished and the layerwise pruning budgets have been calculated, the training with pruning begins. The mask during this training is a soft mask, consisting of values ranging between (and including) 0 and 1. \n",
    "\n",
    "The fine-tuning step in PDP is optional (not mentioned in the original paper), and during it the mask is fixed and rounded to 0s and 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251515c3-00ac-4110-b8d8-a8c9100f6e6b",
   "metadata": {},
   "source": [
    "## Create data set\n",
    "#### Let's create the data loader and the training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c24bff-9937-4670-8cff-022ddc7a0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def get_cifar10_data(batch_size):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), \n",
    "                                          transforms.ToTensor(), normalize])\n",
    "    test_transform = transforms.Compose([transforms.ToTensor(), normalize])  \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "    valset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=test_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "from quantizers.fixed_point.fixed_point_ops import get_fixed_quantizer\n",
    "# Set up input quantizer\n",
    "quantizer = get_fixed_quantizer(overflow_mode=\"SAT\")\n",
    "\n",
    "\n",
    "def train_resnet(model, trainloader, device, loss_func, epoch, optimizer, scheduler, *args, **kwargs):\n",
    "    for data in trainloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(1.), f=torch.tensor(6.)) # 8 bits input quantization\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        losses = get_model_losses(model, torch.tensor(0.).to(device))\n",
    "        loss += losses\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch += 1\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "from pquant.core.compressed_layers import get_layer_keep_ratio, get_model_losses\n",
    "\n",
    "def validate_resnet(model, testloader, device, loss_func, epoch, *args, **kwargs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(1.), f=torch.tensor(6.)) # 8 bits input quantization\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        ratio = get_layer_keep_ratio(model)\n",
    "        print(f'Accuracy: {100 * correct / total:.2f}%, remaining_weights: {ratio * 100:.2f}%')\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader, val_loader = get_cifar10_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3794868-9103-47cd-9baf-4cbcd703115a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d28865b-afdb-4773-be30-717486d9786a",
   "metadata": {},
   "source": [
    "## Create loss function, scheduler and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88af88-ef7a-4d30-8cff-f39225d5a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850c23a-2abc-4904-9c69-859492b450a8",
   "metadata": {},
   "source": [
    "## Train model\n",
    "Training time. We use the train_compressed_model function from pquant to train. We need to provide some parameters such as training and validation functions, their input parameters, the model and the config file. The function automatically adds pruning layers and replaces activations with a quantized variant, trains the model, and removes the pruning layers after training is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e8da1-c13c-41f4-9d96-07427b66669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pquant.core.train import iterative_train\n",
    "\"\"\"\n",
    "Inputs to train_resnet we defined previously are:\n",
    "          model, trainloader, device, loss_func, epoch, optimizer, scheduler, **kwargs\n",
    "\"\"\"\n",
    "\n",
    "trained_model = iterative_train(model = model, \n",
    "                                config = config, \n",
    "                                train_func = train_resnet, \n",
    "                                valid_func = validate_resnet, \n",
    "                                trainloader = train_loader, \n",
    "                                testloader = val_loader, \n",
    "                                device = device, \n",
    "                                loss_func = loss_function,\n",
    "                                optimizer = optimizer, \n",
    "                                scheduler = scheduler\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994c123-92ad-4815-9217-c2dca2f80a6b",
   "metadata": {},
   "source": [
    "We see from that with PDP, the number of weights goes down during training, until it reaches the target sparsity (~50% remaining weights). The algorithm actually increases the sparsity linearly, but since the mask used before fine-tuning is a soft mask, the sparsity that is printed before fine-tuning seems to increase somewhat noisily. \n",
    "During fine-tuning the mask is fixed, and turned into a mask of 0s and 1s by a simple rounding operation. \n",
    "\n",
    "In the original paper for PDP there was no fine-tuning after the creation of the final hard mask. We have added fine-tuning here as an option that can be turned off by simply setting ```fine_tuning_epochs``` to 0 in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678209db-91ad-4090-9480-76f6061fdf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove compression layers\n",
    "from pquant.core.compressed_layers import remove_pruning_from_model\n",
    "model = remove_pruning_from_model(trained_model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f0f07-4f01-40d8-a162-8778ec310b9a",
   "metadata": {},
   "source": [
    "## Custom config from existing config\n",
    "Using the ```pquant/configs/config_pdp.yaml``` as base, let's customize the quantization and pruning scheme. \n",
    "\n",
    "The function we use will go through the model's layers and do the following: \n",
    "\n",
    "Quantization:\n",
    "\n",
    "        1. Looks for the names of convolutional and linear layers, as well as names of the activations (layer type activations and functional types)\n",
    "        2. Adds the name of the layer to the layer_specific list, along with a default quantization scheme of 0 and 7 for weight and bias (if bias is not None)\n",
    "\n",
    "Pruning: \n",
    "\n",
    "        1. Looks for convolutional and linear layers and adds their name to the disable_pruning_for_layers list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc068a-4e3b-4e61-8c23-2b8ba37adcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base config\n",
    "pruning_method = \"pdp\"\n",
    "config = get_default_config(pruning_method)\n",
    "model = resnet20()\n",
    "\n",
    "\n",
    "from pquant.core.compressed_layers import add_default_layer_quantization_pruning_to_config\n",
    "config = add_default_layer_quantization_pruning_to_config(config, model)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdfe45b-1432-4cec-a3dd-b46f837beb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save config\n",
    "from pquant.core.utils import write_config_to_yaml\n",
    "write_config_to_yaml(config, \"prune_quantize_example.yaml\", sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb7669-ea71-4e86-b128-e313e83fbc96",
   "metadata": {},
   "source": [
    "Now that we have the custom config, it is up to us to modify the quantization bits for each layer that will not use the default value. If a layer uses the default value it can be removed from the ```layer_specific``` list.\n",
    "\n",
    "For pruning, leave those layers to the ```disable_pruning_for_layers``` list that will not be pruned, others need to be removed from the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742e0b00-220d-47a7-9774-af694f0afe1f",
   "metadata": {},
   "source": [
    "## About replacing layers and activations\n",
    "Layers that can currently be compressed: ```nn.Conv1d, nn.Conv2d, nn.Linear```.\n",
    "\n",
    "Activations that can currently be automatically be replaced with a quantized variant: ```nn.ReLU, nn.Tanh```. The activations are replaced by a quantized variant, found in ```pquant.core.activations_quantizer.py```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baece7d0-e23a-4e31-ba1e-1cf61cb05356",
   "metadata": {},
   "source": [
    "## More about activations\n",
    "If using layer type activations, note that if you want to keep the fine-grained control over the quantization of the activation, reusing an activation layer can cause problems, as all activations will use the quantization bits set for that particular layer. To avoid this, use a separate ```nn.Tanh``` / ```nn.ReLU``` for each activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa0c9a3-ac59-4aa9-8b54-909f690e6c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aadafd9-f0df-473c-84de-2c2a72148ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473019d7-502c-44b5-bf06-612b857756c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
