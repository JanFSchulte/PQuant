{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5676e100-c255-4871-b167-01a788309112",
   "metadata": {},
   "source": [
    "## In this tutorial we create a CNN and dataloaders, and train / prune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27197caf-85a2-48b7-af76-a5ff943408ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\" # Needs to be set, some pruning layers as well as the quantizers are Keras\n",
    "import keras\n",
    "keras.config.set_backend(\"torch\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "keras.backend.set_image_data_format(\"channels_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e520e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet_paca_pruned.pth\n",
      "pquant\n",
      "data\n",
      "smartpixels\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(\"/home/das214/PQuant/mdmm_dev/src\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for f in os.listdir(os.getcwd()):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ea5a763-a029-495d-a03a-390048d749f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd71c9-86b2-4911-aa71-bd18b1e75aa1",
   "metadata": {},
   "source": [
    "## Add pruning and quantization\n",
    "Begin prunning with MDMM pruning with Unstructured Sparsity metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec145f1-502c-4fd0-84ed-e87b84a27374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "batch_size": 128,
       "cosine_tmax": 200,
       "gamma": 0.1,
       "l2_decay": 0.0001,
       "label_smoothing": 0,
       "lr": 0.001,
       "lr_schedule": "multistep",
       "milestones": [
        75,
        120
       ],
       "momentum": 0.9,
       "optimizer": "sgd",
       "plot_frequency": 100,
       "pruning_parameters": {
        "constraint_type": "Equality",
        "damping": 1,
        "disable_pruning_for_layers": [
         null
        ],
        "enable_pruning": true,
        "epsilon": 0.001,
        "l0_mode": "coarse",
        "metric_type": "UnstructuredSparsity",
        "pruning_method": "mdmm",
        "rf": 1,
        "scale": 50,
        "target_sparsity": 0.9,
        "target_value": 0,
        "use_grad": false
       },
       "quantization_parameters": {
        "default_fractional_bits": 7,
        "default_integer_bits": 0,
        "enable_quantization": false,
        "hgq_gamma": 0.0003,
        "hgq_heterogeneous": true,
        "layer_specific": [],
        "use_high_granularity_quantization": false,
        "use_real_tanh": false,
        "use_symmetric_quantization": false
       },
       "training_parameters": {
        "epochs": 100,
        "fine_tuning_epochs": 30,
        "pretraining_epochs": 0,
        "pruning_first": false,
        "rewind": "never",
        "rounds": 1,
        "save_weights_epoch": -1
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pquant import get_default_config\n",
    "from IPython.display import JSON\n",
    "\n",
    "pruning_method = \"mdmm\"\n",
    "config = get_default_config(pruning_method)\n",
    "JSON(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3ef3115-2f3d-43e1-a199-4a19d667f796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): CompressedLayerConv2d(\n",
       "    (pruning_layer): <MDMM name=mdmm, built=True>\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_1, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_2, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_3, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_4, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_5, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_6, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_7, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_8, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_9, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_10, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_11, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_12, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_13, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_14, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_15, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_16, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_17, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_18, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_19, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): CompressedLayerLinear(\n",
       "    (pruning_layer): <MDMM name=mdmm_20, built=True>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace layers with compressed layers\n",
    "from pquant import add_compression_layers\n",
    "input_shape = (256,3,32,32)\n",
    "model = add_compression_layers(model, config, input_shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82dd0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from pquant import get_layer_keep_ratio, get_model_losses\n",
    "from quantizers.fixed_point.fixed_point_ops import get_fixed_quantizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_cifar10_data(batch_size):\n",
    "    normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), \n",
    "                                          transforms.ToTensor(), normalize])\n",
    "    test_transform = transforms.Compose([transforms.ToTensor(), normalize])  \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "    valset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=test_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Set up input quantizer\n",
    "quantizer = get_fixed_quantizer(overflow_mode=\"SAT\")\n",
    "\n",
    "def train_resnet(model, trainloader, device, loss_func,\n",
    "                 epoch, optimizer, scheduler, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    One epoch of training with a live ETA/throughput bar.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(trainloader,\n",
    "              desc=f\"Train ‖ Epoch {epoch}\",\n",
    "              total=len(trainloader),\n",
    "              unit=\"batch\",\n",
    "              dynamic_ncols=True) as pbar:\n",
    "\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.))\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)              # cleaner gradient reset\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            losses = get_model_losses(model, torch.tensor(0.).to(device))\n",
    "            loss += losses\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f} \")\n",
    "        \n",
    "    # ----- Diagnostics on Last mini-batch -----\n",
    "    print(f\"Loss={loss_func(outputs, labels).item():.4f} | Reg={loss.item() - loss_func(outputs, labels).item():.4f}\")\n",
    "\n",
    "def validate_resnet(model, testloader, device, loss_func, epoch, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Validation with progress bar and accuracy summary.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(testloader,\n",
    "                  desc=f\"Val   ‖ Epoch {epoch}\",\n",
    "                  total=len(testloader),\n",
    "                  unit=\"batch\",\n",
    "                  dynamic_ncols=True) as pbar:\n",
    "\n",
    "            for inputs, labels in pbar:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.))\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "                running_acc = 100. * correct / total\n",
    "                pbar.set_postfix(acc=f\"{running_acc:.2f}%\")\n",
    "\n",
    "    ratio = get_layer_keep_ratio(model)\n",
    "    print(f\"Accuracy: {correct/total*100:.2f}% | Remaining weights: {ratio*100:.2f}% \\n\")\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader, val_loader = get_cifar10_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cff4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e20af1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 0: 100%|██████████| 196/196 [00:08<00:00, 24.06batch/s, loss=12.3773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.4446 | Reg=10.9328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 0: 100%|██████████| 196/196 [00:05<00:00, 37.40batch/s, acc=47.94%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.94% | Remaining weights: 96.47% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 1: 100%|██████████| 196/196 [00:07<00:00, 25.55batch/s, loss=23.6083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.6338 | Reg=21.9745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 1: 100%|██████████| 196/196 [00:05<00:00, 37.89batch/s, acc=46.52%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.52% | Remaining weights: 95.72% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 2: 100%|██████████| 196/196 [00:07<00:00, 25.92batch/s, loss=23.8973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0892 | Reg=22.8081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 2: 100%|██████████| 196/196 [00:05<00:00, 38.13batch/s, acc=58.02%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.02% | Remaining weights: 94.27% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 3: 100%|██████████| 196/196 [00:07<00:00, 25.27batch/s, loss=25.7729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.4191 | Reg=24.3539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 3: 100%|██████████| 196/196 [00:05<00:00, 37.28batch/s, acc=47.09%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.09% | Remaining weights: 93.09% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 4: 100%|██████████| 196/196 [00:07<00:00, 25.25batch/s, loss=21.7308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1449 | Reg=20.5859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 4: 100%|██████████| 196/196 [00:05<00:00, 37.94batch/s, acc=62.11%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.11% | Remaining weights: 90.59% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 5: 100%|██████████| 196/196 [00:07<00:00, 25.10batch/s, loss=23.3598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.3947 | Reg=21.9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 5: 100%|██████████| 196/196 [00:05<00:00, 37.71batch/s, acc=32.79%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 32.79% | Remaining weights: 89.45% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 6: 100%|██████████| 196/196 [00:07<00:00, 24.64batch/s, loss=18.9369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0513 | Reg=17.8856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 6: 100%|██████████| 196/196 [00:05<00:00, 37.67batch/s, acc=63.11%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.11% | Remaining weights: 86.23% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 7: 100%|██████████| 196/196 [00:08<00:00, 24.36batch/s, loss=20.5161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.4461 | Reg=19.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 7: 100%|██████████| 196/196 [00:05<00:00, 37.84batch/s, acc=41.66%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 41.66% | Remaining weights: 85.29% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 8: 100%|██████████| 196/196 [00:07<00:00, 24.84batch/s, loss=16.4888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9126 | Reg=15.5762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 8: 100%|██████████| 196/196 [00:05<00:00, 38.22batch/s, acc=65.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.06% | Remaining weights: 81.50% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 9: 100%|██████████| 196/196 [00:07<00:00, 24.71batch/s, loss=18.0069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.3131 | Reg=16.6939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 9: 100%|██████████| 196/196 [00:05<00:00, 38.31batch/s, acc=45.66%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.66% | Remaining weights: 80.77% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 10: 100%|██████████| 196/196 [00:07<00:00, 25.02batch/s, loss=14.5856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9198 | Reg=13.6658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 10: 100%|██████████| 196/196 [00:05<00:00, 38.03batch/s, acc=63.76%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.76% | Remaining weights: 76.61% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 11: 100%|██████████| 196/196 [00:07<00:00, 25.36batch/s, loss=15.7773]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2031 | Reg=14.5742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 11: 100%|██████████| 196/196 [00:05<00:00, 38.56batch/s, acc=38.01%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 38.01% | Remaining weights: 75.95% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 12: 100%|██████████| 196/196 [00:07<00:00, 25.40batch/s, loss=13.1531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0850 | Reg=12.0681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 12: 100%|██████████| 196/196 [00:05<00:00, 38.47batch/s, acc=63.52%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.52% | Remaining weights: 71.74% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 13: 100%|██████████| 196/196 [00:07<00:00, 25.79batch/s, loss=13.8810]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0718 | Reg=12.8092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 13: 100%|██████████| 196/196 [00:05<00:00, 38.18batch/s, acc=47.69%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.69% | Remaining weights: 71.18% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 14: 100%|██████████| 196/196 [00:07<00:00, 25.35batch/s, loss=11.8679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2313 | Reg=10.6366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 14: 100%|██████████| 196/196 [00:05<00:00, 38.81batch/s, acc=64.28%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.28% | Remaining weights: 66.97% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 15: 100%|██████████| 196/196 [00:07<00:00, 25.44batch/s, loss=12.4398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2508 | Reg=11.1890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 15: 100%|██████████| 196/196 [00:05<00:00, 38.62batch/s, acc=34.99%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 34.99% | Remaining weights: 66.45% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 16: 100%|██████████| 196/196 [00:07<00:00, 25.17batch/s, loss=10.3955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0073 | Reg=9.3882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 16: 100%|██████████| 196/196 [00:05<00:00, 38.09batch/s, acc=64.03%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.03% | Remaining weights: 62.45% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 17: 100%|██████████| 196/196 [00:07<00:00, 25.16batch/s, loss=11.1640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.5261 | Reg=9.6379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 17: 100%|██████████| 196/196 [00:05<00:00, 38.25batch/s, acc=45.42%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.42% | Remaining weights: 61.75% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 18: 100%|██████████| 196/196 [00:07<00:00, 25.27batch/s, loss=9.3397] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0561 | Reg=8.2836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 18: 100%|██████████| 196/196 [00:05<00:00, 37.99batch/s, acc=64.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.61% | Remaining weights: 58.08% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 19: 100%|██████████| 196/196 [00:07<00:00, 25.43batch/s, loss=9.6715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2358 | Reg=8.4357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 19: 100%|██████████| 196/196 [00:05<00:00, 38.08batch/s, acc=51.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.17% | Remaining weights: 57.36% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 20: 100%|██████████| 196/196 [00:07<00:00, 25.79batch/s, loss=8.4744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1549 | Reg=7.3195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 20: 100%|██████████| 196/196 [00:05<00:00, 37.97batch/s, acc=63.42%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.42% | Remaining weights: 54.04% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 21: 100%|██████████| 196/196 [00:07<00:00, 25.06batch/s, loss=8.4750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2509 | Reg=7.2240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 21: 100%|██████████| 196/196 [00:05<00:00, 37.48batch/s, acc=62.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.06% | Remaining weights: 53.05% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 22: 100%|██████████| 196/196 [00:07<00:00, 25.36batch/s, loss=7.4258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9128 | Reg=6.5130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 22: 100%|██████████| 196/196 [00:05<00:00, 38.57batch/s, acc=61.03%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.03% | Remaining weights: 50.33% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 23: 100%|██████████| 196/196 [00:07<00:00, 25.07batch/s, loss=7.2038]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9646 | Reg=6.2392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 23: 100%|██████████| 196/196 [00:05<00:00, 38.48batch/s, acc=61.60%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.60% | Remaining weights: 49.11% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 24: 100%|██████████| 196/196 [00:07<00:00, 24.73batch/s, loss=6.8300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0032 | Reg=5.8268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 24: 100%|██████████| 196/196 [00:05<00:00, 38.58batch/s, acc=61.23%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.23% | Remaining weights: 46.99% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 25: 100%|██████████| 196/196 [00:07<00:00, 25.43batch/s, loss=6.3334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9891 | Reg=5.3443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 25: 100%|██████████| 196/196 [00:05<00:00, 38.86batch/s, acc=60.52%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.52% | Remaining weights: 45.45% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 26: 100%|██████████| 196/196 [00:07<00:00, 25.44batch/s, loss=6.3911]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2415 | Reg=5.1496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 26: 100%|██████████| 196/196 [00:05<00:00, 38.53batch/s, acc=60.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.85% | Remaining weights: 43.76% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 27: 100%|██████████| 196/196 [00:07<00:00, 25.54batch/s, loss=5.5610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9881 | Reg=4.5729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 27: 100%|██████████| 196/196 [00:05<00:00, 38.57batch/s, acc=63.13%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.13% | Remaining weights: 42.10% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 28: 100%|██████████| 196/196 [00:07<00:00, 25.38batch/s, loss=5.7857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2331 | Reg=4.5526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 28: 100%|██████████| 196/196 [00:05<00:00, 38.42batch/s, acc=55.05%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.05% | Remaining weights: 40.88% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 29: 100%|██████████| 196/196 [00:07<00:00, 25.03batch/s, loss=4.7685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8737 | Reg=3.8948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 29: 100%|██████████| 196/196 [00:05<00:00, 38.09batch/s, acc=66.81%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.81% | Remaining weights: 39.18% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 30: 100%|██████████| 196/196 [00:07<00:00, 25.25batch/s, loss=4.9883]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9470 | Reg=4.0413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 30: 100%|██████████| 196/196 [00:05<00:00, 38.70batch/s, acc=58.34%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.34% | Remaining weights: 38.32% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 31: 100%|██████████| 196/196 [00:07<00:00, 25.38batch/s, loss=4.1744]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7937 | Reg=3.3807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 31: 100%|██████████| 196/196 [00:05<00:00, 38.12batch/s, acc=68.80%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.80% | Remaining weights: 36.58% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 32: 100%|██████████| 196/196 [00:07<00:00, 25.37batch/s, loss=4.9017]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1756 | Reg=3.7261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 32: 100%|██████████| 196/196 [00:05<00:00, 38.97batch/s, acc=59.69%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.69% | Remaining weights: 36.18% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 33: 100%|██████████| 196/196 [00:07<00:00, 25.49batch/s, loss=3.8215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8926 | Reg=2.9289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 33: 100%|██████████| 196/196 [00:05<00:00, 38.39batch/s, acc=71.30%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.30% | Remaining weights: 34.34% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 34: 100%|██████████| 196/196 [00:07<00:00, 25.23batch/s, loss=4.5809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1829 | Reg=3.3980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 34: 100%|██████████| 196/196 [00:05<00:00, 38.48batch/s, acc=61.48%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.48% | Remaining weights: 34.25% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 35: 100%|██████████| 196/196 [00:07<00:00, 25.87batch/s, loss=3.5990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0569 | Reg=2.5421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 35: 100%|██████████| 196/196 [00:05<00:00, 38.79batch/s, acc=72.35%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.35% | Remaining weights: 32.34% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 36: 100%|██████████| 196/196 [00:07<00:00, 25.56batch/s, loss=4.2675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1446 | Reg=3.1229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 36: 100%|██████████| 196/196 [00:05<00:00, 38.41batch/s, acc=59.56%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.56% | Remaining weights: 32.45% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 37: 100%|██████████| 196/196 [00:07<00:00, 25.73batch/s, loss=3.2050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9648 | Reg=2.2402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 37: 100%|██████████| 196/196 [00:05<00:00, 38.42batch/s, acc=75.03%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.03% | Remaining weights: 30.59% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 38: 100%|██████████| 196/196 [00:07<00:00, 25.65batch/s, loss=4.0733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2401 | Reg=2.8331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 38: 100%|██████████| 196/196 [00:05<00:00, 38.36batch/s, acc=54.43%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.43% | Remaining weights: 30.78% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 39: 100%|██████████| 196/196 [00:07<00:00, 26.03batch/s, loss=2.8344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8551 | Reg=1.9793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 39: 100%|██████████| 196/196 [00:05<00:00, 38.73batch/s, acc=75.52%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.52% | Remaining weights: 28.95% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 40: 100%|██████████| 196/196 [00:07<00:00, 26.07batch/s, loss=3.8724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2063 | Reg=2.6662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 40: 100%|██████████| 196/196 [00:05<00:00, 38.56batch/s, acc=54.56%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.56% | Remaining weights: 29.44% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 41: 100%|██████████| 196/196 [00:07<00:00, 26.03batch/s, loss=2.4767]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6962 | Reg=1.7805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 41: 100%|██████████| 196/196 [00:05<00:00, 38.35batch/s, acc=75.68%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.68% | Remaining weights: 27.59% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 42: 100%|██████████| 196/196 [00:07<00:00, 25.75batch/s, loss=3.3010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8848 | Reg=2.4163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 42: 100%|██████████| 196/196 [00:05<00:00, 38.68batch/s, acc=59.66%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.66% | Remaining weights: 28.02% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 43: 100%|██████████| 196/196 [00:07<00:00, 25.83batch/s, loss=2.3353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7402 | Reg=1.5950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 43: 100%|██████████| 196/196 [00:05<00:00, 38.58batch/s, acc=76.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.06% | Remaining weights: 26.25% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 44: 100%|██████████| 196/196 [00:07<00:00, 25.87batch/s, loss=3.2159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9658 | Reg=2.2501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 44: 100%|██████████| 196/196 [00:05<00:00, 38.71batch/s, acc=53.51%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.51% | Remaining weights: 26.85% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 45: 100%|██████████| 196/196 [00:07<00:00, 25.38batch/s, loss=2.2311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7763 | Reg=1.4548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 45: 100%|██████████| 196/196 [00:05<00:00, 38.41batch/s, acc=76.33%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.33% | Remaining weights: 25.18% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 46: 100%|██████████| 196/196 [00:07<00:00, 25.58batch/s, loss=3.3386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1741 | Reg=2.1645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 46: 100%|██████████| 196/196 [00:05<00:00, 38.29batch/s, acc=43.30%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 43.30% | Remaining weights: 25.86% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 47: 100%|██████████| 196/196 [00:07<00:00, 25.23batch/s, loss=2.1457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8147 | Reg=1.3309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 47: 100%|██████████| 196/196 [00:05<00:00, 38.02batch/s, acc=76.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.39% | Remaining weights: 24.13% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 48: 100%|██████████| 196/196 [00:07<00:00, 25.36batch/s, loss=3.1541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0646 | Reg=2.0894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 48: 100%|██████████| 196/196 [00:05<00:00, 38.18batch/s, acc=61.89%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.89% | Remaining weights: 24.99% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 49: 100%|██████████| 196/196 [00:07<00:00, 25.03batch/s, loss=1.9825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7631 | Reg=1.2194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 49: 100%|██████████| 196/196 [00:05<00:00, 38.11batch/s, acc=76.62%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.62% | Remaining weights: 23.28% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 50: 100%|██████████| 196/196 [00:07<00:00, 25.32batch/s, loss=3.0624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1186 | Reg=1.9437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 50: 100%|██████████| 196/196 [00:05<00:00, 38.29batch/s, acc=54.75%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.75% | Remaining weights: 24.04% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 51: 100%|██████████| 196/196 [00:07<00:00, 25.45batch/s, loss=2.0437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9014 | Reg=1.1423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 51: 100%|██████████| 196/196 [00:05<00:00, 38.53batch/s, acc=76.80%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.80% | Remaining weights: 22.50% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 52: 100%|██████████| 196/196 [00:07<00:00, 25.89batch/s, loss=2.8642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1141 | Reg=1.7501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 52: 100%|██████████| 196/196 [00:05<00:00, 38.76batch/s, acc=62.58%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.58% | Remaining weights: 23.18% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 53: 100%|██████████| 196/196 [00:07<00:00, 25.65batch/s, loss=1.9044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8379 | Reg=1.0666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 53: 100%|██████████| 196/196 [00:05<00:00, 37.90batch/s, acc=76.72%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.72% | Remaining weights: 21.81% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 54: 100%|██████████| 196/196 [00:07<00:00, 25.76batch/s, loss=2.8847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1240 | Reg=1.7608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 54: 100%|██████████| 196/196 [00:05<00:00, 38.35batch/s, acc=56.53%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.53% | Remaining weights: 22.58% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 55: 100%|██████████| 196/196 [00:07<00:00, 25.45batch/s, loss=1.8266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8315 | Reg=0.9951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 55: 100%|██████████| 196/196 [00:05<00:00, 38.29batch/s, acc=76.79%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.79% | Remaining weights: 21.14% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 56: 100%|██████████| 196/196 [00:07<00:00, 25.12batch/s, loss=2.7393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1344 | Reg=1.6049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 56: 100%|██████████| 196/196 [00:05<00:00, 38.22batch/s, acc=61.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.54% | Remaining weights: 21.77% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 57: 100%|██████████| 196/196 [00:07<00:00, 25.10batch/s, loss=1.6599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7120 | Reg=0.9479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 57: 100%|██████████| 196/196 [00:05<00:00, 38.36batch/s, acc=76.70%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.70% | Remaining weights: 20.55% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 58: 100%|██████████| 196/196 [00:07<00:00, 25.84batch/s, loss=2.4045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9864 | Reg=1.4181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 58: 100%|██████████| 196/196 [00:05<00:00, 38.15batch/s, acc=57.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.54% | Remaining weights: 21.01% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 59: 100%|██████████| 196/196 [00:07<00:00, 25.61batch/s, loss=1.7127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7721 | Reg=0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 59: 100%|██████████| 196/196 [00:05<00:00, 38.76batch/s, acc=76.21%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.21% | Remaining weights: 20.13% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 60: 100%|██████████| 196/196 [00:07<00:00, 25.28batch/s, loss=2.3458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9672 | Reg=1.3786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 60: 100%|██████████| 196/196 [00:05<00:00, 38.77batch/s, acc=62.91%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.91% | Remaining weights: 20.50% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 61: 100%|██████████| 196/196 [00:07<00:00, 25.27batch/s, loss=1.6244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7318 | Reg=0.8926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 61: 100%|██████████| 196/196 [00:05<00:00, 38.54batch/s, acc=75.48%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.48% | Remaining weights: 19.61% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 62: 100%|██████████| 196/196 [00:07<00:00, 25.46batch/s, loss=2.0155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7104 | Reg=1.3051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 62: 100%|██████████| 196/196 [00:05<00:00, 38.77batch/s, acc=59.29%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.29% | Remaining weights: 19.94% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 63: 100%|██████████| 196/196 [00:07<00:00, 25.26batch/s, loss=1.9216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0045 | Reg=0.9170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 63: 100%|██████████| 196/196 [00:05<00:00, 38.41batch/s, acc=74.20%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.20% | Remaining weights: 19.24% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 64: 100%|██████████| 196/196 [00:07<00:00, 25.39batch/s, loss=2.2756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0977 | Reg=1.1779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 64: 100%|██████████| 196/196 [00:05<00:00, 38.80batch/s, acc=58.15%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.15% | Remaining weights: 19.30% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 65: 100%|██████████| 196/196 [00:07<00:00, 25.39batch/s, loss=1.8048]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9292 | Reg=0.8757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 65: 100%|██████████| 196/196 [00:05<00:00, 38.37batch/s, acc=74.91%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.91% | Remaining weights: 18.90% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 66: 100%|██████████| 196/196 [00:07<00:00, 25.42batch/s, loss=2.2300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1194 | Reg=1.1106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 66: 100%|██████████| 196/196 [00:05<00:00, 38.65batch/s, acc=60.22%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.22% | Remaining weights: 18.84% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 67: 100%|██████████| 196/196 [00:07<00:00, 25.59batch/s, loss=1.5277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6433 | Reg=0.8844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 67: 100%|██████████| 196/196 [00:05<00:00, 38.98batch/s, acc=73.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.54% | Remaining weights: 18.60% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 68: 100%|██████████| 196/196 [00:07<00:00, 25.48batch/s, loss=2.0157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0041 | Reg=1.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 68: 100%|██████████| 196/196 [00:05<00:00, 38.34batch/s, acc=61.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.61% | Remaining weights: 18.37% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 69: 100%|██████████| 196/196 [00:07<00:00, 25.59batch/s, loss=1.8178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8886 | Reg=0.9292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 69: 100%|██████████| 196/196 [00:05<00:00, 38.08batch/s, acc=71.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.54% | Remaining weights: 18.39% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 70: 100%|██████████| 196/196 [00:07<00:00, 25.46batch/s, loss=1.6255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6958 | Reg=0.9297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 70: 100%|██████████| 196/196 [00:05<00:00, 37.93batch/s, acc=67.02%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.02% | Remaining weights: 17.96% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 71: 100%|██████████| 196/196 [00:07<00:00, 25.27batch/s, loss=1.9374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9985 | Reg=0.9389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 71: 100%|██████████| 196/196 [00:05<00:00, 38.04batch/s, acc=68.69%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.69% | Remaining weights: 18.15% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 72: 100%|██████████| 196/196 [00:07<00:00, 25.55batch/s, loss=1.6028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7996 | Reg=0.8032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 72: 100%|██████████| 196/196 [00:05<00:00, 38.06batch/s, acc=72.26%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.26% | Remaining weights: 17.56% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 73: 100%|██████████| 196/196 [00:07<00:00, 25.10batch/s, loss=1.7736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7896 | Reg=0.9840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 73: 100%|██████████| 196/196 [00:05<00:00, 37.94batch/s, acc=69.93%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.93% | Remaining weights: 17.97% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 74: 100%|██████████| 196/196 [00:07<00:00, 25.27batch/s, loss=1.3698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6455 | Reg=0.7243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 74: 100%|██████████| 196/196 [00:05<00:00, 38.46batch/s, acc=68.99%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.99% | Remaining weights: 17.24% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 75: 100%|██████████| 196/196 [00:07<00:00, 25.41batch/s, loss=1.9790]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9851 | Reg=0.9939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 75: 100%|██████████| 196/196 [00:05<00:00, 38.34batch/s, acc=69.57%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.57% | Remaining weights: 17.82% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 76: 100%|██████████| 196/196 [00:07<00:00, 25.57batch/s, loss=1.6130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9531 | Reg=0.6599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 76: 100%|██████████| 196/196 [00:05<00:00, 38.41batch/s, acc=70.46%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.46% | Remaining weights: 16.99% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 77: 100%|██████████| 196/196 [00:07<00:00, 25.43batch/s, loss=1.7068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6883 | Reg=1.0185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 77: 100%|██████████| 196/196 [00:05<00:00, 38.39batch/s, acc=68.48%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.48% | Remaining weights: 17.69% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 78: 100%|██████████| 196/196 [00:07<00:00, 25.32batch/s, loss=1.4754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8600 | Reg=0.6154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 78: 100%|██████████| 196/196 [00:05<00:00, 38.93batch/s, acc=74.11%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.11% | Remaining weights: 16.80% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 79: 100%|██████████| 196/196 [00:07<00:00, 25.44batch/s, loss=2.0501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0407 | Reg=1.0095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 79: 100%|██████████| 196/196 [00:05<00:00, 38.55batch/s, acc=65.20%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.20% | Remaining weights: 17.50% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 80: 100%|██████████| 196/196 [00:07<00:00, 25.67batch/s, loss=1.3416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7558 | Reg=0.5857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 80: 100%|██████████| 196/196 [00:05<00:00, 38.68batch/s, acc=76.84%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.84% | Remaining weights: 16.60% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 81: 100%|██████████| 196/196 [00:07<00:00, 25.38batch/s, loss=2.3743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.3263 | Reg=1.0480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 81: 100%|██████████| 196/196 [00:05<00:00, 38.61batch/s, acc=69.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.82% | Remaining weights: 17.44% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 82: 100%|██████████| 196/196 [00:07<00:00, 25.30batch/s, loss=1.2417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6737 | Reg=0.5679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 82: 100%|██████████| 196/196 [00:05<00:00, 38.73batch/s, acc=78.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.17% | Remaining weights: 16.48% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 83: 100%|██████████| 196/196 [00:07<00:00, 25.62batch/s, loss=1.8326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7377 | Reg=1.0950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 83: 100%|██████████| 196/196 [00:05<00:00, 38.36batch/s, acc=63.12%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.12% | Remaining weights: 17.28% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 84: 100%|██████████| 196/196 [00:07<00:00, 25.57batch/s, loss=1.3843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8342 | Reg=0.5501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 84: 100%|██████████| 196/196 [00:05<00:00, 38.61batch/s, acc=78.10%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.10% | Remaining weights: 16.29% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 85: 100%|██████████| 196/196 [00:07<00:00, 25.62batch/s, loss=1.7983]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7403 | Reg=1.0580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 85: 100%|██████████| 196/196 [00:05<00:00, 38.30batch/s, acc=65.83%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.83% | Remaining weights: 17.12% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 86: 100%|██████████| 196/196 [00:07<00:00, 25.51batch/s, loss=1.2648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7239 | Reg=0.5409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 86: 100%|██████████| 196/196 [00:05<00:00, 38.55batch/s, acc=78.83%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.83% | Remaining weights: 16.16% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 87: 100%|██████████| 196/196 [00:07<00:00, 25.45batch/s, loss=2.3026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1298 | Reg=1.1727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 87: 100%|██████████| 196/196 [00:05<00:00, 38.91batch/s, acc=65.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.96% | Remaining weights: 17.11% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 88: 100%|██████████| 196/196 [00:07<00:00, 25.31batch/s, loss=1.2348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6912 | Reg=0.5436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 88: 100%|██████████| 196/196 [00:05<00:00, 38.47batch/s, acc=79.88%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.88% | Remaining weights: 16.06% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 89: 100%|██████████| 196/196 [00:07<00:00, 25.43batch/s, loss=2.1509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0536 | Reg=1.0973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 89: 100%|██████████| 196/196 [00:05<00:00, 38.75batch/s, acc=64.26%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.26% | Remaining weights: 16.88% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 90: 100%|██████████| 196/196 [00:07<00:00, 25.30batch/s, loss=1.2405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7038 | Reg=0.5367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 90: 100%|██████████| 196/196 [00:05<00:00, 38.76batch/s, acc=80.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.54% | Remaining weights: 15.93% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 91: 100%|██████████| 196/196 [00:07<00:00, 25.48batch/s, loss=2.0550]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8499 | Reg=1.2050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 91: 100%|██████████| 196/196 [00:05<00:00, 38.39batch/s, acc=65.46%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.46% | Remaining weights: 16.92% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 92: 100%|██████████| 196/196 [00:07<00:00, 25.44batch/s, loss=1.2307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6942 | Reg=0.5366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 92: 100%|██████████| 196/196 [00:05<00:00, 38.99batch/s, acc=80.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.54% | Remaining weights: 15.81% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 93: 100%|██████████| 196/196 [00:07<00:00, 25.61batch/s, loss=2.1410]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0347 | Reg=1.1063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 93: 100%|██████████| 196/196 [00:05<00:00, 38.86batch/s, acc=61.75%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.75% | Remaining weights: 16.69% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 94: 100%|██████████| 196/196 [00:07<00:00, 25.38batch/s, loss=1.2001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6618 | Reg=0.5383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 94: 100%|██████████| 196/196 [00:05<00:00, 38.93batch/s, acc=81.15%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.15% | Remaining weights: 15.72% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 95: 100%|██████████| 196/196 [00:07<00:00, 25.31batch/s, loss=1.9488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8387 | Reg=1.1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 95: 100%|██████████| 196/196 [00:05<00:00, 38.29batch/s, acc=67.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.47% | Remaining weights: 16.54% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 96: 100%|██████████| 196/196 [00:07<00:00, 25.32batch/s, loss=1.1792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6540 | Reg=0.5252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 96: 100%|██████████| 196/196 [00:05<00:00, 38.39batch/s, acc=81.03%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.03% | Remaining weights: 15.54% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 97: 100%|██████████| 196/196 [00:07<00:00, 25.38batch/s, loss=2.4828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.3889 | Reg=1.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 97: 100%|██████████| 196/196 [00:05<00:00, 38.21batch/s, acc=65.97%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.97% | Remaining weights: 16.34% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 98: 100%|██████████| 196/196 [00:07<00:00, 25.29batch/s, loss=1.1184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5858 | Reg=0.5326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 98: 100%|██████████| 196/196 [00:05<00:00, 38.35batch/s, acc=80.88%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.88% | Remaining weights: 15.48% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 99: 100%|██████████| 196/196 [00:07<00:00, 25.31batch/s, loss=2.1864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0187 | Reg=1.1676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 99: 100%|██████████| 196/196 [00:05<00:00, 38.63batch/s, acc=67.77%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.77% | Remaining weights: 16.29% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 100: 100%|██████████| 196/196 [00:03<00:00, 53.18batch/s, loss=0.5966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5966 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 100: 100%|██████████| 196/196 [00:02<00:00, 79.52batch/s, acc=81.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.54% | Remaining weights: 14.04% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 101: 100%|██████████| 196/196 [00:03<00:00, 52.15batch/s, loss=0.6617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6617 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 101: 100%|██████████| 196/196 [00:02<00:00, 79.99batch/s, acc=76.14%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.14% | Remaining weights: 13.18% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 102: 100%|██████████| 196/196 [00:03<00:00, 52.76batch/s, loss=0.5068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5068 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 102: 100%|██████████| 196/196 [00:02<00:00, 81.77batch/s, acc=83.59%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.59% | Remaining weights: 12.52% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 103: 100%|██████████| 196/196 [00:03<00:00, 53.31batch/s, loss=0.7891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7891 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 103: 100%|██████████| 196/196 [00:02<00:00, 79.25batch/s, acc=79.64%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.64% | Remaining weights: 12.23% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 104: 100%|██████████| 196/196 [00:03<00:00, 52.09batch/s, loss=0.7835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7835 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 104: 100%|██████████| 196/196 [00:02<00:00, 80.24batch/s, acc=84.72%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.72% | Remaining weights: 11.85% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 105: 100%|██████████| 196/196 [00:03<00:00, 52.51batch/s, loss=0.6559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6559 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 105: 100%|██████████| 196/196 [00:02<00:00, 79.56batch/s, acc=81.57%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.57% | Remaining weights: 11.69% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 106: 100%|██████████| 196/196 [00:03<00:00, 52.73batch/s, loss=0.4323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4323 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 106: 100%|██████████| 196/196 [00:02<00:00, 80.64batch/s, acc=85.42%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.42% | Remaining weights: 11.42% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 107: 100%|██████████| 196/196 [00:03<00:00, 53.11batch/s, loss=0.5188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5188 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 107: 100%|██████████| 196/196 [00:02<00:00, 80.47batch/s, acc=83.60%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.60% | Remaining weights: 11.33% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 108: 100%|██████████| 196/196 [00:03<00:00, 52.62batch/s, loss=0.5311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5311 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 108: 100%|██████████| 196/196 [00:02<00:00, 79.15batch/s, acc=86.21%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.21% | Remaining weights: 11.10% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 109: 100%|██████████| 196/196 [00:03<00:00, 53.20batch/s, loss=0.5847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5847 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 109: 100%|██████████| 196/196 [00:02<00:00, 80.43batch/s, acc=84.25%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.25% | Remaining weights: 11.04% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 110: 100%|██████████| 196/196 [00:03<00:00, 53.79batch/s, loss=0.3312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3312 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 110: 100%|██████████| 196/196 [00:02<00:00, 81.51batch/s, acc=87.04%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.04% | Remaining weights: 10.85% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 111: 100%|██████████| 196/196 [00:03<00:00, 52.78batch/s, loss=0.5172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5172 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 111: 100%|██████████| 196/196 [00:02<00:00, 82.05batch/s, acc=84.08%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.08% | Remaining weights: 10.81% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 112: 100%|██████████| 196/196 [00:03<00:00, 53.35batch/s, loss=0.5172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5172 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 112: 100%|██████████| 196/196 [00:02<00:00, 81.63batch/s, acc=87.18%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.18% | Remaining weights: 10.65% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 113: 100%|██████████| 196/196 [00:03<00:00, 52.80batch/s, loss=0.4123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4123 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 113: 100%|██████████| 196/196 [00:02<00:00, 81.11batch/s, acc=85.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.54% | Remaining weights: 10.62% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 114: 100%|██████████| 196/196 [00:03<00:00, 52.74batch/s, loss=0.5364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5364 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 114: 100%|██████████| 196/196 [00:02<00:00, 81.69batch/s, acc=87.36%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.36% | Remaining weights: 10.47% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 115: 100%|██████████| 196/196 [00:03<00:00, 52.62batch/s, loss=0.4361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4361 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 115: 100%|██████████| 196/196 [00:02<00:00, 81.20batch/s, acc=86.77%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.77% | Remaining weights: 10.45% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 116: 100%|██████████| 196/196 [00:03<00:00, 52.49batch/s, loss=0.3802]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3802 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 116: 100%|██████████| 196/196 [00:02<00:00, 80.58batch/s, acc=88.14%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.14% | Remaining weights: 10.32% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 117: 100%|██████████| 196/196 [00:03<00:00, 52.78batch/s, loss=0.4123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4123 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 117: 100%|██████████| 196/196 [00:02<00:00, 79.19batch/s, acc=87.22%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.22% | Remaining weights: 10.30% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 118: 100%|██████████| 196/196 [00:03<00:00, 52.62batch/s, loss=0.6124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6124 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 118: 100%|██████████| 196/196 [00:02<00:00, 79.65batch/s, acc=88.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.41% | Remaining weights: 10.18% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 119: 100%|██████████| 196/196 [00:03<00:00, 52.06batch/s, loss=0.5266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5266 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 119: 100%|██████████| 196/196 [00:02<00:00, 77.87batch/s, acc=88.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.06% | Remaining weights: 10.17% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 120: 100%|██████████| 196/196 [00:03<00:00, 52.07batch/s, loss=0.3990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3990 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 120: 100%|██████████| 196/196 [00:02<00:00, 80.91batch/s, acc=88.49%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.49% | Remaining weights: 10.06% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 121: 100%|██████████| 196/196 [00:03<00:00, 52.51batch/s, loss=0.3918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3918 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 121: 100%|██████████| 196/196 [00:02<00:00, 82.38batch/s, acc=89.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.06% | Remaining weights: 10.05% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 122: 100%|██████████| 196/196 [00:03<00:00, 52.42batch/s, loss=0.6368]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6368 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 122: 100%|██████████| 196/196 [00:02<00:00, 80.18batch/s, acc=88.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.82% | Remaining weights: 9.95% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 123: 100%|██████████| 196/196 [00:03<00:00, 52.07batch/s, loss=0.3721]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3721 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 123: 100%|██████████| 196/196 [00:02<00:00, 81.41batch/s, acc=89.67%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.67% | Remaining weights: 9.94% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 124: 100%|██████████| 196/196 [00:03<00:00, 53.51batch/s, loss=0.5262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5262 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 124: 100%|██████████| 196/196 [00:02<00:00, 79.34batch/s, acc=89.08%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.08% | Remaining weights: 9.84% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 125: 100%|██████████| 196/196 [00:03<00:00, 52.58batch/s, loss=0.4540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4540 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 125: 100%|██████████| 196/196 [00:02<00:00, 79.76batch/s, acc=90.40%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.40% | Remaining weights: 9.83% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 126: 100%|██████████| 196/196 [00:03<00:00, 52.84batch/s, loss=0.3604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3604 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 126: 100%|██████████| 196/196 [00:02<00:00, 80.53batch/s, acc=89.43%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.43% | Remaining weights: 9.75% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 127: 100%|██████████| 196/196 [00:03<00:00, 53.31batch/s, loss=0.3222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3222 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 127: 100%|██████████| 196/196 [00:02<00:00, 80.05batch/s, acc=90.64%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.64% | Remaining weights: 9.74% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 128: 100%|██████████| 196/196 [00:03<00:00, 52.21batch/s, loss=0.4401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4401 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 128: 100%|██████████| 196/196 [00:02<00:00, 80.13batch/s, acc=89.35%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.35% | Remaining weights: 9.67% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 129: 100%|██████████| 196/196 [00:03<00:00, 52.63batch/s, loss=0.3618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3618 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 129: 100%|██████████| 196/196 [00:02<00:00, 80.53batch/s, acc=90.94%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.94% | Remaining weights: 9.66% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pquant import iterative_train\n",
    "\"\"\"\n",
    "Inputs to train_resnet we defined previously are:\n",
    "          model, trainloader, device, loss_func, epoch, optimizer, scheduler, **kwargs\n",
    "\"\"\"\n",
    "\n",
    "trained_model = iterative_train(model = model, \n",
    "                                config = config, \n",
    "                                train_func = train_resnet, \n",
    "                                valid_func = validate_resnet, \n",
    "                                trainloader = train_loader, \n",
    "                                testloader = val_loader, \n",
    "                                device = device, \n",
    "                                loss_func = loss_function,\n",
    "                                optimizer = optimizer, \n",
    "                                scheduler = scheduler\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd70fed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_403428/1596906618.py:29: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax[0].set_yticklabels(new_ytick)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAHpCAYAAACful8UAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAlaJJREFUeJzs3XdYU+f/PvA7YW+KgyEqOBGtCxARByofcVbUqrS2orVO0CLu1omrWrVWpbV24Ki2am3V2hYHCiriwlEHblGrAi5AUYbw/P7wR76NgAKeJAe4X9eVq80Zd955Ys6Th7MUQggBIiIiIiIiIpKcUtcFEBEREREREZVXHHQTERERERERaQgH3UREREREREQawkE3ERERERERkYZw0E1ERERERESkIRx0ExEREREREWkIB91EREREREREGsJBNxEREREREZGGcNBNREREREREpCEcdBPpiJOTEwYNGlSqdX18fODj4yNpPXKhUCgwc+bMUq8bHBwsbUFlyJu0HRERldygQYPg5ORU6nXNzc2lLagMeZO2IyprOOimMmv16tVQKBSqh76+PqpVq4ZBgwbh9u3bui6PZOzQoUOYOXMmUlNTdV0KERFJbNOmTVAoFPj9998LzGvSpAkUCgX27dtXYF6NGjXQqlUrbZRYIk+fPsXMmTMRHR2t61KIqJT0dV0A0ZsKCwuDs7MzMjMzcfjwYaxevRoHDx7E2bNnYWxsrOvyinTx4kUolaX7u9euXbskrkY+nj17Bn19zW6aDh06hFmzZmHQoEGwtrbW6GsREZF2tW7dGgBw8OBB9OrVSzU9PT0dZ8+ehb6+PmJjY9G+fXvVvFu3buHWrVsICAgo0Wt99913yMvLk6bwIjx9+hSzZs0CgHJ7lBtRecdBN5V5Xbp0gbu7OwDg448/RuXKlbFgwQJs374d/fr103F1RTMyMir1uoaGhhJWIi9y/kOJrmVmZsLQ0LDUf6zRpbJcOxGVLQ4ODnB2dsbBgwfVpsfFxUEIgb59+xaYl/88f8BeXAYGBm9WbDkmhEBmZiZMTEx0XUqJleXaSZ7464fKnTZt2gAArl69qjb9woULePfdd2FjYwNjY2O4u7tj+/btasvkH7J+8OBBjBkzBlWqVIG1tTWGDx+O7OxspKamYuDAgXjrrbfw1ltvYeLEiRBCqGUsWrQIrVq1QqVKlWBiYgI3Nzf8+uuvBep8+Zzu/NeOjY1FaGgoqlSpAjMzM/Tq1Qv37t1TW/flc7qjo6OhUCiwadMmzJ07F46OjjA2NkbHjh1x5cqVAq8dHh6OWrVqwcTEBC1atMCBAweKdZ5479690bx5c7VpPXr0gEKhUGvLI0eOQKFQ4O+//1ZNS01NRUhICKpXrw4jIyPUqVMHCxYsKLCHoLDzkqOjo+Hu7g5jY2PUrl0b3377LWbOnAmFQlFonVu3bkWjRo1gZGSEhg0bIjIyUjVv5syZmDBhAgDA2dlZdXpCYmIiAGD37t1o3bo1rK2tYW5ujvr16+PTTz99Zbvk1x0cHIz169ejfv36MDY2hpubG/bv319g2du3b+Ojjz6Cra2tqsYff/yxwHtWKBT45ZdfMHXqVFSrVg2mpqZIT09/bS35bty4gVGjRqF+/fowMTFBpUqV0LdvX9V7BYBr165BoVDgyy+/LLD+oUOHoFAo8PPPP2u9diKiN9G6dWucPHkSz549U02LjY1Fw4YN0aVLFxw+fFit/4mNjYVCoYC3t7dq2k8//QQ3NzeYmJjAxsYGAQEBuHXrltrrFHZe8oMHD/Dhhx/C0tIS1tbWCAwMxOnTp6FQKLB69eoCtd6+fRv+/v4wNzdHlSpVMH78eOTm5gIAEhMTUaVKFQDArFmzVH1Wfj+ZlJSEwYMHw9HREUZGRrC3t0fPnj3VtvOFyT+f/Nq1a/Dz84OZmRkcHBwQFhZW4HdNXl4eli5dioYNG8LY2Bi2trYYPnw4Hj16pLack5MTunfvjp07d8Ld3R0mJib49ttvX1nHy4rzG6pdu3Zo0qRJoevXr18ffn5+Oqmd6FW4p5vKnfyO5q233lJNO3fuHLy9vVGtWjVMnjwZZmZm2LRpE/z9/bFlyxa1w88AYPTo0bCzs8OsWbNw+PBhrFq1CtbW1jh06BBq1KiBefPm4a+//sIXX3yBRo0aYeDAgap1v/rqK7zzzjsYMGAAsrOz8csvv6Bv377YsWMHunXr9tr6R48ejbfeegszZsxAYmIili5diuDgYGzcuPG1637++edQKpUYP3480tLSsHDhQgwYMABHjhxRLfPNN98gODgYbdq0wdixY5GYmAh/f3+89dZbcHR0fGV+mzZtsG3bNqSnp8PS0hJCCMTGxkKpVOLAgQN45513AAAHDhyAUqlU/Xh5+vQp2rVrh9u3b2P48OGoUaMGDh06hClTpuDu3btYunRpka958uRJdO7cGfb29pg1axZyc3MRFham+hHysoMHD+K3337DqFGjYGFhgWXLlqFPnz64efMmKlWqhN69e+PSpUv4+eef8eWXX6Jy5coAgCpVquDcuXPo3r07GjdujLCwMBgZGeHKlSuIjY19bdsDQExMDDZu3IgxY8bAyMgIX3/9NTp37oyjR4+iUaNGAIDk5GS0bNlSNUivUqUK/v77bwwZMgTp6ekICQlRy5w9ezYMDQ0xfvx4ZGVllegoh2PHjuHQoUMICAiAo6MjEhMT8c0338DHxwfnz5+HqakpatWqBW9vb6xfvx5jx45VW3/9+vWwsLBAz549tV47EdGbaN26NdatW4cjR46o/qAcGxuLVq1aoVWrVkhLS8PZs2fRuHFj1TwXFxdUqlQJADB37lxMmzYN/fr1w8cff4x79+5h+fLlaNu2LU6ePFnkqUl5eXno0aMHjh49ipEjR8LFxQXbtm1DYGBgocvn5ubCz88Pnp6eWLRoEfbs2YPFixejdu3aGDlyJKpUqYJvvvkGI0eORK9evdC7d28AUNXdp08fnDt3DqNHj4aTkxNSUlKwe/du3Lx587UXKcvNzUXnzp3RsmVLLFy4EJGRkZgxYwaeP3+OsLAw1XLDhw/H6tWrMXjwYIwZMwbXr1/HihUrcPLkScTGxqrt7b948SLee+89DB8+HEOHDkX9+vVf+1n9V3F+Q3344YcYOnQozp49q+pbgRd93qVLlzB16lSd1E70SoKojIqIiBAAxJ49e8S9e/fErVu3xK+//iqqVKkijIyMxK1bt1TLduzYUbz99tsiMzNTNS0vL0+0atVK1K1bt0Cmn5+fyMvLU0338vISCoVCjBgxQjXt+fPnwtHRUbRr106trqdPn6o9z87OFo0aNRIdOnRQm16zZk0RGBhY4LV9fX3VXnvs2LFCT09PpKamqqa1a9dO7XX37dsnAIgGDRqIrKws1fSvvvpKABBnzpwRQgiRlZUlKlWqJDw8PEROTo5qudWrVwsABd7Ly44dOyYAiL/++ksIIcQ///wjAIi+ffsKT09P1XLvvPOOaNasmer57NmzhZmZmbh06ZJa3uTJk4Wenp64efOmahoAMWPGDNXzHj16CFNTU3H79m3VtMuXLwt9fX3x8iYMgDA0NBRXrlxRTTt9+rQAIJYvX66a9sUXXwgA4vr162rrf/nllwKAuHfv3ivboTAABABx/Phx1bQbN24IY2Nj0atXL9W0IUOGCHt7e3H//n219QMCAoSVlZXq30/+Z1qrVq0C/6ZeVcN/266w9eLi4gQAsXbtWtW0b7/9VgAQCQkJqmnZ2dmicuXKav9GNVk7EZGUzp07JwCI2bNnCyGEyMnJEWZmZmLNmjVCCCFsbW1FeHi4EEKI9PR0oaenJ4YOHSqEECIxMVHo6emJuXPnqmWeOXNG6Ovrq00PDAwUNWvWVD3fsmWLACCWLl2qmpabmys6dOggAIiIiAi1dQGIsLAwtddp1qyZcHNzUz2/d+9ege27EEI8evRIABBffPFFCVvn/1579OjRqml5eXmiW7duwtDQUNUPHjhwQAAQ69evV1s/MjKywPSaNWsKACIyMrLYNfy37YQo3m+o1NRUYWxsLCZNmqS27JgxY4SZmZl48uSJxmsnKikeXk5lnq+vL6pUqYLq1avj3XffhZmZGbZv367aa/vw4UPs3bsX/fr1w+PHj3H//n3cv38fDx48gJ+fHy5fvlzgaudDhgxRO3TZ09MTQggMGTJENU1PTw/u7u64du2a2rr/Pf/n0aNHSEtLQ5s2bXDixIlivZ9hw4apvXabNm2Qm5uLGzduvHbdwYMHq+1NzD/UPr/G48eP48GDBxg6dKjaxcoGDBigdmRAUZo1awZzc3PVIdMHDhyAo6MjBg4ciBMnTuDp06cQQuDgwYOq1waAzZs3o02bNnjrrbdU7X///n34+voiNze30EOwgRd/hd+zZw/8/f3h4OCgml6nTh106dKl0HV8fX1Ru3Zt1fPGjRvD0tKywOdUmPw9F9u2bSvVhXG8vLzg5uamel6jRg307NkTO3fuRG5uLoQQ2LJlC3r06AEhhFpb+Pn5IS0trcC/k8DAwFKfU/bf9XJycvDgwQPUqVMH1tbWaq/Tr18/GBsbY/369appO3fuxP379/HBBx8AgNZrJyJ6Ew0aNEClSpVU52qfPn0aGRkZqquTt2rVSnUUU1xcHHJzc1Xnc//222/Iy8tDv3791LZ1dnZ2qFu3bqFXPs8XGRkJAwMDDB06VDVNqVQiKCioyHVGjBih9rxNmzbF6rNMTExgaGiI6OjoAodLF9d/b7OZfxRTdnY29uzZA+BF/21lZYX//e9/am3h5uYGc3PzAm3h7Oysdnh3SRXnN5SVlRV69uyJn3/+WXUofG5uLjZu3Ah/f3+YmZnppHaiV+Hh5VTmhYeHo169ekhLS8OPP/6I/fv3q12k7MqVKxBCYNq0aZg2bVqhGSkpKahWrZrqeY0aNdTmW1lZAQCqV69eYPrLHd2OHTswZ84cnDp1CllZWarpRZ1//LKXXzt/MFycDvV16+YP3OvUqaO2nL6+frHulamnpwcvLy8cOHAAwItBd5s2bdC6dWvk5ubi8OHDsLW1xcOHD9UG3ZcvX8Y///xT5CHhKSkpRU5/9uxZgXoLew/5Xm4D4EU7FKf9+vfvj++//x4ff/wxJk+ejI4dO6J379549913i3UBsLp16xaYVq9ePTx9+hT37t2DUqlEamoqVq1ahVWrVhWa8XJbODs7v/Z1i/Ls2TPMnz8fERERuH37ttp5emlpaar/t7a2Ro8ePbBhwwbMnj0bwItDy6tVq4YOHToAAO7du6fV2omI3oRCoUCrVq2wf/9+5OXlITY2FlWrVlX1Ha1atcKKFSsAQDX4zh90X758GUKIQrfpwKsvnnbjxg3Y29vD1NRUbXpRfZaxsXGBvrG4fZaRkREWLFiAcePGwdbWFi1btkT37t0xcOBA2NnZvXZ9pVKJWrVqqU2rV68egP87Ve/y5ctIS0tD1apVC82Qertf3N9QAwcOxMaNG3HgwAG0bdsWe/bsQXJyMj788EPVMtqunehVOOimMq9Fixaqq5f7+/ujdevWeP/993Hx4kWYm5ur9liOHz++yL9gvtwZ6unpFbpcYdP/O5DJP6+5bdu2+Prrr2Fvbw8DAwNERERgw4YNxXo/Rb22eOnCJlKvW1ytW7fG3LlzkZmZiQMHDuCzzz6DtbU1GjVqhAMHDsDW1hYA1AbdeXl5+N///oeJEycWmpnfyUvhTdrAxMQE+/fvx759+/Dnn38iMjISGzduRIcOHbBr164is4sr/9/iBx98UOT5ffnn6f23ptIaPXo0IiIiEBISAi8vL1hZWUGhUCAgIKDAnvyBAwdi8+bNOHToEN5++21s374do0aNUv2xQdu1ExG9qdatW+OPP/7AmTNnVOdz52vVqhUmTJiA27dv4+DBg3BwcFANQPPy8lQXAy1su29ubi5ZjW/ar4SEhKBHjx7YunUrdu7ciWnTpmH+/PnYu3cvmjVr9sb15eXloWrVqmpHQv3Xy38weJPtfkl+Q/n5+cHW1hY//fQT2rZti59++gl2dnbw9fXVSe1Er8NBN5Urenp6mD9/Ptq3b48VK1Zg8uTJqk7UwMBAbWOsCVu2bIGxsTF27typtrc9IiJCo69bXDVr1gTwYu//f+9P+vz5cyQmJhYYNBWmTZs2yM7Oxs8//4zbt2+rBtdt27ZVDbrr1aunGnwDQO3atfHkyZMSt3/VqlVhbGxc6BXYC5tWXK866kCpVKJjx47o2LEjlixZgnnz5uGzzz7Dvn37Xlv/5cuXC0y7dOkSTE1NVZ27hYUFcnNzNf5vEQB+/fVXBAYGYvHixappmZmZSE1NLbBs586dUaVKFaxfvx6enp54+vSp2h6DKlWqaLV2IqI39d/7dcfGxqpd7NHNzQ1GRkaIjo7GkSNH0LVrV9W82rVrQwgBZ2fnEv9RuGbNmti3bx+ePn2qtrdbU30W8KLecePGYdy4cbh8+TKaNm2KxYsX46effnrlenl5ebh27Zrae7x06RIAqI5+q127Nvbs2QNvb2+ND0pL8htKT08P77//PlavXo0FCxZg69atGDp0qNofMbRZO9Hr8JxuKnd8fHzQokULLF26FJmZmahatSp8fHzw7bff4u7duwWWf/l2XG9CT08PCoVCdasP4MUhWlu3bpXsNd6Eu7s7KlWqhO+++w7Pnz9XTV+/fn2xzwfz9PSEgYEBFixYABsbGzRs2BDAi8H44cOHERMTo7aXG3hxznBcXBx27txZIC81NVWtlv/S09ODr68vtm7dijt37qimX7lyRe12ZCWVf77Xy4PPhw8fFli2adOmAKB2mFtR4uLi1M47u3XrFrZt24ZOnTpBT08Penp66NOnD7Zs2YKzZ88WWF/Kf4vAi/Z7eQ//8uXL1f595tPX18d7772HTZs2YfXq1Xj77bfV/gij7dqJiN5U/q0m169fj9u3b6vt6TYyMkLz5s0RHh6OjIwMtftz9+7dG3p6epg1a1aBbagQAg8ePCjyNf38/JCTk4PvvvtONS0vLw/h4eGlfh/5g/eX+6ynT58iMzNTbVrt2rVhYWFRrD4LgOoQe+DFe1uxYgUMDAzQsWNHAC/679zcXNWpR//1/PnzQv+IW1ol/Q314Ycf4tGjRxg+fDiePHmiugZJPm3WTvQ63NNN5dKECRPQt29frF69GiNGjEB4eDhat26Nt99+G0OHDkWtWrWQnJyMuLg4/Pvvvzh9+rQkr9utWzcsWbIEnTt3xvvvv4+UlBSEh4ejTp06+OeffyR5jTdhaGiImTNnYvTo0ejQoQP69euHxMRErF69GrVr1y7WeeempqZwc3PD4cOHVffoBl7s6c7IyEBGRkaBQfeECROwfft2dO/eHYMGDYKbmxsyMjJw5swZ/Prrr0hMTFTduutlM2fOxK5du+Dt7Y2RI0ciNzcXK1asQKNGjXDq1KlStUP+xc4+++wzBAQEwMDAAD169EBYWBj279+Pbt26oWbNmkhJScHXX38NR0dHtR9kRWnUqBH8/PzUbhkGvLi3ar7PP/8c+/btg6enJ4YOHQpXV1c8fPgQJ06cwJ49ewod+JdW9+7dsW7dOlhZWcHV1RVxcXHYs2eP6pY4Lxs4cCCWLVuGffv2YcGCBQXma7N2IqI3ZWhoCA8PDxw4cABGRkZqF7oEXhxinn8k0H+38bVr18acOXMwZcoU1W01LSwscP36dfz+++8YNmwYxo8fX+hr+vv7o0WLFhg3bhyuXLkCFxcXbN++XbV9LO71Xf7LxMQErq6u2LhxI+rVqwcbGxs0atQIz58/R8eOHdGvXz+4urpCX18fv//+O5KTkxEQEPDaXGNjY0RGRiIwMBCenp74+++/8eeff+LTTz9VHZ3Vrl07DB8+HPPnz8epU6fQqVMnGBgY4PLly9i8eTO++uorvPvuuyV+T4Up6W+oZs2aoVGjRti8eTMaNGiA5s2bq83XZu1Er6Xlq6UTSSb/FlvHjh0rMC83N1fUrl1b1K5dWzx//lwIIcTVq1fFwIEDhZ2dnTAwMBDVqlUT3bt3F7/++utrM2fMmFHoraQCAwOFmZmZ2rQffvhB1K1bVxgZGQkXFxcRERGhWv+/irpl2MuvnX/7pX379qmmFXXLsM2bN6ute/369QK3KBFCiGXLlomaNWsKIyMj0aJFCxEbGyvc3NxE586dC7RlYSZMmCAAiAULFqhNr1OnjgAgrl69WmCdx48fiylTpog6deoIQ0NDUblyZdGqVSuxaNEikZ2drVoOhdwWJSoqSjRr1kwYGhqK2rVri++//16MGzdOGBsbqy0HQAQFBRV47ZfbWogXtzGrVq2aUCqVqtuHRUVFiZ49ewoHBwdhaGgoHBwcxHvvvVfgVmeFyX/tn376SfX5N2vWTO1zy5ecnCyCgoJE9erVhYGBgbCzsxMdO3YUq1atUi1T1Gf6uhr+23aPHj0SgwcPFpUrVxbm5ubCz89PXLhwodD2yNewYUOhVCrFv//+W+h8TdVORKQJU6ZMEQBEq1atCsz77bffBABhYWGh+q3wX1u2bBGtW7cWZmZmwszMTLi4uIigoCBx8eJF1TKF3fbq3r174v333xcWFhbCyspKDBo0SMTGxgoA4pdfflFb9+XfEEKIQn8zHDp0SLi5uQlDQ0PVtv7+/fsiKChIuLi4CDMzM2FlZSU8PT3Fpk2bXtsu+a999epV0alTJ2FqaipsbW3FjBkzRG5uboHlV61aJdzc3ISJiYmwsLAQb7/9tpg4caK4c+eOapmaNWuKbt26vfa1/1vDy21X3N9Q+RYuXCgAiHnz5hX5OpqonaikFEJIeIUlIiqT8vLyUKVKFfTu3VvtkDg58/f3x7lz5wo9j1oXFAoFgoKC1A7VK4uaNWsGGxsbREVF6boUIqJyY+vWrejVqxcOHjwIb29vXZeDQYMG4ddff8WTJ090Xcob+eqrrzB27FgkJiYWevcSIrngOd1EFUxmZmaBc9TWrl2Lhw8fwsfHRzdFvcazZ8/Unl++fBl//fWXbOstq44fP45Tp05h4MCBui6FiKjMernPys3NxfLly2FpaVngEGgqPSEEfvjhB7Rr144DbpI9ntNNVMEcPnwYY8eORd++fVGpUiWcOHECP/zwAxo1aoS+ffvqurxC1apVC4MGDUKtWrVw48YNfPPNNzA0NCzyFmRUMmfPnkV8fDwWL14Me3t79O/fX9clERGVWaNHj8azZ8/g5eWFrKws/Pbbbzh06BDmzZvHq2hLICMjA9u3b8e+fftw5swZbNu2TdclEb0WB91EFYyTkxOqV6+OZcuW4eHDh7CxscHAgQPx+eefw9DQUNflFapz5874+eefkZSUBCMjI3h5eWHevHmoW7eurksrF3799VeEhYWhfv36+Pnnn2FsbKzrkoiIyqwOHTpg8eLF2LFjBzIzM1GnTh0sX74cwcHBui6tXLh37x7ef/99WFtb49NPP8U777yj65KIXovndBMRERERERFpCM/pJiIiIiIiItIQHl7+hvLy8nDnzh1YWFiU6t6LREREmiKEwOPHj+Hg4AClsuz+nZ19LRERyVFx+1kOut/QnTt3UL16dV2XQUREVKRbt27B0dFR12WUGvtaIiKSs9f1sxx0vyELCwsALxra0tJSx9UQERH9n/T0dFSvXl3VV5VV7GuJiEiOitvPctD9hvIPc7O0tOQPASIikqWyekh2eHg4wsPDkZubC4B9LRERydPr+tmye4IXERERlWtBQUE4f/48jh07putSiIiISo2DbiIiIiIiIiIN4aCbiIiIiIiISEN4TjcRERGVeXl5ecjOztZ1GVSBGRgYQE9PT9dlEJEMcdBNREREZVp2djauX7+OvLw8XZdCFZy1tTXs7OzK7MULiUgzOOgmIiKiMksIgbt370JPTw/Vq1eHUskz50j7hBB4+vQpUlJSAAD29vY6roiI5ISDbiIiIiqznj9/jqdPn8LBwQGmpqa6LocqMBMTEwBASkoKqlatykPNiUilxH8O3r9/P3r06AEHBwcoFAps3bpVbb4QAtOnT4e9vT1MTEzg6+uLy5cvqy3z8OFDDBgwAJaWlrC2tsaQIUPw5MmTV75uZmYmgoKCUKlSJZibm6NPnz5ITk5WW+bmzZvo1q0bTE1NUbVqVUyYMAHPnz9XzT958iSaNWsGc3Nz9OjRAw8fPlTNe/78Odzc3HD06NGSNgkRERHpSP49vA0NDXVcCRFUf/jJycnRcSVEJCclHnRnZGSgSZMmCA8PL3T+woULsWzZMqxcuRJHjhyBmZkZ/Pz8kJmZqVpmwIABOHfuHHbv3o0dO3Zg//79GDZs2Ctfd+zYsfjjjz+wefNmxMTE4M6dO+jdu7dqfm5uLrp164bs7GwcOnQIa9aswerVqzF9+nTVMh9//DE6dOiAEydOIC0tDfPmzVPNW7x4Mby9vdGiRYuSNgkRERHpGM+hJTngv0MiKoxCCCFKvbJCgd9//x3+/v4AXuzldnBwwLhx4zB+/HgAQFpaGmxtbbF69WoEBAQgISEBrq6uOHbsGNzd3QEAkZGR6Nq1K/799184ODgUeJ20tDRUqVIFGzZswLvvvgsAuHDhAho0aIC4uDi0bNkSf//9N7p37447d+7A1tYWALBy5UpMmjQJ9+7dg6GhIUxNTXHixAm4uLjgm2++wY4dO/Dnn3/i2rVr6Ny5M+Lj42FhYfHK95yVlYWsrCzV8/T0dFSvXh1paWmwtLQsbVMSERFJLj09HVZWVmW+j3rV+8jMzMT169fh7OwMY2NjHVVI9AL/PRJVLMXtZyU9p/v69etISkqCr6+vapqVlRU8PT0RFxeHgIAAxMXFwdraWjXgBgBfX18olUocOXIEvXr1KpAbHx+PnJwctVwXFxfUqFFDNeiOi4vD22+/rRpwA4Cfnx9GjhyJc+fOoVmzZmjSpAl2796NOnXqICoqCo0bNwYAjBgxAgsXLnztgBsA5s+fj1mzZpWqfYrDafKfpV438fNuGsmSY01vkiXHml7OYk1lt6Y3yXo5p7yTss3lpjy/NyIi0qy0NxhrWM2YIWEl/0eqmuT43rRB0kF3UlISAKgNfPOf589LSkpC1apV1YvQ14eNjY1qmcJyDQ0NYW1t/crcwl73v3V9//33GDVqFBYtWgRvb29MmTIF69atg6mpKTw8PODn54erV68iICAAc+bMKbSWKVOmIDQ0VPU8f083EZHUOHAjKr03+WFXGnL/MTho0CCkpqYWuBaPpsycORNbt27FqVOnir2Oj48PmjZtiqVLl2qsLiIiXahQ99Vo2LAhYmJicOPGDWzYsAE5OTmYMWMGVqxYgdGjR6NVq1Y4ffo0fvvtN/zxxx+FZhgZGcHS0lLtQURERFQSPj4+CAkJ0dp62jZ+/HhERUVJnlvYRXyJiORO0kG3nZ0dABS4qnhycrJqnp2dneoehvmeP3+Ohw8fqpYpLDc7OxupqamvzC3sdf9b18tCQ0MREhICR0dHREdHo2/fvjAzM0O3bt0QHR39+jdMRERERAWYm5ujUqVKui6DiEgWJB10Ozs7w87OTu0vm+np6Thy5Ai8vLwAAF5eXkhNTUV8fLxqmb179yIvLw+enp6F5rq5ucHAwEAt9+LFi7h586Za7pkzZ9QG9Lt374alpSVcXV0LZEZFRSEhIQHBwcEAXlz9PP/2Djk5OapbkBARERFJadCgQYiJicFXX30FhUIBhUKBxMREAEBMTAxatGgBIyMj2NvbY/Lkyarbnxa1Xm5uLoYMGQJnZ2eYmJigfv36+Oqrr4pdjxACVapUwa+//qqa1rRpU9jb26ueHzx4EEZGRnj69CkAIDU1FR9//DGqVKkCS0tLdOjQAadPn1YtP3PmTDRt2lT1/Pnz5xgzZgysra1RqVIlTJo0CYGBgaqL8ebLy8vDxIkTYWNjAzs7O8ycOVM1z8nJCQDQq1cvKBQK1fPTp0+jffv2sLCwgKWlJdzc3HD8+PFiv38iIk0r8aD7yZMnOHXqlOocnevXr+PUqVO4efMmFAoFQkJCMGfOHGzfvh1nzpzBwIED4eDgoNqoNmjQAJ07d8bQoUNx9OhRxMbGIjg4GAEBAaorl9++fRsuLi6qe2ZbWVlhyJAhCA0Nxb59+xAfH4/BgwfDy8sLLVu2BAB06tQJrq6u+PDDD3H69Gns3LkTU6dORVBQEIyMjNTeQ2ZmJoKDg7Fq1SoolS+awNvbG+Hh4Th9+jS2bNkCb2/vUjUoERER0at89dVX8PLywtChQ3H37l3cvXsX1atXx+3bt9G1a1d4eHjg9OnT+Oabb/DDDz+orjNT1Hp5eXlwdHTE5s2bcf78eUyfPh2ffvopNm3aVKx6FAoF2rZtqzrK79GjR0hISMCzZ89w4cIFAC/+GODh4aG6D3Xfvn2RkpKCv//+G/Hx8WjevDk6duyIhw8fFvoaCxYswPr16xEREYHY2Fikp6cXepj4mjVrYGZmhiNHjmDhwoUICwvD7t27AQDHjh0DAERERODu3buq5wMGDICjoyOOHTuG+Ph4TJ48GQYGBsX7MIiItKDEF1I7fvw42rdvr3qef1GxwMBArF69GhMnTkRGRgaGDRuG1NRUtG7dGpGRkWq3TVi/fj2Cg4PRsWNHKJVK9OnTB8uWLVPNz8nJwcWLF1V/TQWAL7/8UrVsVlYW/Pz88PXXX6vm6+npYceOHRg5ciS8vLxgZmaGwMBAhIWFFXgPs2bNQrdu3dT+Arts2TK8//77aNu2LQYMGIA+ffqUtGmIiIiIXsvKykp1K9P/ngL39ddfo3r16lixYgUUCgVcXFxw584dTJo0CdOnTy9yPT09PbU7qzg7OyMuLg6bNm1Cv379ilWTj48Pvv32WwDA/v370axZM9jZ2SE6OhouLi6Ijo5Gu3btALzY63306FGkpKSodmwsWrQIW7duxa+//ophw4YVyF++fDmmTJmiukvNihUr8NdffxVYrnHjxpjx/y9KV7duXaxYsQJRUVH43//+hypVqgAArK2t1d7/zZs3MWHCBLi4uKjWIyKSkxIPun18fPCqW3srFAqEhYUVOtjNZ2Njgw0bNhQ538nJqcBrGBsbIzw8HOHh4UWuV7NmzUI34C+bP39+gWl16tRR7VknIiIi3cvv9yvKKV8JCQnw8vKCQqFQTfP29saTJ0/w77//okaNGkWuGx4ejh9//BE3b97Es2fPkJ2drbZz4XXatWuHTz75BPfu3UNMTAx8fHxUg+4hQ4bg0KFDmDhxIoAXh3M/efKkwDnbz549w9WrVwtkp6WlITk5GS1atFBN09PTg5ubG/Ly8tSWzb+daz57e/sC1wJ6WWhoKD7++GOsW7cOvr6+6Nu3L2rXrl3s905EpGkV6urlREREVHYEBQXh/PnzqsOIqXC//PILxo8fjyFDhmDXrl04deoUBg8ejOzs7GJnvP3227CxsUFMTIxq0O3j44OYmBgcO3YMOTk5aNWqFYAXpxra29urTjfMf1y8eBETJkx4o/fy8mHhCoWiwMD8ZTNnzsS5c+fQrVs37N27F66urvj999/fqA4iIilJep9uIiIiIno9Q0PDAnvwGzRogC1btkAIodrbHRsbCwsLCzg6Oha5XmxsLFq1aoVRo0apphW2x/lVFAoF2rRpg23btuHcuXNo3bo1TE1NkZWVhW+//Rbu7u4wMzMDADRv3hxJSUnQ19dXXczsVaysrGBra4tjx46hbdu2AF5cwPbEiRMl2hsPvBiUF3bkQ7169VCvXj2MHTsW7733HiIiIlSHshMR6Rr3dBMRERFpmZOTE44cOYLExETcv38feXl5GDVqFG7duoXRo0fjwoUL2LZtG2bMmIHQ0FDVhV8LW69u3bo4fvw4du7ciUuXLmHatGmlOjrAx8cHP//8M5o2bQpzc3MolUq0bdsW69evV53PDQC+vr7w8vKCv78/du3ahcTERBw6dAifffZZkVcNHz16NObPn49t27bh4sWL+OSTT/Do0SO1Q+mL225RUVFISkrCo0eP8OzZMwQHByM6Oho3btxAbGwsjh07hgYNGpT4/RMRaQr3dBMREVG5Y/X/L8YlV+PHj0dgYCBcXV3x7NkzXL9+HU5OTvjrr78wYcIENGnSBDY2NhgyZAimTp36yvWGDx+OkydPon///lAoFHjvvfcwatQo/P333yWqqV27dsjNzYWPj49qmo+PD7Zt26Y2TaFQ4K+//sJnn32GwYMH4969e7Czs0Pbtm1ha2tbaPakSZOQlJSEgQMHQk9PD8OGDYOfnx/09PRKVOPixYsRGhqK7777DtWqVcOlS5fw4MEDDBw4EMnJyahcuTJ69+6tdmE5IiJd46CbiIiISMvq1auHuLi4AtPbtWv3ygu7FrVeREQEIiIi1Kb998Kxq1evfm1NTZs2LXAh25CQEISEhBRY1sLCAsuWLVO7+8x/zZw5U+0e2/r6+li+fDmWL18O4MX9uBs0aKB2dfX8W5b918u3FevRowd69OihNu3nn39+xbsiItI9DrqJiIiISKNu3LiBXbt2oV27dsjKysKKFStw/fp1vP/++7oujYhI43hONxERERFplFKpxOrVq+Hh4QFvb2+cOXMGe/bs4bnXRFQhcE83EREREWlU9erVERsbq+syiIh0gnu6iYiIiIiIiDSEg24iIiIiIiIiDeGgm4iIiIiIiEhDOOgmIiIiIiIi0hAOuomIiIiIiIg0hINuIiIiIiIiIg3hLcOIiIio3PGb/adWX2/ntG5afb2KbtCgQUhNTcXWrVuLvY6TkxNCQkIQEhKisbqIiArDQTcRERERlSlfffUVhBCSZiYmJsLZ2RknT55E06ZNJc0mooqNg24iIiIiKpbs7GwYGhrqugxYWVnpugQiomLjOd1EREREWubj44MxY8Zg4sSJsLGxgZ2dHWbOnKm2zM2bN9GzZ0+Ym5vD0tIS/fr1Q3Jysmr+zJkz0bRpU6xbtw5OTk6wsrJCQEAAHj9+DODFnluFQlHg4ePjo8o4ePAg2rRpAxMTE1SvXh1jxoxBRkaGar6TkxNmz56NgQMHwtLSEsOGDQMAbNmyBQ0bNoSRkRGcnJywePHiIt9rWloa9PT0cPz4cQBAXl4ebGxs0LJlS9UyP/30E6pXr656fuvWLfTr1w/W1tawsbFBz549kZiYqJo/aNAg+Pv7q54/fvwYAwYMgJmZGezt7fHll1/Cx8enwKHkT58+xUcffQQLCwvUqFEDq1atUs1zdnYGADRr1kytnaKjo9GiRQuYmZnB2toa3t7euHHjRpHvl4joZRx0ExEREenAmjVrYGZmhiNHjmDhwoUICwvD7t27AbwYmPbs2RMPHz5ETEwMdu/ejWvXrqF///5qGVevXsXWrVuxY8cO7NixAzExMfj8888BANWrV8fdu3dVj5MnT6JSpUpo27atat3OnTujT58++Oeff7Bx40YcPHgQwcHBaq+xaNEiNGnSBCdPnsS0adMQHx+Pfv36ISAgAGfOnMHMmTMxbdo0rF69utD3aWVlhaZNmyI6OhoAcObMGSgUCpw8eRJPnjwBAMTExKBdu3YAgJycHPj5+cHCwgIHDhxAbGwszM3N0blzZ2RnZxf6GqGhoYiNjcX27duxe/duHDhwACdOnCiw3OLFi+Hu7o6TJ09i1KhRGDlyJC5evAgAOHr0KABgz549uHv3Ln777Tc8f/4c/v7+aNeuHf755x/ExcVh2LBhUCgUr/xsiYj+i4eXExEREelA48aNMWPGDABA3bp1sWLFCkRFReF///sfoqKicObMGVy/fl21B3jt2rVo2LAhjh07Bg8PDwAvBuerV6+GhYUFAODDDz9EVFQU5s6dCz09PdjZ2QEAMjMz4e/vDy8vL9Ue9fnz52PAgAGqvcF169bFsmXL0K5dO3zzzTcwNjYGAHTo0AHjxo1T1T1gwAB07NgR06ZNAwDUq1cP58+fxxdffIFBgwYV+l59fHwQHR2N8ePHIzo6Gv/73/9w4cIFHDx4EJ07d0Z0dDQmTpwIANi4cSPy8vLw/fffqwa3ERERsLa2RnR0NDp16qSW/fjxY6xZswYbNmxAx44dVcs7ODgUqKNr164YNWoUAGDSpEn48ssvsW/fPtSvXx9VqlQBAFSqVEnVbg8fPkRaWhq6d++O2rVrAwAaNGjwik+ViKgg7ukmIiIi0oHGjRurPbe3t0dKSgoAICEhAdWrV1c75NrV1RXW1tZISEhQTXNyclINuF/O+K+PPvoIjx8/xoYNG6BUvvj5d/r0aaxevRrm5uaqh5+fH/Ly8nD9+nXVuu7u7mpZCQkJ8Pb2Vpvm7e2Ny5cvIzc3t9D32q5dOxw8eBC5ubmIiYmBj4+PaiB+584dXLlyRXU49+nTp3HlyhVYWFio6rKxsUFmZiauXr1aIPvatWvIyclBixYtVNOsrKxQv379Asv+t80VCgXs7OwKba98NjY2GDRoEPz8/NCjRw989dVXuHv3bpHLExEVhnu6iYiIiHTAwMBA7blCoUBeXp7kGXPmzMHOnTtx9OhRtQH6kydPMHz4cIwZM6ZAbo0aNVT/b2ZmVqKaCtO2bVs8fvwYJ06cwP79+zFv3jzY2dnh888/R5MmTeDg4IC6deuq6nJzc8P69esL5OTvjS6t0rR5REQExowZg8jISGzcuBFTp07F7t271c5JJyJ6FQ66iYiIiGSmQYMGuHXrFm7duqXa233+/HmkpqbC1dW12DlbtmxBWFgY/v77b9Xh0fmaN2+O8+fPo06dOiWuLTY2Vm1abGws6tWrBz09vULXsba2RuPGjbFixQoYGBjAxcUFVatWRf/+/bFjxw7V+dz5dW3cuBFVq1aFpaXla+upVasWDAwMcOzYMdUfC9LS0nDp0iXV+evFkX9V9sL21jdr1gzNmjXDlClT4OXlhQ0bNnDQTUTFxkE3ERERkcz4+vri7bffxoABA7B06VI8f/4co0aNQrt27Qoc7l2Us2fPYuDAgZg0aRIaNmyIpKQkAC8GlzY2Npg0aRJatmyJ4OBgfPzxxzAzM8P58+exe/durFixosjccePGwcPDA7Nnz0b//v0RFxeHFStW4Ouvv35lPT4+Pli+fDneffddAC8O3W7QoAE2btyI8PBw1XIDBgzAF198gZ49eyIsLAyOjo64ceMGfvvtN0ycOBGOjo5quRYWFggMDMSECRNgY2ODqlWrYsaMGVAqlSW64FnVqlVhYmKCyMhIODo6wtjYGA8fPsSqVavwzjvvwMHBARcvXsTly5cxcODAYudSxZA2a1ap17X6/9d2oPKLg24iIiIqd3ZO66brEt6IQqHAtm3bMHr0aLRt2xZKpRKdO3fG8uXLi51x/PhxPH36FHPmzMGcOXNU09u1a4fo6Gg0btwYMTEx+Oyzz9CmTRsIIVC7du0CV0h/WfPmzbFp0yZMnz4ds2fPhr29PcLCwoq8iNp/X3fp0qVqtyzz8fHB6dOn1aaZmppi//79mDRpEnr37o3Hjx+jWrVq6NixY5F7vpcsWYIRI0age/fusLS0xMSJE3Hr1i3VxeCKQ19fH8uWLUNYWBimT5+ONm3aYOPGjbhw4QLWrFmDBw8ewN7eHkFBQRg+fHixc4mIOOgmIiIijbt+/To++ugjJCcnQ09PD4cPH5bkXOGyKv/2Wf+1detWtec1atTAtm3bisyYOXNmgXt7h4SEqK5GPmjQoNcOhD08PLBr164i5//33tj/1adPH/Tp0+eV2S/z9/eHEEJt2tKlS7F06dICy9rZ2WHNmjVFZr18ezILCwu1c8AzMjIwa9Ys1X3FgcLfy6lTp9Sef/zxx/j444/Vpv3+++9F1kFEVBwcdBMREZHGDRo0CHPmzEGbNm3w8OFDGBkZ6bokKkdOnjyJCxcuoEWLFkhLS0NYWBgAoGfPnjqujIiIg24iIiLSsHPnzsHAwABt2rQB8OJcXiKpLVq0CBcvXoShoSHc3Nxw4MABVK5cWddlERHxPt1ERET0avv370ePHj3g4OAAhUJR4DBoAAgPD4eTkxOMjY3h6emJo0ePquZdvnwZ5ubm6NGjB5o3b4558+ZpsXqqCJo1a4b4+Hg8efIEDx8+xO7du/H222/ruiwiIgAcdBMREdFrZGRkoEmTJmpXmP6vjRs3IjQ0FDNmzMCJEyfQpEkT+Pn5ISUlBQDw/PlzHDhwAF9//TXi4uKwe/du7N69u8jXy8rKQnp6utqDiIiorOKgm4iIiF6pS5cumDNnDnr16lXo/CVLlmDo0KEYPHgwXF1dsXLlSpiamuLHH38EAFSrVg3u7u6oXr06jIyM0LVr1wIXsPqv+fPnw8rKSvXIv0/1q7x8gS4iXcjLy9N1CUQkQzynm4iIiEotOzsb8fHxmDJlimqaUqmEr68v4uLiALy4QnZKSgoePXoEKysr7N+//5W3XJoyZQpCQ0NVz9PT04sceBsYGEChUODevXuoUqVKie7LTCQVIQSys7Nx7949KJVKGBoa6rokIpIRDrqJiIio1O7fv4/c3FzY2tqqTbe1tcWFCxcAvLj/8bx589C2bVsIIdCpUyd07969yEwjI6NiX91cT08Pjo6O+Pfff4u8vRWRtpiamqJGjRpQKnkwKRH9Hw66iYiISOO6dOmCLl26aCTb3NwcdevWRU5OjkbyiYpDT08P+vr6PNqCiArgoJuIiIhKrXLlytDT00NycrLa9OTkZNjZ2b1Rdnh4OMLDw5Gbm/vaZfX09KCnp/dGr0dERKQJPPaFiIiISi3/nshRUVGqaXl5eYiKioKXl9cbZQcFBeH8+fM4duzYm5ZJRESkM9zTTURERK/05MkTXLlyRfX8+vXrOHXqFGxsbFCjRg2EhoYiMDAQ7u7uaNGiBZYuXYqMjAwMHjxYh1UTERHJAwfdRERE9ErHjx9H+/btVc/zryweGBiI1atXo3///rh37x6mT5+OpKQkNG3aFJGRkQUurkZERFQRcdBNREREr+Tj4/Pa+2AHBwcjODhYSxURERGVHTynm4iIiIiIiEhDOOgmIiIiWQoPD4erqys8PDx0XQoREVGpST7ozs3NxbRp0+Ds7AwTExPUrl0bs2fPVjssTQiB6dOnw97eHiYmJvD19cXly5dfmx0eHg4nJycYGxvD09MTR48eVZufmZmJoKAgVKpUCebm5ujTp4/aLUwePnyIHj16wNzcHM2aNcPJkyfV1g8KCsLixYvfsAWIiIhICrx6ORERlQeSD7oXLFiAb775BitWrEBCQgIWLFiAhQsXYvny5aplFi5ciGXLlmHlypU4cuQIzMzM4Ofnh8zMzCJzN27ciNDQUMyYMQMnTpxAkyZN4Ofnh5SUFNUyY8eOxR9//IHNmzcjJiYGd+7cQe/evVXz586di8ePH+PEiRPw8fHB0KFDVfMOHz6MI0eOICQkRNoGISIiIiIiogpL8kH3oUOH0LNnT3Tr1g1OTk5499130alTJ9VeaSEEli5diqlTp6Jnz55o3Lgx1q5dizt37mDr1q1F5i5ZsgRDhw7F4MGD4erqipUrV8LU1BQ//vgjACAtLQ0//PADlixZgg4dOsDNzQ0RERE4dOgQDh8+DABISEhAQEAA6tWrh2HDhiEhIQEAkJOTgxEjRmDlypXQ09OTukmIiIiIiIiogpJ80N2qVStERUXh0qVLAIDTp0/j4MGD6NKlC4AX9/ZMSkqCr6+vah0rKyt4enoiLi6u0Mzs7GzEx8erraNUKuHr66taJz4+Hjk5OWrLuLi4oEaNGqplmjRpgr179+L58+fYuXMnGjduDODFnncfHx+4u7u/9v1lZWUhPT1d7UFERERERERUGMkH3ZMnT0ZAQABcXFxgYGCAZs2aISQkBAMGDAAAJCUlAUCBe3fa2tqq5r3s/v37yM3NfeU6SUlJMDQ0hLW1dZHLTJ48Gfr6+qhduzZ+//13/PDDD7h8+TLWrFmDadOmYcSIEahVqxb69euHtLS0QmuZP38+rKysVI/q1auXrIGIiIiIiIiowpB80L1p0yasX78eGzZswIkTJ7BmzRosWrQIa9askfqlSszKygobNmzAjRs3EBMTA1dXVwwfPhxffPEF1q9fj2vXruHixYswNTVFWFhYoRlTpkxBWlqa6nHr1i0tvwsiIqKKgVcvJyKi8kDyQfeECRNUe7vffvttfPjhhxg7dizmz58PALCzswMAtauK5z/Pn/eyypUrQ09P75Xr2NnZITs7G6mpqcXOjYiIgLW1NXr27Ino6Gj4+/vDwMAAffv2RXR0dKHrGBkZwdLSUu1BRERE0uPVy4mIqDyQfND99OlTKJXqsXp6esjLywMAODs7w87ODlFRUar56enpOHLkCLy8vArNNDQ0hJubm9o6eXl5iIqKUq3j5uYGAwMDtWUuXryImzdvFpp77949hIWFqa6qnpubi5ycHAAvLqyWm5tbmrdPREREREREpKIvdWCPHj0wd+5c1KhRAw0bNsTJkyexZMkSfPTRRwAAhUKBkJAQzJkzB3Xr1oWzszOmTZsGBwcH+Pv7q3I6duyIXr16ITg4GAAQGhqKwMBAuLu7o0WLFli6dCkyMjIwePBgAC8OHR8yZAhCQ0NhY2MDS0tLjB49Gl5eXmjZsmWBOkNCQjBu3DhUq1YNAODt7Y1169ahU6dOWLVqFby9vaVuGiIiIiIiIqpgJB90L1++HNOmTcOoUaOQkpICBwcHDB8+HNOnT1ctM3HiRGRkZGDYsGFITU1F69atERkZCWNjY9UyV69exf3791XP+/fvj3v37mH69OlISkpC06ZNERkZqXZxtS+//BJKpRJ9+vRBVlYW/Pz88PXXXxeocefOnbhy5QrWrVunmhYcHIzjx4/D09MTLVq0wIwZM6RuGiIiIiIiIqpgJB90W1hYYOnSpVi6dGmRyygUCoSFhRV5sTIASExMLDAtODhYtee7MMbGxggPD0d4ePgra/Tz84Ofn5/aNFNTU2zatOmV6xERERERERGVhOTndBMRERFJgVcvJyKi8kDyPd1EREREUggKCkJQUBDS09NhZWWl63KIKiS/2X+Wet2d07pJWMn/kbKm0mZp6r1R+cQ93UREREREREQawkE3ERERERERkYZw0E1ERERERESkITynm4iIiIiI6A30U7qXet2dEtZB8sQ93UREREREREQawj3dREREREREVKbI8cr6ReGebiIiIpIl3qebiIjKAw66iYiISJaCgoJw/vx5HDt2TNelEBERlRoPLyciIiIiIiKNq6gXnOOgm4iIiIiIiIpUUQfLUuHh5UREREREREQawkE3ERERERERkYbw8HIiIiIiIiKZ4KHc5Q/3dBMRERERERFpCAfdRERERERERBrCQTcRERERERGRhnDQTURERLIUHh4OV1dXeHh46LoUIiKiUuOgm4iIiGQpKCgI58+fx7Fjx3RdChERUalx0E1ERERERESkIRx0ExEREREREWkIB91EREREREREGsJBNxEREREREZGGcNBNREREREREpCEcdBMRERERERFpCAfdRERERERERBrCQTcRERERERGRhnDQTURERERERKQhHHQTERERERERaQgH3UREREREREQawkE3ERERyVJ4eDhcXV3h4eGh61KIiIhKTV/XBRAREREVJigoCEFBQUhPT4eVlZWuyyEqM/xm/1nqdXdO6yZhJUQEcE83ERERERERkcZw0E1ERERERESkIRx0ExEREREREWkIB91EREREREREGsJBNxEREREREZGGcNBNREREREREpCEcdBMRERERERFpCAfdRERERERERBrCQTcRERERERGRhmhk0H379m188MEHqFSpEkxMTPD222/j+PHjqvlCCEyfPh329vYwMTGBr68vLl++/Nrc8PBwODk5wdjYGJ6enjh69Kja/MzMTAQFBaFSpUowNzdHnz59kJycrJr/8OFD9OjRA+bm5mjWrBlOnjyptn5QUBAWL178hu+eiIiIiIiI6AXJB92PHj2Ct7c3DAwM8Pfff+P8+fNYvHgx3nrrLdUyCxcuxLJly7By5UocOXIEZmZm8PPzQ2ZmZpG5GzduRGhoKGbMmIETJ06gSZMm8PPzQ0pKimqZsWPH4o8//sDmzZsRExODO3fuoHfv3qr5c+fOxePHj3HixAn4+Phg6NChqnmHDx/GkSNHEBISIm2DEBERERERUYUl+aB7wYIFqF69OiIiItCiRQs4OzujU6dOqF27NoAXe7mXLl2KqVOnomfPnmjcuDHWrl2LO3fuYOvWrUXmLlmyBEOHDsXgwYPh6uqKlStXwtTUFD/++CMAIC0tDT/88AOWLFmCDh06wM3NDRERETh06BAOHz4MAEhISEBAQADq1auHYcOGISEhAQCQk5ODESNGYOXKldDT03vl+8vKykJ6errag4iIiIiIiKgwkg+6t2/fDnd3d/Tt2xdVq1ZFs2bN8N1336nmX79+HUlJSfD19VVNs7KygqenJ+Li4grNzM7ORnx8vNo6SqUSvr6+qnXi4+ORk5OjtoyLiwtq1KihWqZJkybYu3cvnj9/jp07d6Jx48YAXux59/Hxgbu7+2vf3/z582FlZaV6VK9evQStQ0RERERERBWJ5IPua9eu4ZtvvkHdunWxc+dOjBw5EmPGjMGaNWsAAElJSQAAW1tbtfVsbW1V8152//595ObmvnKdpKQkGBoawtraushlJk+eDH19fdSuXRu///47fvjhB1y+fBlr1qzBtGnTMGLECNSqVQv9+vVDWlpaobVMmTIFaWlpqsetW7dK1kBERERERERUYehLHZiXlwd3d3fMmzcPANCsWTOcPXsWK1euRGBgoNQvVyJWVlbYsGGD2rQOHTrgiy++wPr163Ht2jVcvHgRQ4cORVhYWKEXVTMyMoKRkZG2SiYiIiIiIqIyTPI93fb29nB1dVWb1qBBA9y8eRMAYGdnBwBqVxXPf54/72WVK1eGnp7eK9exs7NDdnY2UlNTi50bEREBa2tr9OzZE9HR0fD394eBgQH69u2L6OjoYr1fIiIiIiIioqJIPuj29vbGxYsX1aZdunQJNWvWBAA4OzvDzs4OUVFRqvnp6ek4cuQIvLy8Cs00NDSEm5ub2jp5eXmIiopSrePm5gYDAwO1ZS5evIibN28Wmnvv3j2EhYVh+fLlAIDc3Fzk5OQAeHFhtdzc3NK8fSIiIpJIeHg4XF1d4eHhoetSiIiISk3yQffYsWNx+PBhzJs3D1euXMGGDRuwatUqBAUFAQAUCgVCQkIwZ84cbN++HWfOnMHAgQPh4OAAf39/VU7Hjh2xYsUK1fPQ0FB89913WLNmDRISEjBy5EhkZGRg8ODBAF4cOj5kyBCEhoZi3759iI+Px+DBg+Hl5YWWLVsWqDMkJATjxo1DtWrVALz4Y8G6deuQkJCAVatWwdvbW+qmISIiohIICgrC+fPncezYMV2XQkREVGqSn9Pt4eGB33//HVOmTEFYWBicnZ2xdOlSDBgwQLXMxIkTkZGRgWHDhiE1NRWtW7dGZGQkjI2NVctcvXoV9+/fVz3v378/7t27h+nTpyMpKQlNmzZFZGSk2sXVvvzySyiVSvTp0wdZWVnw8/PD119/XaDGnTt34sqVK1i3bp1qWnBwMI4fPw5PT0+0aNECM2bMkLppiIiIiIiIqIKRfNANAN27d0f37t2LnK9QKBAWFoawsLAil0lMTCwwLTg4GMHBwUWuY2xsjPDwcISHh7+yPj8/P/j5+alNMzU1xaZNm165HhEREREREVFJSH54ORERERERERG9wEE3ERERERERkYZw0E1ERERERESkIRx0ExEREREREWkIB91EREREREREGsJBNxEREREREZGGcNBNREREREREpCEcdBMRERERERFpCAfdRERERERERBrCQTcRERERERGRhujrugAiIiIiouLwm/1nqdfdOa2bhJX8HylrkuP7I6I3xz3dRERERERERBrCQTcRERERERGRhnDQTURERERERKQhHHQTERERERERaQgvpEZERERERFTO8MJ88sE93UREREREREQawkE3ERERERERkYZw0E1ERERERESkIRx0ExEREREREWkIB91EREREREREGsKrlxMREZHGOTk5wdLSEkqlEm+99Rb27dun65KIiIi0goNuIiIi0opDhw7B3Nxc12UQERFpFQ8vJyIiIiIiItIQDrqJiIjolfbv348ePXrAwcEBCoUCW7duLbBMeHg4nJycYGxsDE9PTxw9elRtvkKhQLt27eDh4YH169drqXIiIiLd46CbiIiIXikjIwNNmjRBeHh4ofM3btyI0NBQzJgxAydOnECTJk3g5+eHlJQU1TIHDx5EfHw8tm/fjnnz5uGff/7RVvlEREQ6xUE3ERERvVKXLl0wZ84c9OrVq9D5S5YswdChQzF48GC4urpi5cqVMDU1xY8//qhaplq1agAAe3t7dO3aFSdOnCjy9bKyspCenq72ICIiKqs46CYiIqJSy87ORnx8PHx9fVXTlEolfH19ERcXB+DFnvLHjx8DAJ48eYK9e/eiYcOGRWbOnz8fVlZWqkf16tU1+yaIiIg0iINuIiIiKrX79+8jNzcXtra2atNtbW2RlJQEAEhOTkbr1q3RpEkTtGzZEgMHDoSHh0eRmVOmTEFaWprqcevWLY2+ByIiIk3iLcOIiIhIo2rVqoXTp08Xe3kjIyMYGRlpsCIiIiLt4Z5uIiIiKrXKlStDT08PycnJatOTk5NhZ2eno6qIiIjkg3u6iYgqAKfJf5ZqvcTPu0lcCZU3hoaGcHNzQ1RUFPz9/QEAeXl5iIqKQnBw8Btlh4eHIzw8HLm5uRJUSkREpBscdBMREdErPXnyBFeuXFE9v379Ok6dOgUbGxvUqFEDoaGhCAwMhLu7O1q0aIGlS5ciIyMDgwcPfqPXDQoKQlBQENLT02FlZfWmb4OIiEgnOOgmIiKiVzp+/Djat2+veh4aGgoACAwMxOrVq9G/f3/cu3cP06dPR1JSEpo2bYrIyMgCF1cjIiKqiDjoJiIiolfy8fGBEOKVywQHB7/x4eRERETlES+kRkRERERERKQhHHQTERERERERaQgH3URERCRL4eHhcHV1hYeHh65LISIiKjUOuomIiEiWgoKCcP78eRw7dkzXpRAREZUaB91EREREREREGsKrlxMRERGRRvnN/rNU6+2c1k3iSoiItE/je7o///xzKBQKhISEqKZlZmYiKCgIlSpVgrm5Ofr06YPk5ORX5gghMH36dNjb28PExAS+vr64fPmy2jIPHz7EgAEDYGlpCWtrawwZMgRPnjxRzU9MTETbtm1hZmaGtm3bIjExUW397t27Y8uWLW/8nomIiIiIiIgADQ+6jx07hm+//RaNGzdWmz527Fj88ccf2Lx5M2JiYnDnzh307t37lVkLFy7EsmXLsHLlShw5cgRmZmbw8/NDZmamapkBAwbg3Llz2L17N3bs2IH9+/dj2LBhqvnjxo1DtWrVcOrUKdjb22P8+PGqeRs3boRSqUSfPn0kevdERET0JnghNSIiKg80Nuh+8uQJBgwYgO+++w5vvfWWanpaWhp++OEHLFmyBB06dICbmxsiIiJw6NAhHD58uNAsIQSWLl2KqVOnomfPnmjcuDHWrl2LO3fuYOvWrQCAhIQEREZG4vvvv4enpydat26N5cuX45dffsGdO3dUywQGBqJu3boYNGgQEhISAACpqamYOnUqwsPDNdUcREREVEK8kBoREZUHGht0BwUFoVu3bvD19VWbHh8fj5ycHLXpLi4uqFGjBuLi4grNun79OpKSktTWsbKygqenp2qduLg4WFtbw93dXbWMr68vlEoljhw5AgBo0qQJ9uzZg7y8POzatUu1B37ChAkICgpC9erVX/u+srKykJ6ervYgIiIiIiIiKoxGBt2//PILTpw4gfnz5xeYl5SUBENDQ1hbW6tNt7W1RVJSUqF5+dNtbW2LXCcpKQlVq1ZVm6+vrw8bGxvVMosWLcKFCxfg5OSEy5cvY9GiRdi/fz9OnTqFgQMHol+/fqhVqxZGjBiB7OzsQmuZP38+rKysVI/iDNSJiIiIiIioYpJ80H3r1i188sknWL9+PYyNjaWOfyPVqlXDjh07cPPmTezYsQOVK1fGqFGjsHLlSsyZMwcWFha4ePEiLl++jG+//bbQjClTpiAtLU31uHXrlpbfBREREREREZUVkg+64+PjkZKSgubNm0NfXx/6+vqIiYnBsmXLoK+vD1tbW2RnZyM1NVVtveTkZNjZ2RWamT/95Suc/3cdOzs7pKSkqM1//vw5Hj58WGTuvHnz0KlTJ7i5uSE6Ohp9+vSBgYEBevfujejo6ELXMTIygqWlpdqDiIiIiIiIqDCSD7o7duyIM2fO4NSpU6qHu7s7BgwYoPp/AwMDREVFqda5ePEibt68CS8vr0IznZ2dYWdnp7ZOeno6jhw5olrHy8sLqampiI+PVy2zd+9e5OXlwdPTs0BmQkICNmzYgNmzZwMAcnNzkZOTAwDIyclBbm7umzcGERERERERVWj6UgdaWFigUaNGatPMzMxQqVIl1fQhQ4YgNDQUNjY2sLS0xOjRo+Hl5YWWLVuq1nFxccH8+fPRq1cv1X2+58yZg7p168LZ2RnTpk2Dg4MD/P39AQANGjRA586dMXToUKxcuRI5OTkIDg5GQEAAHBwc1OoRQmDYsGH48ssvYWZmBgDw9vbGd999h3r16mHt2rV47733pG4aIiIiKoHw8HCEh4fzD+FERFSmafQ+3UX58ssv0b17d/Tp0wdt27aFnZ0dfvvtN7VlLl68iLS0NNXziRMnYvTo0Rg2bBg8PDzw5MkTREZGqp03vn79eri4uKBjx47o2rUrWrdujVWrVhV4/VWrVsHW1hbdu3dXTZs5cyYyMzPh6emJOnXqICgoSAPvnIiIiIqLtwwjIqLyQPI93YV5+fxoY2Nj1V+viyKEUHuuUCgQFhaGsLCwItexsbHBhg0bXlvP8OHDMXz4cLVpVatWxZ49e167LhEREREREVFx6WRPNxEREREREVFFwEE3ERERERERkYZw0E1ERERERESkIRx0ExEREREREWkIB91EREREREREGsJBNxEREclSeHg4XF1d4eHhoetSiIiISo2DbiIiIpIl3qebiIjKAw66iYiIiIiIiDSEg24iIiIiIiIiDeGgm4iIiIiIiEhDOOgmIiIiIiIi0hAOuomIiIiIiIg0hINuIiIiIiIiIg3hoJuIiIiIiIhIQzjoJiIiIiIiItIQDrqJiIhIlsLDw+Hq6goPDw9dl0JERFRqHHQTERGRLAUFBeH8+fM4duyYrkshIiIqNQ66iYiIiIiIiDSEg24iIiIiIiIiDdHXdQFEREREZZHf7D9Lve7Oad1kl6XJmoiIKjLu6SYiIiIiIiLSEA66iYiIiIiIiDSEg24iIiIiIiIiDeGgm4iIiIiIiEhDeCE1IiIiIiLSKF6Yjyoy7ukmIiIiIiIi0hAOuomIiIiIiIg0hINuIiIiIiIiIg3hoJuIiIhkKTw8HK6urvDw8NB1KURERKXGQTcRERHJUlBQEM6fP49jx47puhQiIqJS46CbiIiIiIiISEM46CYiIiIiIiLSEA66iYiIiIiIiDSEg24iIiIiIiIiDeGgm4iIiIiIiEhDOOgmIiIiIiIi0hAOuomIiIiIiIg0hINuIiIiIiIiIg3hoJuIiIiIiIhIQzjoJiIiIiIiItIQDrqJiIiIiIiINISDbiIiIiIiIiINkXzQPX/+fHh4eMDCwgJVq1aFv78/Ll68qLZMZmYmgoKCUKlSJZibm6NPnz5ITk5+Za4QAtOnT4e9vT1MTEzg6+uLy5cvqy3z8OFDDBgwAJaWlrC2tsaQIUPw5MkT1fzExES0bdsWZmZmaNu2LRITE9XW7969O7Zs2fJmDUBERERERET0/0k+6I6JiUFQUBAOHz6M3bt3IycnB506dUJGRoZqmbFjx+KPP/7A5s2bERMTgzt37qB3796vzF24cCGWLVuGlStX4siRIzAzM4Ofnx8yMzNVywwYMADnzp3D7t27sWPHDuzfvx/Dhg1TzR83bhyqVauGU6dOwd7eHuPHj1fN27hxI5RKJfr06SNhaxAREREREVFFpi91YGRkpNrz1atXo2rVqoiPj0fbtm2RlpaGH374ARs2bECHDh0AABEREWjQoAEOHz6Mli1bFsgUQmDp0qWYOnUqevbsCQBYu3YtbG1tsXXrVgQEBCAhIQGRkZE4duwY3N3dAQDLly9H165dsWjRIjg4OCAhIQFLlixB3bp1MWjQINWgOzU1FVOnTsXevXtf+/6ysrKQlZWlep6enl66hiIiItlxmvxnqddN/LybhJUQERFReSH5oPtlaWlpAAAbGxsAQHx8PHJycuDr66taxsXFBTVq1EBcXFyhg+7r168jKSlJbR0rKyt4enoiLi4OAQEBiIuLg7W1tWrADQC+vr5QKpU4cuQIevXqhSZNmmDPnj3o1KkTdu3ahcaNGwMAJkyYgKCgIFSvXv2172f+/PmYNWtW6RqDiIgkx4EyERERyZlGL6SWl5eHkJAQeHt7o1GjRgCApKQkGBoawtraWm1ZW1tbJCUlFZqTP93W1rbIdZKSklC1alW1+fr6+rCxsVEts2jRIly4cAFOTk64fPkyFi1ahP379+PUqVMYOHAg+vXrh1q1amHEiBHIzs4utJYpU6YgLS1N9bh161bJGoWIiIiKJTw8HK6urvDw8NB1KURERKWm0T3dQUFBOHv2LA4ePKjJlym2atWqYceOHarnWVlZ8PPzw5o1azBnzhxYWFjg4sWL6Ny5M7799luMHj26QIaRkRGMjIy0WTYREVGFFBQUhKCgIKSnp8PKykrX5RAREZWKxvZ0BwcHY8eOHdi3bx8cHR1V0+3s7JCdnY3U1FS15ZOTk2FnZ1doVv70l69w/t917OzskJKSojb/+fPnePjwYZG58+bNQ6dOneDm5obo6Gj06dMHBgYG6N27N6Kjo0vydomIiIiIiIgKkHxPtxACo0ePxu+//47o6Gg4OzurzXdzc4OBgQGioqJUVwq/ePEibt68CS8vr0IznZ2dYWdnh6ioKDRt2hTAiwuYHTlyBCNHjgQAeHl5ITU1FfHx8XBzcwMA7N27F3l5efD09CyQmZCQgA0bNuDUqVMAgNzcXOTk5AAAcnJykJub+8ZtQURU3vD8aSIiIqKSkXxPd1BQEH766Sds2LABFhYWSEpKQlJSEp49ewbgxQXQhgwZgtDQUOzbtw/x8fEYPHgwvLy81C6i5uLigt9//x0AoFAoEBISgjlz5mD79u04c+YMBg4cCAcHB/j7+wMAGjRogM6dO2Po0KE4evQoYmNjERwcjICAADg4OKjVKITAsGHD8OWXX8LMzAwA4O3tje+++w4JCQlYu3YtvL29pW4aIiIiIiIiqmAkH3R/8803SEtLg4+PD+zt7VWPjRs3qpb58ssv0b17d/Tp0wdt27aFnZ0dfvvtN7Wcixcvqq58DgATJ07E6NGjMWzYMHh4eODJkyeIjIyEsbGxapn169fDxcUFHTt2RNeuXdG6dWusWrWqQI2rVq2Cra0tunfvrpo2c+ZMZGZmwtPTE3Xq1EFQUJCUzUJEREREREQVkEYOL38dY2NjhIeHIzw8vNg5CoUCYWFhCAsLK3IdGxsbbNiw4bWvP3z4cAwfPlxtWtWqVbFnz57XrktERERERERUXBq9ZRgRERERERFRRcZBNxEREREREZGGcNBNREREREREpCEcdBMRERERERFpCAfdRERERERERBrCQTcRERERERGRhnDQTURERERERKQhHHQTERERERERaQgH3UREREREREQawkE3ERERERERkYZw0E1ERERERESkIRx0ExEREREREWkIB91EREREREREGsJBNxEREREREZGGcNBNREREREREpCEcdBMRERERERFpCAfdRERERERERBrCQTcRERERERGRhnDQTURERFrx9OlT1KxZE+PHj9d1KURERFrDQTcRERFpxdy5c9GyZUtdl0FERKRVHHQTERGRxl2+fBkXLlxAly5ddF0KERGRVnHQTURERK+0f/9+9OjRAw4ODlAoFNi6dWuBZcLDw+Hk5ARjY2N4enri6NGjavPHjx+P+fPna6liIiIi+eCgm4iIiF4pIyMDTZo0QXh4eKHzN27ciNDQUMyYMQMnTpxAkyZN4Ofnh5SUFADAtm3bUK9ePdSrV0+bZRMREcmCvq4LICIiInnr0qXLKw8LX7JkCYYOHYrBgwcDAFauXIk///wTP/74IyZPnozDhw/jl19+webNm/HkyRPk5OTA0tIS06dPLzQvKysLWVlZqufp6enSviEiIiIt4p5uIiIiKrXs7GzEx8fD19dXNU2pVMLX1xdxcXEAgPnz5+PWrVtITEzEokWLMHTo0CIH3PnLW1lZqR7Vq1fX+PsgIiLSFA66iYiIqNTu37+P3Nxc2Nraqk23tbVFUlJSqTKnTJmCtLQ01ePWrVtSlEpERKQTPLyciIiItGbQoEGvXcbIyAhGRkYaq8Fv9p+lXnfntG4SVkJERBUB93QTERFRqVWuXBl6enpITk5Wm56cnAw7OzsdVUVERCQfHHQTERFRqRkaGsLNzQ1RUVGqaXl5eYiKioKXl9cbZYeHh8PV1RUeHh5vWiYREZHO8PByIiIieqUnT57gypUrqufXr1/HqVOnYGNjgxo1aiA0NBSBgYFwd3dHixYtsHTpUmRkZKiuZl5aQUFBCAoKQnp6OqysrN70bRAREekEB91ERET0SsePH0f79u1Vz0NDQwEAgYGBWL16Nfr374979+5h+vTpSEpKQtOmTREZGVng4mpEREQVEQfdRERE9Eo+Pj4QQrxymeDgYAQHB2upIiIiorKDg24iIiIiqnB4FXsi0hZeSI2IiIhkiRdSIyKi8oCDbiIiIpKloKAgnD9/HseOHdN1KURERKXGQTcRERERERGRhnDQTURERERERKQhHHQTERERERERaQgH3UREREREREQawkE3ERERyRKvXk5EROUBB91EREQkS7x6ORERlQc6G3SHh4fDyckJxsbG8PT0xNGjR1+5/ObNm+Hi4gJjY2O8/fbb+Ouvv9TmCyEwffp02Nvbw8TEBL6+vrh8+bJqflZWFj788ENYWlqiXr162LNnj9r6X3zxBUaPHi3dGyQiIiIiIqIKTyeD7o0bNyI0NBQzZszAiRMn0KRJE/j5+SElJaXQ5Q8dOoT33nsPQ4YMwcmTJ+Hv7w9/f3+cPXtWtczChQuxbNkyrFy5EkeOHIGZmRn8/PyQmZkJAFi1ahXi4+MRFxeHYcOG4f3334cQAgBw/fp1fPfdd5g7d67m3zwRERERERFVGDoZdC9ZsgRDhw7F4MGD4erqipUrV8LU1BQ//vhjoct/9dVX6Ny5MyZMmIAGDRpg9uzZaN68OVasWAHgxV7upUuXYurUqejZsycaN26MtWvX4s6dO9i6dSsAICEhAe+88w4aNmyIoKAg3Lt3D/fv3wcAjBw5EgsWLIClpaVW3j8RERERERFVDPrafsHs7GzEx8djypQpqmlKpRK+vr6Ii4srdJ24uDiEhoaqTfPz81MNqK9fv46kpCT4+vqq5ltZWcHT0xNxcXEICAhAkyZNsG7dOjx79gw7d+6Evb09KleujPXr18PY2Bi9evUqVv1ZWVnIyspSPU9LSwMApKenF2v918nLelrqdV+uQaosOdb0JllyrOnlLNZUdmt6kyzWVLqs8l7Tm8jPyj+yq6zKr1+qtnmeKc3nI1WOXLJYU+myWFPZrelNsuRY08tZrKn4WW+a89p+VmjZ7du3BQBx6NAhtekTJkwQLVq0KHQdAwMDsWHDBrVp4eHhomrVqkIIIWJjYwUAcefOHbVl+vbtK/r16yeEECI7O1uMGjVKODk5CXd3d3HgwAHx4MEDUatWLXHz5k3x2Wefidq1a4tOnTqJf//9t8j6Z8yYIQDwwQcffPDBR5l53Lp1q8T9tZzcunVL523IBx988MEHH0U9XtfPan1Pt64YGBggPDxcbdrgwYMxZswYnDx5Elu3bsXp06excOFCjBkzBlu2bCk0Z8qUKWp73fPy8vDw4UNUqlQJCoVCo+8hPT0d1atXx61bt97oUHipcsp7TVJmsSbWJNcs1lR2ayoOIQQeP34MBwcHjb6Opjk4OODWrVuwsLDQaF8rx8+ZNWk/izWxJrlmsSbt1/Q6xe1ntT7orly5MvT09JCcnKw2PTk5GXZ2doWuY2dn98rl8/+bnJwMe3t7tWWaNm1aaOa+fftw7tw5fP/995gwYQK6du0KMzMz9OvXT3WueGGMjIxgZGSkNs3a2rrI5TXB0tJSkn9AUuVImSXHmqTMYk3azZEyS441SZnFmrSbI3XWq1hZWWn8NTRNqVTC0dFRa68nx8+ZNWk/izVpN0fKLDnWJGUWa9JuzusUp5/V+oXUDA0N4ebmhqioKNW0vLw8REVFwcvLq9B1vLy81JYHgN27d6uWd3Z2hp2dndoy6enpOHLkSKGZmZmZCAoKwrfffgs9PT3k5uYiJycHAJCTk4Pc3Nw3fp9EREREREREOrl6eWhoKL777jusWbMGCQkJGDlyJDIyMjB48GAAwMCBA9UutPbJJ58gMjISixcvxoULFzBz5kwcP34cwcHBAACFQoGQkBDMmTMH27dvx5kzZzBw4EA4ODjA39+/wOvPnj0bXbt2RbNmzQAA3t7e+O233/DPP/9gxYoV8Pb21nwjEBERERERUbmnk3O6+/fvj3v37mH69OlISkpC06ZNERkZCVtbWwDAzZs3oVT+398DWrVqhQ0bNmDq1Kn49NNPUbduXWzduhWNGjVSLTNx4kRkZGRg2LBhSE1NRevWrREZGQljY2O11z579iw2bdqEU6dOqaa9++67iI6ORps2bVC/fn1s2LBBsw1QSkZGRpgxY0aBw9t1lVPea5IyizWxJrlmsaayWxNJT46fM2vSfhZrYk1yzWJN2q9JKgohyvh9RIiIiIiIiIhkSieHlxMRERERERFVBBx0ExEREREREWkIB91EREREREREGsJBNxEREREREZGGcNBNREREREREpCE6uWUYlV5WVhYuX76MZ8+eoUGDBjA3Ny9xRocOHVDci9bv27dP4zmsiTVJncWaWJNcayrM48ePMWbMGERERJR4XZKenPpZKbNYE2uSaxZrYk1SZxVG130tB91lSFhYGBYsWIDMzEwAgKGhIcaMGYPPP/8cCoWi2DlNmzaVpB6pcqTMYk3azZFrFmvSbo6UWeW5pq+++qrQ6Y8fP8aaNWvg5uaGOnXqoHPnzpK8HpWc3PpZKbNYk3ZzpMySY01SZrEm7eZImSXHmuTa1/I+3WXE/PnzsXjxYixcuBAdO3YEAOzduxcTJkzAxIkTMXHixBLl3b9/Hzdv3oSLiwtMTU1LXZdUOayJNUmdxZpYk9xqqlWrVqHTc3Nz8e+//6JGjRq4e/cuBg4ciFWrVpW6ViodufazUmaxJtYk1yzWxJqkypJtXyuoTHB2dhZr164tMH3dunWiTp06Jcr65ZdfhJGRkVAoFKJy5cri+PHjQgghIiIixLp167Sew5pYk9RZrIk1ybWmwqSkpAiFQiGEEOLw4cPCxsbmjTOp5OTYz0qZxZpYk1yzWBNrkjqrMLruaznoLiOMjIzE1atXC0y/du2aMDIyKlFWrVq1xMSJE8W///4rPvzwQ9GjRw8hhBCRkZHC3d1d6zmsiTVJncWaWJNcaxJCiMzMTHHmzBlx9OhR8fjxY3H//n3h7OwshBAiOTlZKJXKEmfSm5NjPytlFmtiTXLNYk2sSeosIeTX13LQXUY4OTmJw4cPF5geGxsratasWaIsExMTce3aNSGEEAcPHhQ1atQQQghx/fp1YWFhofUc1sSapM5iTaxJrjXNmjVLmJqaCqVSKZRKpTA2NhYTJ05Uzc/NzRWnT58uUSZJQ479rJRZrIk1yTWLNbEmqbPk2NfylmFlxIgRI3Du3LkC0y9cuIDhw4eXKKt58+Y4c+YMAKBKlSp49OgRACAlJQVmZmZaz2FNrEnqLNbEmuRY0/z587Fs2TIsX74c165dw7Vr1/D111/jhx9+wMKFCwEASqUSjRs3LnYmSUeO/ayUWayJNck1izWxJimzZNvXanWIT2/k6dOn4rvvvhOhoaEiNDRUrFq1SmRkZJQ4Z8eOHaJ+/fpi3bp1YteuXcLMzEwcO3ZMeHt7i/fff1/rOayJNVWE98eaWJOU5wyTZsitn5UyizWxJrlmsSbWJGWWXPtaDrrLiLNnzwoHBwdRqVIl0aFDB9GhQwdRqVIl4eDgIP75558SZeUfavHyo2vXriIlJUXrOayJNVWE98eaWJOU5wyT9OTYz0qZxZpYk1yzWBNrkjJLrn0tbxlWRvj6+sLGxgZr1qyBiYkJACAzMxMDBw7EgwcPEBUVVeysf/75R+25oaEhatSoUeJL80uVw5pYk9RZrIk1ybEmZ2dn/PLLL/D09FSbfujQIbz//vtITEwsUR5JS479rJRZrIk1yTWLNbEmKbPk2tdy0F1GmJmZ4ejRo2jYsKHa9ISEBLi5ueHp06c6qoyIiIpjwYIFqFKlCj766CO16T/++COSk5MxZcoUHVVGAPtZIqLyQK59LS+kVkaYmpoiJSWlwPTk5ORi/QVozZo1yM7OLnL+3bt3MX/+fNStW1crOayJNUmdxZpYk1xryjdp0qQCPwIA4KOPPuKAWwbk0s9KmcWaWJNcs1gTa5I6K59s+1qdHdhOJTJixAhRu3Zt8eeff4q0tDSRlpYm/vrrL1GrVi0xbNiw166vp6cnbt26pTYtNzdX/PHHH6Jnz57CwMBANGrUSCxevFgrOayJNVWE98eaWBOVHXLpZ6XMYk2sSa5ZrIk1SZ0ldxx0lxEZGRnio48+Evr6+kKhUAiFQiH09PTE4MGDxZMnT167voeHh+jYsaPYv3+/uHbtmvjss8+Eo6OjeOutt8TIkSPFsWPHilWHVDmsiTVVhPfHmlgTlR1y6WelzGJNrEmuWayJNUmdJXccdJcxKSkp4sCBA+LAgQMlupLfnTt3xIcffiiMjIyEQqEQRkZGYsmSJSIzM7NEry9VDmtiTVJnsSbWJNeaqGzRdT8rZRZrYk1yzWJNrEnqLLnjoLuCSUlJEYsXLxYNGzYUBgYGokePHmLLli0iJydHJzmsiTVJncWaWJNca6KKobz/O2ZNrEnKLNbEmqTOkisOusuIQYMGvfJRGocPHxbDhg0TVlZWokqVKmLMmDHi5MmTOsthTaxJ6izWxJrkWhPJj5z7WSmzWBNrkmsWa2JNUmfJCQfdZUSvXr3UHt27dxfOzs7CyspK+Pv7v1H2s2fPxLp164SPj49QKBQ6z2FNrEnqLNbEmuRaE8lHWehnpcxiTaxJrlmsiTVJnSUHHHSXYXl5eWLUqFFi0aJFkmVevXpVVjlSZrEm7ebINYs1aTdHyqzyXhPJj5z7WSmzWJN2c6TMkmNNUmaxJu3mSJklx5p0SSGEELq7YRm9qUuXLsHHxwd37twp8bp79uzBiRMnYG5ujsaNG6N169alqkGqHNbEmqTOYk2sSa41Udkhh35WyizWxJrkmsWaWJPUWbKi61E/vZk///xTVKpUqUTrPHnyRLRt21YYGBiI6tWrCz09PWFtbS18fX1Famqq1nNYE2uqCO+PNbEmKpt02c9KmcWaWJNcs1gTa5I6S46Uuh70U/GMHTtW7RESEoL+/fujX79+CAgIKFHWZ599hsePH+PKlSuIiYmBiYkJUlJSYG5ujnHjxmk9hzWxporw/lgTayJ5k2M/K2UWa2JNcs1iTaxJ6ixZ0vWon4qnffv2ao+OHTuK9957T3z//ffi+fPnJcpydHQUu3btEkK8OEfC3NxcCCHEiRMnRJUqVbSew5pYk9RZrIk1ybUmki859rNSZrEm1iTXLNbEmqTOkiN9XQ/6qXj27t0rWda9e/dQr169AtMtLS2RlZWl9RzWxJqkzmJNrEmuNZF8ybGflTKLNbEmuWaxJtYkdZYc8fDyMu78+fP47LPPSrSOnZ0dbt++XWD6t99+Cw8PD63nsCbWJHUWa2JNcq2Jyh5d9rNSZrEm1iTXLNbEmqTOkiPu6S6Dbt++jZ9//hnr16/HP//8Aw8PD8ydO7fY67dt2xZ//fUXWrVqBQDIzMxE3bp1kZaWhj179mg9hzWxJqmzWBNrkmtNVDbIpZ+VMos1sSa5ZrEm1iR1lizp+vh2Kp7U1FTx/fffiw4dOgg9PT3h6uoq5syZI65du1birH///VfEx8cLIYR48OCBmDx5svjuu+/Eo0ePdJLDmliT1FmsiTXJtSaSLzn2s1JmsSbWJNcs1sSapM6SI96nu4wwMTFBpUqVEBAQgA8++ABNmzbVdUlERETlBvtZIiLSFB5eXkYYGBggOzsbz549Q0ZGxhtlzZo165XzZ8yYodUc1sSapM5iTaxJrjWRfMmxn5UyizWxJrlmsSbWJHWWHHFPdxnx9OlTbN26FevXr8fu3bvh6OiIgIAADBgwAA0bNixRVvPmzdWeZ2Rk4MaNGzAwMECdOnVw8uRJreawJtYkdRZrYk1yrYnkS479rJRZrIk1yTWLNbEmqbNkSbdHt1Np3Lt3T4SHhwsvLy+hVCpF48aN3zjzwYMHomvXruL777+XRQ5rYk1SZ7Em1iTXmkh+5NzPSpnFmliTXLNYE2uSOkvXOOgu465duyZmz54tSdapU6eEs7OzbHKkzGJN2s2RaxZr0m6OlFnlvSaSLzn2s1JmsSbt5kiZJceapMxiTdrNkTJLjjXpGu/TXcY5Oztj6tSpkmTp6enh5s2byMnJkUUOa2JNUmexJtYk15pIvuTYz0qZxZpYk1yzWBNrkjpLl3hOdxlx48YNODg4wMDAoND5x48fh6mpKVxdXbVcGRGVdRkZGdizZw/c3Nzg6Ogoi6zyXhPJD/tZItIkOfZF5bkmueGe7jLC2dkZ58+fL3L+5s2bMX369GJlxcTEvPLKrJGRkThw4IDWcuRak1KpxKhRo4qc37VrV8yfP19rOeW9JgD4+OOP8emnnxY5/88//8TKlSu1llPea8p348YN9O7dG+7u7vjll1+KvZ4ms8p7TSQ/cuxnpcySY01y7IvKe01y7IvKe0355NgXleeaZEfXx7dT8SiVSnHy5Mki52/cuFE4OTkVK0uhUIhTp04VOX/atGmiR48eWsuRa01KpVJYW1uLoKCgQuevXbtWeHh4aC2nvNckhBDOzs5i//79quc5OTnizJkzqud///13sS5oJFVOea8p37lz54SBgYE4c+aMaNy4sRg4cKB4/PhxsdfXRFZ5r4nkR479rJRZcqxJjn1Rea9Jjn1Rea8pnxz7ovJck9xw0F1GKJVK4eDgIJycnAp9ODg4CIVCUeysV/2w2Lp1q3BwcNBajpxr2r9/v3B0dCy0ozt//rywsLDQWk55r0kIIYyNjUViYqLq+dWrV4W5ubnq+ZUrV4SlpaXWcsp7TfnOnTsn9PX1hRBCZGdni3Hjxol69eqJo0ePFjtD6qzyXhPJjxz7WSmz5FqT3Pqi8l6THPui8l5TPjn2ReW5JrnR1/Wediq+Dz74ANWqVZMk6+V74f2XQqGAKOap/lLlyLWmevXqISYmBu3bt0dubi6+/vprKBQKAMDz589hYmKi1ZzyXpOVlRUeP36sep6WlobMzEzk5eVBqVQW+7OTKqe811QYAwMDLFq0CF27dkX//v0xbNgwTJ48WadZ5b0mkg859rNSZsmxJjn2ReW5Jjn2ReW9psLIsS8qzzXJAQfdZcj777+PJk2aSJK1ZMkS1KpVSzY5UmZJWRMA1KpVCwcOHICPjw969OiBpUuXwtraGlOmTEGrVq20nlOea2rcuDHWr1+vOjft119/hYWFBTZt2oSAgACsXr0aDRs21FpOea+pQ4cOEEIgIyMDubm5aN++vdp8KysrfPrpp8Xq4KTKKu81kbzJsZ+VMkuONQHy64vKc01y7IvKe01y7IvKc01yxUF3GdGuXTuYm5tLlte+fXtJflhIlSNllpQ15atRowZiY2MREBCA+vXrQwiBGjVqYNeuXTrJKa81TZ48GZ06dcLRo0ehVCpx+vRpbNiwAb169UJQUBCePHmCbdu2aS2nvNfUtGlTAMCDBw8QHx+PZs2aFVjm5U5P01nlvSaSL7n2s1JmybGmfHLqi8pzTXLsi8p7TXLsi8pzTbKlyWPXSZ4GDx4sbt68KZscKbOkrGnNmjXi2bNnBaafO3dOHDhwQDx9+lSrOeW9pnxRUVFi8ODBYsSIESIhIUEIIURCQoL48ccfxfnz57WeU95rEuLF+YAGBgYlWkfTWeW9Jirfynv/KFWWHPui8l6TEPLsi8p7TULIsy8qzzXJDe/TTUREEEKozg+US1Z5r4mIiCoWOfZF5bkmOeGgm4iIiIiIiEhDlLougLRv8ODB+PTTT2WTI2WWlDV16NABgYGBssmRMkuONUmZxZq0myNlVnmviSqG8t4/SpVV3r/v5bkmKbNYk3ZzpMySY01yxQupVUA3btxAXl6ebHKkzJKyJicnJ9jZ2ckmR8osOdYkZRZr0m6OlFnlvSaqGMp7/yhVVnn/vpfnmqTMYk3azZEyS441yRUPLyciIiIiIiLSEB5eTkRERERERKQhHHRXUImJiZg8eTLatWuH+vXro379+mjXrh0mTZqExMREree8zo0bN5CUlKT1nLi4OAQEBKBmzZowMjKCkZERatasiYCAABw6dEjrOVJl3bhxAzk5OUXOP378OM6fP6/1LIBtru0subWTlDlStZPU/8apYpCyf9RGXytVP1vSLG6DtL8NYpuX3TaX22cn1982csTDyyug/fv3o1u3bqhVqxY6duwIW1tbAEBycjL27NmDa9euYceOHfDx8dFKDgAolUqcPHkSTZo0KXT+2LFjkZKSgvXr12slBwC2bNmC999/H507dy70/f3999/YsGED+vbtq5UcKbNe106TJk3C1atX8euvv762Jimz2ObabXM5tpOUNUnVTlJ+dlQxSNk/SpUlZf8oVRa3QdrfBrHNy26by/Gzk+NvG9nS9o3BSfeaN28uJk6cWOT8CRMmiObNm2stRwghlEqlOHnyZJHz161bJ+rVq6e1HCGEqF+/vli8eHGR8xcvXixcXFy0liNl1uvaaePGjcLJyalYNUmZxTbXbpvLsZ2krEmqdpLys6OKQcr+UaosKftHqbK4DdL+NohtXnbbXI6fnRx/28gV93RXQCYmJjh16hTq169f6PyLFy+iSZMmyMzM1EoOAOjp6aF58+YwNzcvdH56ejpOnTqF3NxcreQAxXt/TZs2xbNnz7SSI2WWnp4e7OzsYGhoWOj87Oxs3L17t1hXp5Uyi22u3TaXYztJWZNU7STlZ0cVg5T9o1RZUvaPUmVxG6T9bRDbvOy2uRw/Ozn+tpEr3jKsAnJ0dERUVFSRX7Q9e/agRo0aWsvJ5+LigipVqhQ5v127dlrNqVu3Ln7++WfMnDmz0Pnr169HvXr1tJYjddYHH3yAatWqFWtZbWWxzYtPiiw5tpOUNQHStbmUnx2Vf1L2j1JmSdU/SpXFbVDxya2flTKLbc7fNprIkhvu6a6A1q1bhyFDhuDdd99Fp06d1M7j2LlzJ3799Vd8//33r71BvVQ5wIu/bp04caLI8ziKS6ocANi5cyd69uwJNzc3/O9//yvw/k6cOIGtW7eiS5cuWsmRMkvKdmKbl902l2M7SVmTHLcrVDFI2T9KlcVtELdBANu8uOTY5nL87OS4XZEt3R7dTrqye/du0aVLF2FtbS2USqVQKpXC2tpadOnSRezatUvrOc7OzuL8+fOleSsaycl37tw5MXLkSNG0aVNhZ2cn7OzsRNOmTcXIkSPFuXPntJ4jVVb79u3FlStXSvS62sgSgm2u7Sy5tZOUOVK1k9T/xqlikKp/lCpLyv5Ryixug7SXk49trr2cfHJrJ6my5PrbRo64p5uQlZUFADAyMpJFDhERUXkiZf/IvpaIqOzhoJuIiIiIiIhIQ5S6LoC0b9asWQgPD5dNDgCsWbMGW7dulU0OAAwePBiffvqpbHKkzJLys5Myi22u3Sw5tpOUNclxW0cVgxy/71L2j1JlcRuk3RyAba7tHECe7VSef9vIFQfdFdCaNWvw+++/yyYHAD766CNMmTJFNjkAcOPGDdy+fVs2OVJmSfnZSZnFNtdulhzbScqa5Lito4pBjt93KftHqbK4DdJuDsA213YOIM92Ks+/beSKh5cTERERERERaQj3dBMRERERERFpCAfdFdTly5fx0Ucfwd3dHQ0bNsSAAQNw6tQpneUAwMOHDxEWFoZ3330X3bp1w6effoo7d+7oLAcAYmJi0KFDB1SuXBlmZmbw9vbGn3/+qbMcANi0aRO8vb1hY2MDGxsbeHt7Y9OmTSXOkfKzkzKLba7dLCnbSaosqdobkOe2jioGOX7fpewfpcriNki7OQDbXNs5AH/b6CJLbjjoroAOHDiAxo0b48qVK+jRowf69euHW7duwcvLC7GxsVrPAYCzZ8/CxcUFa9euhYWFBapWrYpNmzahcePGOH/+vNZzAGDr1q3w9fWFo6MjFi9ejK+//hp16tSBv78//vjjD63nAMDnn3+OwYMHo3nz5li2bBmWLVsGNzc3BAYG4vPPPy92jpSfnZRZbHPtZknZTlJlSdXegDy3dVQxyPH7LmX/KFUWt0HazQHY5trOAfjbRhdZsqTLm4STbrRu3VqMGjWqwPTg4GDh4+Oj9RwhhOjcubPo06ePeP78uWra8+fPRd++fUX37t21niOEEM2bNxczZswoMH3WrFnCw8ND6zlCCFG1alURERFRYHpERISwtbUtdo6Un52UWWxz7WZJ2U5SZUnV3kLIc1tHFYMcv+9S9o9SZXEbpN0cIdjm2s4Rgr9tdJElRxx0V0AmJibin3/+KTD9n3/+EaamplrPEUIIc3Nzcfz48QLTT5w4ISwtLbWeI4QQxsbGIiEhocD0CxcuCGNjY63nCCGEpaWluHTpUoHply5dKtH7k/KzkzKLba7dLCnbSaosqdpbCHlu66hikOP3Xcr+UaosboO0myME21zbOULwt40usuSIh5dXQCYmJjAwMCgwXV9fH0ZGRlrPAQA9PT1YWVkVmG5hYQFRggvsS5UDAJaWlsjJySkwPTs7G+bm5lrPAYA+ffrgp59+KjB97dq16Nu3b7FzpPzspMxim2s3S8p2kipLqvYG5Lmto4pBjt93KftHqbK4DdJuDsA213YOwN82usiSI31dF0Da5+npiejoaLi4uKhN37dvHzw9PbWeAwBNmzbF4cOHUadOHbXpsbGxaNasmdZzAKBt27b4+++/8fbbb6tN/+uvv9C2bVut5wCAra0tli5dit27d6Nly5YAgLi4OJw/fx6jRo3CrFmzVMvOmDGjyBwpPzsps9jmxSNVlpTtJFWWVO0NyHNbRxWDHL/vUvaPUmVxG1Q8cuxnpcxim2s3Byjfv23kivfproDS09Px/Plz2NjYqE1/+PBhkX+91mQOACQmJiInJwd169ZVm3758mXo6+vD2dlZqzly1bx582ItJ4TAyZMni5wv5WcnZZYcsc21S6r2BuS5raOKQY7fdyn7x/Lc13IbpH1sc+3jbxvt46C7Anv69CmuXr0KAKhduzZMTU11mpPv8ePHAF4cpiaHnCtXriAhIQEA0KBBgwJ/2dd2jpSk/OykzGKbazdLynYqz20u9baOyj85ft8B6fpHqbK4DdJuDsA213YOwN82usiSFV2cSE66lZmZKUJCQoSRkZFQKpVCqVQKQ0NDMWbMGJGVlaX1HCGEyMvLE0uXLhXVqlUTCoVCKBQKUa1aNbFkyRKRl5en9RwhhEhNTRX+/v6q92VoaCgUCoV45513xKNHj7Se87L09HSRnp5eqnWl/OykzGKbazdLynbSRJu/SXsLIc9tHVUMcvy+S9k/SpXFbZB2c4Rgm2s7Rwj+ttFFlhxx0F0BffLJJ8LR0VH8/PPP4ubNm+LmzZvil19+EY6OjmL06NFazxFCiLCwMGFtbS3mz58v9u/fL/bv3y8+//xzYW1tLWbNmqX1HCGECAwMFI0aNRJxcXEiLy9P5OXlicOHD4uGDRuKDz/8UOs5Qkj3Q0fKz07KLLa5drOkbCepsqQcGMhxW0cVgxy/71L2j1JlcRuk3Rwh2ObazhGCv210kSVHHHRXQFWrVhWRkZEFpu/cuVNUrVpV6zlCCFG9enWxcePGAtM3bdokHB0dtZ4jhBBvvfWWOHjwYIHpsbGx4q233tJ6jhDS/dCR8rOTMottrt0sKdtJqiwpBwZy3NZRxSDH77uU/aNUWdwGaTdHCLa5tnOE4G8bXWTJEQfdFZCJiYk4d+5cgekJCQklus+fVDlCCGFkZCQuXrxYYPqlS5eEkZGR1nOEEMLMzEycOnWqwPSS3i9QqhwhpPuhI+VnJ2UW21y7WVK2k1RZUg4M5Lito4pBjt93KftHqbK4DdJujhBsc23nCMHfNrrIkiMOuiugNm3aiMDAQJGTk6OalpOTIwYNGiRat26t9RwhhGjWrJmYNGlSgemTJk0STZs21XqOEEJ07dpVdO7cWdy/f1817cGDB6Jz586iS5cuWs8RQrofOlJ+dlJmsc21myVlO0mVJeXAQI7bOqoY5Ph9l7J/lCqL2yDt5gjBNtd2jhD8baOLLDnioLsCio+PF5UqVRKOjo6iV69eolevXqJatWrCxsZGHDt2TOs5Qgixa9cuYWRkJFq0aCHGjh0rxo4dK1q0aCEMDQ0LPdRE0zlCCHHlyhVRv359YWpqKpo1ayaaNWsmTE1NRd26dcXly5e1niOEdD90pPzspMxim2s3S8p2kipLyoGBHLd1VDHI8fsuZf8oVRa3QdrNEYJtru0cIfjbRhdZcsRbhlVQaWlp+PHHH3Hu3DkAL245MGTIEFhbW+skBwCuXr2Kr776CufPn1dlhYSEoHbt2jrJAYDc3Fxs375d7f35+/tDT09PJzm7d+9Gjx490KRJE3h7ewMAYmNjcerUKWzfvh1+fn7FzpLys5Myi22u3Syp2kmqLCnbG5Dnto4qBjl+36XsH6XK4jZIuzkA21zbOQB/2+giS2446CYqY6T80UTFwzbXLrY3EekSt0HaxzbXPra5dnHQXQHFxMS8cn67du20mgMAN27ceOX8mjVrajUHANasWfPK+YGBgVrNkZKUn52UWWxz7WZJ2U7luc2l/OyoYpDj913K/lGqLG6DtL8NYpuX3TYvz+0tdZYccdBdAenp6UEIAYVCoTY9/59CXl6eVnNezirsn2RpanqTHACwsbFRe56Tk4OnT59CX18fpqamePTokVZzinL27FlER0cjJiYGmzdvLtY6mvrs3jSLbV48UmVJ2U6abPPStDcgz20dVQxy/L5L2T9KlcVtkPa3QWzzstvm/G1TtvtafV0XQNr38pcyJycHZ86cwaeffoo5c+ZoPQcATp48WWjWF198UaIsqXIA4OHDhwWmJSYmYvjw4Rg3bpzWc4AXG54zZ86oNor79+/Ho0eP4OrqCh8fn2LnSPnZSZnFNtdulpTtJFWWVO0NyHNbRxWDHL/vUvaPUmVxG6TdHIBtru0cgL9tdJElS1JelY3KtgMHDgh3d3fZ5AghxN9//y3at28vmxwhhDhx4oRo0KCBTnIqVaoklEqlaNSokQgODhZbtmxRu3XEm5Lys5Myi22u3Syp2rs0WZpubyHkua2jikGO33cp+0epsrgN0m6OEGxzbecIwd82usjSJaWuB/0kH5UrV1ZdTEEOOQBQp04dHDlyRDY5AKBQKHDr1i2d5NSvXx9GRkYwNjaGkZERDAwMSnW10aJI+dlJmcU2126WVO1dmixNtzcgz20dVQxy/L5L2T9KlcVtkHZzALa5tnMA/rbRRZYu8fDyCuj06dNqz4UQuHv3Lj7//HM0bdpU6znAi1sEFJY1Y8YM1K1bV+s5ALBt27ZCs1asWIHWrVtrPQd4cTuHp0+f4uDBg4iOjsb8+fPRt29fuLq6om3btli6dGmxcqT87KTMYpsXj1RZUraTVFlStTcgz20dVQxy/L5L2T9KlcVtkHZzALa5tnMA/rYprnLf12p+ZzrJjVKpFAqFQiiVSrVH69atxaVLl7Se89+s/z6USqVwdnYWcXFxWs/Jz/rvQ09PT9jb24sPPvhAJCUlaT3nZXl5eeL06dNi7ty5omrVqkKhUJSoJqk/O6my2Obay5KynTTR5m/S3vk1yW1bRxWDXL/vUvaPUvXZ3AZpLyc/i22uvZz8LP620W6WHPHq5RXQzZs31Z4rlUpUrVoVhoaGOskBgP379xeaVadOHSiVxT8LQqocuTp9+jSio6MRHR2NAwcOwMjICO3atYOPjw98fHxQr169YuVI+dlJmSVHbHPtkqq9AXlu66hikOP3Xcr+sTz3tdwGaR/bXPv420YHdD3qJ3mR6i9JUv5FKj09XVY5ubm5YteuXTrJUSqVQl9fXwQGBoqzZ8++cQ2FkfKzkyqLba7dLKnauzRZ2mhvIeS5raOKQW7fdyGk6x+lyuI2SPs5bHPt5/C3jW6ydIWDbhJJSUli6dKlokWLFiU+jEcTOUIIkZ2dLbZu3Sr69u0rTExMdJ4jhBDHjh0TISEhwt7eXhgbG+sk57PPPhOtWrUShoaGwtzcXHTq1EnMnTtXHDx4UGRnZ5e6Jik/Oymz2ObazZKqvd8kS1PtLYQ8t3VUMcjx+y5l/yhVFrdB2s0Rgm2u7Rwh+NtGF1lywEF3BfX48WOxZs0a0alTJ6Gvry/q1q0rpk+fLi5evKiTnHwxMTFi2LBhwsbGRlhaWoqBAweKyMhIneVcuXJFzJo1S9SrV0/o6+sLX19f8eOPP4q0tDSd5OR7+vSp2L17t5g6dapo3bq1MDIyEmZmZiXKkPKzkzKLba7dLCnbScosKdpbCPlu66j8k+P3XQjp+kepsrgN0m6OEGxzbecIwd82usiSGw66K6D+/fsLU1NTYWdnJz755BNx5MgRneYIIcSkSZNEjRo1hJGRkejZs6fYuHGjePbsmc5yhBDC09NTKBQK4ebmJr788ktx9+5dnea8SmZmpoiKiir28lJ+dlJmsc21myVlO2m6zUva3kLIc1tHFYMcv+9S9o9SZXEbpN0cIdjm2s4Rgr9tdJElRxx0V0BKpVI0bdq0xFcr1VTOf7POnTsni5z8rCZNmohNmzaV+oeJlDlS0sRnJ1UW21x7WVK2U3lucyk/O6oY5Pp9l7J/lKrP5jZIezn5WWxz7eXkZ/G3jXaz5KhsX2KSSiUiIgJVqlRBmzZtUK9ePcyYMQOXLl3SWQ4AzJgxAxkZGWjcuDE6deqEiIgIPH78WGc5ALB37160aNECw4cPh62tLQYNGoRdu3YhLy9PJzkA0KFDB7Rv377IR3FJ+dlJmcU2126WlO0kVZZU7Q3Ic1tHFYMcv+9S9o9SZXEbpN0cgG2u7RyAv210kSVLuh71k+7cvXtXLF26VLi7uwulUinc3d3FkiVLdJYjxIuLQnzyySfCzs5OmJiYiHfffVf89ttvOssR4v8uEPPuu+8KExMTYWdnJ8aMGaOTnLFjx6o9goODRdu2bYW1tXWpapLys5Myi22u3Syp2luKLKnbWwh5buuoYpDj913K/lGqLG6DtJsjBNtc2zlC8LeNLrLkhINuEkIIcfHiRTFjxgxRu3ZtWeTk3/4gMDBQWFhY6DwnX1pamoiIiBAdO3aURU6+sLAwMXHixDfKkOqzkzqLba7dLCnbScosKdpbCPlt66jikNv3Xcr+UcosboO0myME21zbOULwt40usnRNIYQQut7bTvQqmZmZMDY2lk2OHF29ehUtWrTAgwcPdF1KhcE21y62N5HmSNk/lte+ltsg7WObax/bXHN4TjeV2qxZs3D27Nki50dEROCff/5549cpSee9Zs0a3Llzp8icnTt34saNG29cEwA8ffoUWVlZr11u8ODB2LdvX5Hz58+fj5iYmDeq5dChQzA0NHyjjNLS1r8DgG2eT1ttXtz2BjTf5hWhvYkKo41/fyUdJGurr+U26AU59rMA25y/baRV7vtaXe9qJ+3z8fERV65cKXL+J598UqzzOZRKpahSpYo4e/ZsofNDQkLEe++9V6yaBg0aJG7evFnk/JkzZ4q5c+cWq6Y6deqIf//9t9D5H330kRgyZEixanJychLnz58vcv6oUaPEoEGDilWTmZmZ2LdvX6Hzp0+fLnr27Fmsmvz9/dUePXv2FC1atBBKpVLMnDmzWBlCSPdvQAhp/x2wzbXb5lK1d35NUrS5VO0thDy3dVQxyPH7LlU/m1+TFH0tt0Ha3waxzctum/O3Tdnua7mnuwLav3//K68yWr9+fcTGxhYrq0uXLujYsSPOnz9fYF7v3r1x8ODBYuWsXbsWDx8+LHK+tbU1/vzzz2JlOTs7o0OHDoX+Ff7999/H3r17i5Vz8+bNV/7lsXnz5oiPjy9WVkhICHr06FHoXyC7dOmCo0ePFivnrbfeUntUrlwZHTt2xK5duzBjxoxiZQDS/hsApPt3wDbXbptL2d6ANG0uVXsD8tzWUcUgx++7lP0sIE1fy22Q9rdBbPOy3eb8bVN2+1p9XRdAurFy5UrY/7/27jam6rqP4/gHEAMkIRaCJhQhjLV0jWIGcxT4oGn6KGrOtWq6WfOmGc628oHzQbVlrZzrgS3IymqlD7J7HNVoa1gBU7rBZUXe0AATEIohBL/rQdNdXGnX2XWd8/2d//+8X9u1xTntv2/vnfM953+du7lzL3ldd3f3P7694989/fTTmjdvnmpra9Xc3Kwbb7zx4nUFBQX67bffIp7p4MGDOnLkyCWvO3Xq1GWv+0979+7Vo48+qpqaGn366ae65pprLl5XXFys3t7eiGfavn27cnJyLnldX1+furq6IjrOpk2bNH/+fK1YsULvvvvutJ9jyMvL0/DwcETHaWxsjOjfi0S0bgNSdG8HNI9MtJpHq7cUnebR7C3F565DYojH+3u0Hmel6D3WsoPsdxDNg9uc5zbBfazlpDtBtbS0KD09/bLX33DDDREf66mnnlJqaqpqa2v1zjvvqKqqStJfnwspKiqK+DjPPvusUlJSLnv9FVdcEdFxUlJS9Nprr2nNmjW6/fbb9dFHH2nBggWSpG+//XbaE4P/5vfff7/sTGlpaVq5cmXEx3rooYeUmpqqlStXqrGxUffcc48k6b333lNJSUlExzh48KCGhoZ0//33S5JOnz6t/fv3q6CgQHV1dRHPIkX3NiBF73ZA88hFo3k0e0v/f/No9pbic9chMcTj/T1aj7NS9B5r2UH2O4jmwW7Oc5uA8v3+dthLTk52R44cicpxent7L/795JNPupkzZ7p7773XbdiwwWVkZLhdu3aZzpSUlDRtpnXr1rmsrCy3bds2t3PnTpeXl+e2b98e8bFi0Wnfvn0uLS3NLVmyxN15550uJSXFvf766xEd69Zbb3UNDQ3OOefGxsZcYWGhKysrc1lZWRH/d12YKRr/bReOFa3bAc0jP1Y0mker96Vm+l+bR6v3hZnibdchMcTj/T2aM0XrsZYdFPlx4u1x9lJz0fzSeG4T+bHC/FjLSXcCitYdpKioyJ05c2baZc3Nze7uu+92S5cudS+88IL5TP95h3XOuYaGBldRUeGKi4vd1q1b3fj4uOlMNTU1bmBgYNplXV1dbuvWrW7t2rXugw8+iPhY2dnZF79g4v3333cFBQXuzz//dE1NTa6wsDDi40RzScbj7YDmkTWP5kzRah6t3s7F565DYgj7/T1aj7XsoMjE4+OsczS3nonnNsF+rOV3uhPQyZMnNW/ePM2YET+fLvj888918803a9asWb5HiWuzZ8/W0aNHVVRUpM2bN2t0dFQvvviiTp8+rQULFmhsbCyi48TjbSBe0dxWtHpLNIc/8Xjb43E2MuwgezS3x3Mbe3x7eQIqLCyMuztHdXU1TwQisHDhQjU2NuqHH37Q/v37tXz5cklSb2+vrr766oiPE4+3gXhFc1vR6i3RHP7E422Px9nIsIPs0dwez23scdINBMgTTzyh5557TmVlZbruuusufvHG0aNH/6cvG8F/R3Nb9AbgEzvIHs3t0dweby8HAmZwcFAnTpzQwoUL//FbaBE9NLdFbwA+sYPs0dwezW3xSjcQMJOTk5qamtL58+d9j5IwaG6L3gB8YgfZo7k9mtvipBsIkLfeeksFBQWqqKjQtddeq/b2dknS3r17tW/fPs/ThRPNbdEbgE/sIHs0t0dze5x0AwHy+OOP6+GHH9bJkye1bNky7dixQ5I0d+5c7dq1y/N04URzW/QG4BM7yB7N7dHcHp/pBgIkIyND3333nYqKivTFF19o9erVOnHihH755RctWrRIw8PDvkcMHZrbojcAn9hB9mhuj+b2eKUbCJDy8nJ98803kqTc3FwNDg5Kkvr7+/kpmBihuS16A/CJHWSP5vZobo8fVgMC5LHHHtOWLVs0PDysvLw8TU1Nqa2tTfX19aqtrfU9XijR3Ba9AfjEDrJHc3s0t8fby4EAudxPOixbtkwvv/yycnNzjScKP5rbojcAn9hB9mhuj+b2OOkGAqSzs3Pa3zNnzlRhYaEyMjI8TRR+NLdFbwA+sYPs0dweze1x0g0AAAAAQIzwmW4gQFpaWv7x+ttuu81oksRBc1v0BuATO8geze3R3B6vdAMBkpKSIueckpKSpl1+4W48NTXlY6xQo7ktegPwiR1kj+b2aG6PnwwDAmRwcFBDQ0MaHBzU4OCg+vv79cknn6iyslIff/yx7/FCiea26A3AJ3aQPZrbo7k9XukGQuDw4cNav369Ojo6fI+SMGhui94AfGIH2aO5PZrHDq90AyGQnp6uY8eO+R4jodDcFr0B+MQOskdzezSPHb5IDQiQV155Zdrfzjn19fWpoaFBVVVVnqYKN5rbojcAn9hB9mhuj+b2eHs5ECA5OTnT/p6YmNDo6Kiqq6v19ttvKzc319Nk4UVzW/QG4BM7yB7N7dHcHm8vBwJkYGBg2v9GRkb0888/Ky0tTW1tbb7HCyWa26I3AJ/YQfZobo/m9nilGwiBzs5OrVq1St9//73vURIGzW3RG4BP7CB7NLdH89jhlW4gBEZGRtTT0+N7jIRCc1v0BuATO8geze3RPHb4IjUgQHbs2DHt7wtffHHgwAGtWLHC01ThRnNb9AbgEzvIHs3t0dweby8HAqS8vHza38nJyZozZ45qamq0adMmpaWleZosvGhui94AfGIH2aO5PZrb46QbAAAAAIAY4TPdAAAAAADECJ/pBuJcbW2tIn1DymeffRbjaRIDzW3RG4BP7CB7NLdHc7846Qbi3E033XTxnycmJvTqq6+qsLBQixcvliQdPnxYp06d0n333edpwvChuS16A/CJHWSP5vZo7hef6QYCZMOGDUpPT9czzzwz7fL6+npNTExo9+7dniYLL5rbojcAn9hB9mhuj+b2OOkGAiQ7O1tfffWVSktLp11+/PhxVVRUaGhoyM9gIUZzW/QG4BM7yB7N7dHcHl+kBgTIjBkz1N7e/rfL29ralJqa6mGi8KO5LXoD8IkdZI/m9mhuj890AwGyfv16rVu3Tp2dnaqsrJQktba2avfu3aqvr/c8XTjR3Ba9AfjEDrJHc3s0t8fby4GAeemll/T888/r+PHjkqSSkhI98sgjWrt2refJwovmtugNwCd2kD2a26O5LU66gYC6cNdNSkryPEnioLktegPwiR1kj+b2aG6Dk24AAAAAAGKEz3QDAXL99dcr0v+frLu7O8bTJAaa26I3AJ/YQfZobo/m9jjpBgJk8+bNvkdIODS3RW8APrGD7NHcHs3t8fZyAAAAAABihFe6gQBqbm5WR0eHMjMztWjRIi1ZssT3SKFHc1v0BuATO8geze3R3A4n3UCA/PHHH1q+fLlaW1uVn5+vX3/9VVdeeaVuueUWHThwQFlZWb5HDB2a26I3AJ/YQfZobo/m9pJ9DwAgctu2bdPIyIh+/PFHtbS0KD09Xf39/crMzNSWLVt8jxdKNLdFbwA+sYPs0dwezT1wAAJj/vz57tChQ84553766SeXmZnpnHOuo6PD5ebm+hwttGhui94AfGIH2aO5PZrb45VuIEDOnDmj0tLSv10+e/ZsnT9/3sNE4UdzW/QG4BM7yB7N7dHcHifdQIDk5+erp6fnb5fv2bNHFRUVHiYKP5rbojcAn9hB9mhuj+b2+CI1IECqq6v14YcfqqqqSpI0NjamkpISnTt3Ts3NzZ6nCyea26I3AJ/YQfZobo/m9vidbiBAenp61NfXp/Lycg0MDGjnzp0qLi5WXV2dsrOzfY8XSjS3RW8APrGD7NHcHs3tcdINAAAAAECM8JluIEBaWlr09ddf+x4jodDcFr0B+MQOskdzezS3xyvdQICkpKSotLRUXV1dvkdJGDS3RW8APrGD7NHcHs3t8UVqQIB0d3crNTXV9xgJhea26A3AJ3aQPZrbo7k9XukGAAAAACBGeKUbCKBjx46ptbVVvb29kv76vcXKykqVlZV5niy8aG6L3gB8YgfZo7k9mtvhpBsIkKGhIa1evVpNTU3Kzs7WnDlzJEn9/f0aHBzUHXfcoTfeeENXXXWV50nDg+a26A3AJ3aQPZrbo7k9vr0cCJCNGzeqv79fbW1tOnv2rLq6utTV1aWzZ8+qvb1dfX192rhxo+8xQ4XmtugNwCd2kD2a26O5PT7TDQRIVlaWmpubVVFRccnr29ratHTpUp07d854svCiuS16A/CJHWSP5vZobo9XuoEASU5O1vj4+GWvHx8fV3Iyd+toorktegPwiR1kj+b2aG6PmkCA1NXVac2aNTp06JAmJycvXj45OammpiY98MADuuuuuzxOGD40t0VvAD6xg+zR3B7N7fH2ciBARkdH9eCDD+rNN99UUlKScnJyJEkDAwNyzmnVqlXas2ePZs2a5XnS8KC5LXoD8IkdZI/m9mhuj5NuIIB6e3v15ZdfTvuJh8WLFys/P9/zZOFFc1v0BuATO8geze3R3A4n3QAAAAAAxAif6QYAAAAAIEY46QYAAAAAIEY46QYAAAAAIEY46QYAAAAAIEY46QYAAAAAIEY46QYAAAAAIEY46QYAAAAAIEY46QYAAAAAIEY46QYAAAAAIEb+BejIXsmfNGeEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pquant import remove_pruning_from_model\n",
    "import matplotlib.pyplot as plt\n",
    "# Remove compression layers, leaves Quantized activations in place\n",
    "model = remove_pruning_from_model(trained_model, config)\n",
    "\n",
    "# Plot remaining weights\n",
    "names = []\n",
    "remaining = []\n",
    "total_w = []\n",
    "nonzeros = []\n",
    "for n, m in trained_model.named_modules():\n",
    "    if isinstance(m, (torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Linear)):\n",
    "        names.append(n)\n",
    "        nonzero = np.count_nonzero(m.weight.detach().cpu())\n",
    "        remaining_pct = nonzero / m.weight.numel()\n",
    "        remaining.append(remaining_pct)\n",
    "        total_w.append(m.weight.numel())\n",
    "        nonzeros.append(nonzero)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].bar(range(len(names)), remaining)\n",
    "ax[0].set_xticks(range(len(names)))\n",
    "ax[0].set_xticklabels(names)\n",
    "ax[0].tick_params(axis='x', labelrotation=270)\n",
    "new_ytick = []\n",
    "for i in ax[0].get_yticklabels():\n",
    "    ytick = f\"{float(i.get_text()) * 100:.2f}%\"\n",
    "    new_ytick.append(ytick)\n",
    "ax[0].set_yticklabels(new_ytick)\n",
    "ax[0].title.set_text(\"Remaining weights per layer\")\n",
    "\n",
    "ax[1].bar(range(len(nonzeros)), total_w, color=\"lightcoral\", label=\"total weights\")\n",
    "ax[1].bar(range(len(nonzeros)), nonzeros, color=\"steelblue\", label=\"nonzero weights\")\n",
    "ax[1].set_xticks(range(len(names)))\n",
    "ax[1].set_xticklabels(names)\n",
    "ax[1].tick_params(axis='x', labelrotation=270)\n",
    "ax[1].title.set_text(\"Weights per layer\")\n",
    "ax[1].legend()\n",
    "ax[1].set_yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19fe7f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conv_patterns(model):\n",
    "    \"\"\"\n",
    "    Extracts unique binary patterns and their counts from convolutional layers\n",
    "    of a given model in a backend-agnostic way.\n",
    "\n",
    "    Args:\n",
    "        model: A Keras or PyTorch model.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are the names of convolutional layers and\n",
    "        values are another dictionary containing:\n",
    "        - 'patterns': A NumPy array of the unique binary patterns.\n",
    "        - 'counts': A NumPy array with the count for each unique pattern.\n",
    "        - 'kernel_shape': The original (height, width) of the kernel.\n",
    "    \"\"\"\n",
    "    patterns_by_layer = {}\n",
    "    backend = keras.backend.backend()\n",
    "\n",
    "    # Iterate through model layers/modules\n",
    "    if backend == \"torch\":\n",
    "        modules = model.named_modules()\n",
    "    else: # TensorFlow\n",
    "        modules = [(layer.name, layer) for layer in model.layers]\n",
    "\n",
    "    for name, module in modules:\n",
    "        is_conv_layer = False\n",
    "        if backend == \"torch\" and isinstance(module, (keras.layers.Conv2D, torch.nn.Conv2d)):\n",
    "            weight = module.weight.detach()\n",
    "            is_conv_layer = len(weight.shape) == 4\n",
    "        elif backend == \"tensorflow\" and isinstance(module, keras.layers.Conv2D):\n",
    "            weight = module.kernel\n",
    "            is_conv_layer = True\n",
    "\n",
    "        if is_conv_layer:\n",
    "            all_patterns_flat = keras.ops.reshape(weight, (weight.shape[0], -1))\n",
    "            all_patterns_binary = keras.ops.cast(all_patterns_flat > 0, dtype=\"float32\")\n",
    "\n",
    "            num_bits = all_patterns_binary.shape[1]\n",
    "            if num_bits == 0:\n",
    "                continue\n",
    "\n",
    "            powers_of_2 = keras.ops.power(2.0, keras.ops.arange(num_bits, dtype=\"float32\"))\n",
    "            hashes = keras.ops.sum(all_patterns_binary * powers_of_2, axis=1)\n",
    "\n",
    "            unique_hashes, counts = np.unique(keras.ops.convert_to_numpy(hashes), return_counts=True)\n",
    "            unique_patterns_binary = ((unique_hashes[:, None] & (1 << np.arange(num_bits))) > 0).astype(float)\n",
    "            \n",
    "            sort_indices = np.argsort(counts)[::-1]\n",
    "            patterns_by_layer[name] = {\n",
    "                'patterns': unique_patterns_binary[sort_indices],\n",
    "                'counts': counts[sort_indices],\n",
    "                'kernel_shape': (weight.shape[2], weight.shape[3])\n",
    "            }\n",
    "            print(f\"Processed layer '{name}', found {len(counts)} unique patterns.\")\n",
    "\n",
    "    return patterns_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae1039e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def visualize_conv_patterns(model, layer_name, max_patterns_to_show=16):\n",
    "    \"\"\"\n",
    "    Visualizes the dominant binary patterns for a specific convolutional layer.\n",
    "    \"\"\"\n",
    "    target_module = None\n",
    "    for name, module in model.named_modules():\n",
    "        if name == layer_name and hasattr(module, \"pruning_layer\"):\n",
    "            pruning_layer = module.pruning_layer\n",
    "            if hasattr(pruning_layer, \"metric_fn\") and \"PACAPattern\" in pruning_layer.metric_fn.__class__.__name__:\n",
    "                target_module = module\n",
    "                break\n",
    "    \n",
    "    if target_module is None:\n",
    "        print(f\"Error: Could not find a PACA-pruned layer named '{layer_name}'.\")\n",
    "        return\n",
    "        \n",
    "    metric_fn = target_module.pruning_layer.metric_fn\n",
    "    # Ensure dominant patterns are selected based on the final weights\n",
    "    metric_fn._select_dominant_patterns(target_module.weight)\n",
    "    \n",
    "    patterns = metric_fn.dominant_patterns\n",
    "    if patterns is None or patterns.shape[0] == 0:\n",
    "        print(f\"No dominant patterns found for layer '{layer_name}'.\")\n",
    "        return\n",
    "        \n",
    "    # Convert to NumPy for plotting\n",
    "    patterns_np = keras.ops.convert_to_numpy(patterns)\n",
    "    \n",
    "    # Get original kernel shape from the weight tensor\n",
    "    kernel_h, kernel_w = target_module.weight.shape[2], target_module.weight.shape[3]\n",
    "    \n",
    "    num_patterns = min(patterns_np.shape[0], max_patterns_to_show)\n",
    "    \n",
    "    # Create a subplot grid for the patterns\n",
    "    cols = math.ceil(math.sqrt(num_patterns))\n",
    "    rows = math.ceil(num_patterns / cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "    if isinstance(axes, plt.Axes):\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    \n",
    "    fig.suptitle(f\"Dominant Patterns for Layer: {layer_name} (found {patterns_np.shape[0]})\", fontsize=16)\n",
    "    \n",
    "    for i in range(num_patterns):\n",
    "        pattern_2d = patterns_np[i].reshape(kernel_h, kernel_w)\n",
    "        print(pattern_2d.shape)\n",
    "        print(pattern_2d)\n",
    "        axes[i].imshow(pattern_2d, cmap='binary', vmin=0, vmax=1)\n",
    "        axes[i].set_title(f\"Pattern {i+1}\")\n",
    "        axes[i].set_xticks([])\n",
    "        axes[i].set_yticks([])\n",
    "        \n",
    "    # Hide any unused subplots\n",
    "    for j in range(num_patterns, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "        \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251515c3-00ac-4110-b8d8-a8c9100f6e6b",
   "metadata": {},
   "source": [
    "## Add PACA prunning\n",
    "#### After pruning we will have multiple patterns, so we force all of them to have a lower num,ber of dominant patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5898e23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "batch_size": 128,
       "cosine_tmax": 200,
       "gamma": 0.1,
       "l2_decay": 0.0001,
       "label_smoothing": 0,
       "lr": 0.001,
       "lr_schedule": "multistep",
       "milestones": [
        75,
        120
       ],
       "momentum": 0.9,
       "optimizer": "sgd",
       "plot_frequency": 100,
       "pruning_parameters": {
        "beta": 0.75,
        "damping": 1,
        "disable_pruning_for_layers": [
         null
        ],
        "distance_metric": "valued_hamming",
        "enable_pruning": true,
        "epsilon": 0.001,
        "metric_type": "PACAPatternSparsity",
        "num_patterns_to_keep": 16,
        "pruning_method": "mdmm",
        "scale": 0.00001,
        "use_grad": false
       },
       "quantization_parameters": {
        "default_fractional_bits": 7,
        "default_integer_bits": 0,
        "enable_quantization": false,
        "hgq_gamma": 0.0003,
        "hgq_heterogeneous": true,
        "layer_specific": [],
        "use_high_granularity_quantization": false,
        "use_real_tanh": false,
        "use_symmetric_quantization": false
       },
       "training_parameters": {
        "epochs": 200,
        "fine_tuning_epochs": 30,
        "pretraining_epochs": 0,
        "pruning_first": false,
        "rewind": "never",
        "rounds": 1,
        "save_weights_epoch": -1
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml \n",
    "\n",
    "with open(\"pquant/configs/config_mdmm_paca.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "JSON(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8da0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = 'resnet_paca_pruned.pth'\n",
    "torch.save(trained_model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e857a26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): CompressedLayerConv2d(\n",
       "    (pruning_layer): <MDMM name=mdmm_65, built=True>\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_66, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_67, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_68, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_69, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_70, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_71, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_72, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_73, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_74, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_75, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_76, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_77, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_78, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_79, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_80, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_81, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_82, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_83, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_84, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): CompressedLayerLinear(\n",
       "    (pruning_layer): <MDMM name=mdmm_85, built=True>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pquant import add_compression_layers\n",
    "input_shape = (256,3,32,32)\n",
    "model_paca = model.to(device)\n",
    "model_paca = add_compression_layers(model_paca, config, input_shape)\n",
    "model_paca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de0c3b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_conv_patterns(model_paca, 'layer1.0.conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90c24bff-9937-4670-8cff-022ddc7a0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(model, trainloader, device, loss_func, epoch, optimizer, scheduler, *args, **kwargs):\n",
    "    first_batch = True\n",
    "    for data in trainloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.)) # 8 bits input quantization\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        losses = get_model_losses(model, torch.tensor(0.).to(device))\n",
    "        loss += losses\n",
    "        loss.backward()\n",
    "\n",
    "        if first_batch:\n",
    "            print(\"\\n ---- Checking Loss values ----\")\n",
    "            print(\"Loss:\", loss_func(outputs, labels).item())\n",
    "            print(\"Model Losses:\", losses.item())\n",
    "            print(\"--------------------------------------------------\")\n",
    "\n",
    "            for name, module in model.named_modules():\n",
    "                if \"conv1\" in name and hasattr(module, \"pruning_layer\"):\n",
    "                    pruning_layer = module.pruning_layer\n",
    "                    if hasattr(pruning_layer, \"metric_fn\") and \"PACAPattern\" in pruning_layer.metric_fn.__class__.__name__:\n",
    "                        metric_fn = pruning_layer.metric_fn\n",
    "                        num_patterns = 0\n",
    "                        if metric_fn.dominant_patterns is not None:\n",
    "                            num_patterns = metric_fn.dominant_patterns.shape[0]\n",
    "\n",
    "                        total_dist = metric_fn(module.weight).item()\n",
    "                        num_kernels = module.weight.shape[0]\n",
    "                        avg_dist = total_dist / num_kernels if num_kernels > 0 else 0\n",
    "\n",
    "                        print(f\"--- PACA Stats for {name} at Epoch {epoch} ---\")\n",
    "                        print(f\"Num Patterns: {num_patterns}, Avg Pattern Dist: {avg_dist:.4f}\")\n",
    "                        print(\"--------------------------------------------------\\n\")\n",
    "                        break\n",
    "            first_batch = False\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch += 1\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "def validate_resnet(model, testloader, device, loss_func, epoch, *args, **kwargs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_paca_patterns = 0\n",
    "    avg_paca_dist = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.))\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for name, module in model.named_modules():\n",
    "            if \"conv1\" in name and hasattr(module, \"pruning_layer\"):\n",
    "                pruning_layer = module.pruning_layer\n",
    "                if hasattr(pruning_layer, \"metric_fn\") and \"PACAPattern\" in pruning_layer.metric_fn.__class__.__name__:\n",
    "                    metric_fn = pruning_layer.metric_fn\n",
    "                    if metric_fn.dominant_patterns is not None:\n",
    "                        num_paca_patterns = metric_fn.dominant_patterns.shape[0]\n",
    "\n",
    "                    total_dist = metric_fn(module.weight).item()\n",
    "                    num_kernels = module.weight.shape[0]\n",
    "                    avg_paca_dist = total_dist / num_kernels if num_kernels > 0 else 0\n",
    "                    break\n",
    "\n",
    "        ratio = get_layer_keep_ratio(model)\n",
    "        print(f'Accuracy: {100 * correct / total:.2f}%, '\n",
    "              f'Remaining Weights: {ratio * 100:.2f}%, '\n",
    "              f'Num Patterns: {num_paca_patterns}, '\n",
    "              f'Avg Pattern Dist: {avg_paca_dist:.4f}')\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader, val_loader = get_cifar10_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28865b-afdb-4773-be30-717486d9786a",
   "metadata": {},
   "source": [
    "## Create loss function, scheduler and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f88af88-ef7a-4d30-8cff-f39225d5a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850c23a-2abc-4904-9c69-859492b450a8",
   "metadata": {},
   "source": [
    "## Train model\n",
    "Training time. We use the train_compressed_model function from pquant to train. We need to provide some parameters such as training and validation functions, their input parameters, the model and the config file. The function automatically adds pruning layers and replaces activations with a quantized variant, trains the model, and removes the pruning layers after training is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18f2414c-1f4d-4a30-a143-920497e60ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.37667882442474365\n",
      "Model Losses: 0.03248338773846626\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 0 ---\n",
      "Num Patterns: 16, Avg Pattern Dist: 0.2495\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 86.72%, Remaining Weights: 20.65%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.4868629276752472\n",
      "Model Losses: 0.0021182468626648188\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 197 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 79.86%, Remaining Weights: 22.02%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.6703610420227051\n",
      "Model Losses: 0.001172768184915185\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 394 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 87.81%, Remaining Weights: 23.09%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.47378596663475037\n",
      "Model Losses: 0.0006894643884152174\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 591 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 83.18%, Remaining Weights: 23.61%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.6249768733978271\n",
      "Model Losses: 0.0004973060567863286\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 788 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 88.61%, Remaining Weights: 24.40%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.42705056071281433\n",
      "Model Losses: 0.0003381596179679036\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 985 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 82.88%, Remaining Weights: 24.72%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.4587056636810303\n",
      "Model Losses: 0.00027241616044193506\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1182 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 89.41%, Remaining Weights: 25.36%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.3738464117050171\n",
      "Model Losses: 0.00019937740580644459\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1379 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 85.59%, Remaining Weights: 25.56%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.4781234562397003\n",
      "Model Losses: 0.00017105134611483663\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1576 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 90.02%, Remaining Weights: 26.16%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.4472445547580719\n",
      "Model Losses: 0.00013130917795933783\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1773 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 84.00%, Remaining Weights: 26.29%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.36218154430389404\n",
      "Model Losses: 0.00011728944082278758\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1970 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 90.39%, Remaining Weights: 26.84%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.3238925635814667\n",
      "Model Losses: 9.286196291213855e-05\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 2167 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 86.71%, Remaining Weights: 26.93%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.3664231598377228\n",
      "Model Losses: 8.519600669387728e-05\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 2364 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iterative_train\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mInputs to train_resnet we defined previously are:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m          model, trainloader, device, loss_func, epoch, optimizer, scheduler, **kwargs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m trained_model_paca \u001b[38;5;241m=\u001b[39m \u001b[43miterative_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_paca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_resnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvalid_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidate_resnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/train.py:8\u001b[0m, in \u001b[0;36miterative_train\u001b[0;34m(model, config, train_func, valid_func, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mbackend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_impl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_torch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iterative_train_torch\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterative_train_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_impl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iterative_train_tf\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/train_torch.py:37\u001b[0m, in \u001b[0;36miterative_train_torch\u001b[0;34m(model, config, train_func, valid_func, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m train_func(model, epoch\u001b[38;5;241m=\u001b[39mepoch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 37\u001b[0m \u001b[43mvalid_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m post_epoch_functions(model, e, training_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     39\u001b[0m epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[37], line 55\u001b[0m, in \u001b[0;36mvalidate_resnet\u001b[0;34m(model, testloader, device, loss_func, epoch, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     54\u001b[0m inputs \u001b[38;5;241m=\u001b[39m quantizer(inputs, k\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.\u001b[39m), i\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m), f\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m7.\u001b[39m))\n\u001b[0;32m---> 55\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     57\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torchvision/models/resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/compressed_layers_torch.py:161\u001b[0m, in \u001b[0;36mCompressedLayerConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 161\u001b[0m     weight, bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune_and_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpruning_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwanda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpruning_layer\u001b[38;5;241m.\u001b[39mcollect_input(x, weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/compressed_layers_torch.py:119\u001b[0m, in \u001b[0;36mCompressedLayerBase.prune_and_quantize\u001b[0;34m(self, weight, bias)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     weight, bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantize(weight, bias)\n\u001b[0;32m--> 119\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weight, bias\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/compressed_layers_torch.py:110\u001b[0m, in \u001b[0;36mCompressedLayerBase.prune\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprune\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_pruning:\n\u001b[0;32m--> 110\u001b[0m         weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpruning_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weight\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/layers/layer.py:936\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/backend/torch/layer.py:41\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/ops/operation.py:58\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     54\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     55\u001b[0m         call_fn,\n\u001b[1;32m     56\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     57\u001b[0m     )\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:516\u001b[0m, in \u001b[0;36mMDMM.call\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    514\u001b[0m         weight \u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_hard_mask(weight)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstraint_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weight\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/layers/layer.py:909\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    905\u001b[0m                 kwargs[expected_mask_arg_name] \u001b[38;5;241m=\u001b[39m mask\n\u001b[1;32m    907\u001b[0m \u001b[38;5;66;03m# We need to cache the `previous_mask` before `__call__` because the\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;66;03m# mask might be removed during the call, such as `MultiHeadAttention`.\u001b[39;00m\n\u001b[0;32m--> 909\u001b[0m previous_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_keras_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst_arg\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;66;03m####################\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# 7. Call the layer.\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/tree/tree_api.py:192\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.tree.map_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructures):\n\u001b[1;32m    164\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Maps `func` through given structures.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m            `assert_same_structure`.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/tree/optree_impl.py:111\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    109\u001b[0m map_func \u001b[38;5;241m=\u001b[39m func_with_check \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(structures) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m func\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnone_is_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkeras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/optree/ops.py:766\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    764\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[1;32m    765\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[0;32m--> 766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/backend/common/masking.py:26\u001b[0m, in \u001b[0;36mget_keras_mask\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_keras_mask\u001b[39m(x):\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets the Keras mask attribute from the given tensor.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m        has been set.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_tensor_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_keras_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/backend/common/tensor_attributes.py:31\u001b[0m, in \u001b[0;36mget_tensor_attr\u001b[0;34m(tensor, attr)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_tensor_attr\u001b[39m(tensor, attr):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(tensor, attr):\n\u001b[0;32m---> 31\u001b[0m         attr_dict \u001b[38;5;241m=\u001b[39m \u001b[43mglobal_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_global_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mattr\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m attr_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m attr_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mid\u001b[39m(tensor), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/backend/common/global_state.py:15\u001b[0m, in \u001b[0;36mget_global_attribute\u001b[0;34m(name, default, set_to_default)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_global_attribute\u001b[39m(name, value):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(GLOBAL_STATE_TRACKER, name, value)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_global_attribute\u001b[39m(name, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, set_to_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     16\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(GLOBAL_STATE_TRACKER, name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pquant import iterative_train\n",
    "\"\"\"\n",
    "Inputs to train_resnet we defined previously are:\n",
    "          model, trainloader, device, loss_func, epoch, optimizer, scheduler, **kwargs\n",
    "\"\"\"\n",
    "\n",
    "trained_model_paca = iterative_train(model = model_paca, \n",
    "                                config = config, \n",
    "                                train_func = train_resnet, \n",
    "                                valid_func = validate_resnet, \n",
    "                                trainloader = train_loader, \n",
    "                                testloader = val_loader, \n",
    "                                device = device, \n",
    "                                loss_func = loss_function,\n",
    "                                optimizer = optimizer, \n",
    "                                scheduler = scheduler\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30cdc72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_conv_patterns(model_paca, 'layer1.0.conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f651848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pquant-gpu-env]",
   "language": "python",
   "name": "conda-env-pquant-gpu-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
