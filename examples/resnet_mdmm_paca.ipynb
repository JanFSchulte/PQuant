{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5676e100-c255-4871-b167-01a788309112",
   "metadata": {},
   "source": [
    "## In this tutorial we create a CNN and dataloaders, and train / prune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27197caf-85a2-48b7-af76-a5ff943408ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\" # Needs to be set, some pruning layers as well as the quantizers are Keras\n",
    "import keras\n",
    "keras.config.set_backend(\"torch\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "keras.backend.set_image_data_format(\"channels_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e520e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pquant\n",
      "data\n",
      "smartpixels\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(\"/home/das214/PQuant/mdmm_dev/src\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for f in os.listdir(os.getcwd()):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea5a763-a029-495d-a03a-390048d749f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd71c9-86b2-4911-aa71-bd18b1e75aa1",
   "metadata": {},
   "source": [
    "## Add pruning and quantization\n",
    "Begin prunning with MDMM pruning with Unstructured Sparsity metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec145f1-502c-4fd0-84ed-e87b84a27374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "batch_size": 128,
       "cosine_tmax": 200,
       "gamma": 0.1,
       "l2_decay": 0.0001,
       "label_smoothing": 0,
       "lr": 0.001,
       "lr_schedule": "multistep",
       "milestones": [
        75,
        120
       ],
       "momentum": 0.9,
       "optimizer": "sgd",
       "plot_frequency": 100,
       "pruning_parameters": {
        "constraint_type": "Equality",
        "damping": 1,
        "disable_pruning_for_layers": [
         null
        ],
        "enable_pruning": true,
        "epsilon": 0.001,
        "l0_mode": "coarse",
        "metric_type": "UnstructuredSparsity",
        "pruning_method": "mdmm",
        "rf": 1,
        "scale": 50,
        "target_sparsity": 0.9,
        "target_value": 0,
        "use_grad": false
       },
       "quantization_parameters": {
        "default_fractional_bits": 7,
        "default_integer_bits": 0,
        "enable_quantization": false,
        "hgq_gamma": 0.0003,
        "hgq_heterogeneous": true,
        "layer_specific": [],
        "use_high_granularity_quantization": false,
        "use_real_tanh": false,
        "use_symmetric_quantization": false
       },
       "training_parameters": {
        "epochs": 1000,
        "fine_tuning_epochs": 30,
        "pretraining_epochs": 0,
        "pruning_first": false,
        "rewind": "never",
        "rounds": 1,
        "save_weights_epoch": -1
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pquant import get_default_config\n",
    "from IPython.display import JSON\n",
    "\n",
    "pruning_method = \"mdmm\"\n",
    "config = get_default_config(pruning_method)\n",
    "JSON(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ef3115-2f3d-43e1-a199-4a19d667f796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): CompressedLayerConv2d(\n",
       "    (pruning_layer): <MDMM name=mdmm, built=True>\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_1, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_2, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_3, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_4, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_5, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_6, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_7, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_8, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_9, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_10, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_11, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_12, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_13, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_14, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_15, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_16, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_17, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_18, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_19, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): CompressedLayerLinear(\n",
       "    (pruning_layer): <MDMM name=mdmm_20, built=True>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace layers with compressed layers\n",
    "from pquant import add_compression_layers\n",
    "input_shape = (256,3,32,32)\n",
    "model = add_compression_layers(model, config, input_shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82dd0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "def get_cifar10_data(batch_size):\n",
    "    normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), \n",
    "                                          transforms.ToTensor(), normalize])\n",
    "    test_transform = transforms.Compose([transforms.ToTensor(), normalize])  \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "    valset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=test_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "from quantizers.fixed_point.fixed_point_ops import get_fixed_quantizer\n",
    "# Set up input quantizer\n",
    "quantizer = get_fixed_quantizer(overflow_mode=\"SAT\")\n",
    "\n",
    "\n",
    "def train_resnet(model, trainloader, device, loss_func, epoch, optimizer, scheduler, *args, **kwargs):\n",
    "    first_batch = True\n",
    "    for data in trainloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.)) # 8 bits input quantization\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        losses = get_model_losses(model, torch.tensor(0.).to(device))\n",
    "        loss += losses\n",
    "        loss.backward()\n",
    "        \n",
    "        if first_batch:\n",
    "            print(\"\\n ---- Checking Loss values ----\")\n",
    "            print(\"Loss:\", loss_func(outputs, labels).item())\n",
    "            print(\"Model Losses:\", losses.item())\n",
    "            print(\"--------------------------------------------------\")\n",
    "            print(f\"\\n--- Checking Gradients for lmbda at Epoch {epoch} ---\")\n",
    "            for name, module in model.named_modules():\n",
    "                if \"pruning_layer.constraint_layer.module\" in name and hasattr(module, \"lmbda\"):\n",
    "                    # Access the underlying tensor via the '.value' attribute\n",
    "                    underlying_tensor = module.lmbda.value\n",
    "                    grad_value = underlying_tensor.grad\n",
    "                    print(f\"Layer: {name}, lmbda: {underlying_tensor.item()}\")\n",
    "                    if grad_value is not None:\n",
    "                        print(f\"Layer: {name}, Gradient: {grad_value.item()}\")\n",
    "                    else:\n",
    "                        print(f\"Layer: {name}, Gradient: None\")\n",
    "                        \n",
    "                    break\n",
    "            print(\"--------------------------------------------------\\n\")\n",
    "            first_batch = False\n",
    "            \n",
    "        optimizer.step()\n",
    "        epoch += 1\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "from pquant import get_layer_keep_ratio, get_model_losses\n",
    "\n",
    "def validate_resnet(model, testloader, device, loss_func, epoch, *args, **kwargs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.)) # 8 bits input quantization\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        ratio = get_layer_keep_ratio(model)\n",
    "        print(f'Accuracy: {100 * correct / total:.2f}%, remaining_weights: {ratio * 100:.2f}%')\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader, val_loader = get_cifar10_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cff4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e20af1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 7.521060943603516\n",
      "Model Losses: 0.6291580200195312\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Checking Gradients for lmbda at Epoch 0 ---\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 48.75%, remaining_weights: 96.49%\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 1.4278274774551392\n",
      "Model Losses: 21.131654739379883\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Checking Gradients for lmbda at Epoch 197 ---\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pquant import iterative_train\n",
    "\"\"\"\n",
    "Inputs to train_resnet we defined previously are:\n",
    "          model, trainloader, device, loss_func, epoch, optimizer, scheduler, **kwargs\n",
    "\"\"\"\n",
    "\n",
    "trained_model = iterative_train(model = model, \n",
    "                                config = config, \n",
    "                                train_func = train_resnet, \n",
    "                                valid_func = validate_resnet, \n",
    "                                trainloader = train_loader, \n",
    "                                testloader = val_loader, \n",
    "                                device = device, \n",
    "                                loss_func = loss_function,\n",
    "                                optimizer = optimizer, \n",
    "                                scheduler = scheduler\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd70fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pquant import remove_pruning_from_model\n",
    "import matplotlib.pyplot as plt\n",
    "# Remove compression layers, leaves Quantized activations in place\n",
    "model = remove_pruning_from_model(trained_model, config)\n",
    "\n",
    "# Plot remaining weights\n",
    "names = []\n",
    "remaining = []\n",
    "total_w = []\n",
    "nonzeros = []\n",
    "for n, m in trained_model.named_modules():\n",
    "    if isinstance(m, (torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Linear)):\n",
    "        names.append(n)\n",
    "        nonzero = np.count_nonzero(m.weight.detach().cpu())\n",
    "        remaining_pct = nonzero / m.weight.numel()\n",
    "        remaining.append(remaining_pct)\n",
    "        total_w.append(m.weight.numel())\n",
    "        nonzeros.append(nonzero)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].bar(range(len(names)), remaining)\n",
    "ax[0].set_xticks(range(len(names)))\n",
    "ax[0].set_xticklabels(names)\n",
    "ax[0].tick_params(axis='x', labelrotation=270)\n",
    "new_ytick = []\n",
    "for i in ax[0].get_yticklabels():\n",
    "    ytick = f\"{float(i.get_text()) * 100:.2f}%\"\n",
    "    new_ytick.append(ytick)\n",
    "ax[0].set_yticklabels(new_ytick)\n",
    "ax[0].title.set_text(\"Remaining weights per layer\")\n",
    "\n",
    "ax[1].bar(range(len(nonzeros)), total_w, color=\"lightcoral\", label=\"total weights\")\n",
    "ax[1].bar(range(len(nonzeros)), nonzeros, color=\"steelblue\", label=\"nonzero weights\")\n",
    "ax[1].set_xticks(range(len(names)))\n",
    "ax[1].set_xticklabels(names)\n",
    "ax[1].tick_params(axis='x', labelrotation=270)\n",
    "ax[1].title.set_text(\"Weights per layer\")\n",
    "ax[1].legend()\n",
    "ax[1].set_yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1039e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def visualize_conv_patterns(model, layer_name, max_patterns_to_show=16):\n",
    "    \"\"\"\n",
    "    Visualizes the dominant binary patterns for a specific convolutional layer.\n",
    "    \"\"\"\n",
    "    target_module = None\n",
    "    for name, module in model.named_modules():\n",
    "        if name == layer_name and hasattr(module, \"pruning_layer\"):\n",
    "            pruning_layer = module.pruning_layer\n",
    "            if hasattr(pruning_layer, \"metric_fn\") and \"PACAPattern\" in pruning_layer.metric_fn.__class__.__name__:\n",
    "                target_module = module\n",
    "                break\n",
    "    \n",
    "    if target_module is None:\n",
    "        print(f\"Error: Could not find a PACA-pruned layer named '{layer_name}'.\")\n",
    "        return\n",
    "        \n",
    "    metric_fn = target_module.pruning_layer.metric_fn\n",
    "    # Ensure dominant patterns are selected based on the final weights\n",
    "    metric_fn._select_dominant_patterns(target_module.weight)\n",
    "    \n",
    "    patterns = metric_fn.dominant_patterns\n",
    "    if patterns is None or patterns.shape[0] == 0:\n",
    "        print(f\"No dominant patterns found for layer '{layer_name}'.\")\n",
    "        return\n",
    "        \n",
    "    # Convert to NumPy for plotting\n",
    "    patterns_np = keras.ops.convert_to_numpy(patterns)\n",
    "    \n",
    "    # Get original kernel shape from the weight tensor\n",
    "    kernel_h, kernel_w = target_module.weight.shape[2], target_module.weight.shape[3]\n",
    "    \n",
    "    num_patterns = min(patterns_np.shape[0], max_patterns_to_show)\n",
    "    \n",
    "    # Create a subplot grid for the patterns\n",
    "    cols = math.ceil(math.sqrt(num_patterns))\n",
    "    rows = math.ceil(num_patterns / cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "    if isinstance(axes, plt.Axes):\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    \n",
    "    fig.suptitle(f\"Dominant Patterns for Layer: {layer_name} (found {patterns_np.shape[0]})\", fontsize=16)\n",
    "    \n",
    "    for i in range(num_patterns):\n",
    "        pattern_2d = patterns_np[i].reshape(kernel_h, kernel_w)\n",
    "        print(pattern_2d.shape)\n",
    "        print(pattern_2d)\n",
    "        axes[i].imshow(pattern_2d, cmap='binary', vmin=0, vmax=1)\n",
    "        axes[i].set_title(f\"Pattern {i+1}\")\n",
    "        axes[i].set_xticks([])\n",
    "        axes[i].set_yticks([])\n",
    "        \n",
    "    # Hide any unused subplots\n",
    "    for j in range(num_patterns, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "        \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# --- How to use it in your notebook after training is finished ---\n",
    "visualize_conv_patterns(model, 'layer1.0.conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251515c3-00ac-4110-b8d8-a8c9100f6e6b",
   "metadata": {},
   "source": [
    "## Add PACA prunning\n",
    "#### After pruning we will have multiple patterns, so we force all of them to have a lower num,ber of dominant patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "with open(\"pquant/configs/config_mdmm_paca.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "JSON(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857a26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pquant import add_compression_layers\n",
    "input_shape = (256,3,32,32)\n",
    "model = add_compression_layers(model, config, input_shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c24bff-9937-4670-8cff-022ddc7a0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(model, trainloader, device, loss_func, epoch, optimizer, scheduler, *args, **kwargs):\n",
    "    first_batch = True\n",
    "    for data in trainloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.)) # 8 bits input quantization\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        losses = get_model_losses(model, torch.tensor(0.).to(device))\n",
    "        loss += losses\n",
    "        loss.backward()\n",
    "\n",
    "        if first_batch:\n",
    "            print(\"\\n ---- Checking Loss values ----\")\n",
    "            print(\"Loss:\", loss_func(outputs, labels).item())\n",
    "            print(\"Model Losses:\", losses.item())\n",
    "            print(\"--------------------------------------------------\")\n",
    "\n",
    "            for name, module in model.named_modules():\n",
    "                if \"conv1\" in name and hasattr(module, \"pruning_layer\"):\n",
    "                    pruning_layer = module.pruning_layer\n",
    "                    if hasattr(pruning_layer, \"metric_fn\") and \"PACAPattern\" in pruning_layer.metric_fn.__class__.__name__:\n",
    "                        metric_fn = pruning_layer.metric_fn\n",
    "                        num_patterns = 0\n",
    "                        if metric_fn.dominant_patterns is not None:\n",
    "                            num_patterns = metric_fn.dominant_patterns.shape[0]\n",
    "\n",
    "                        total_dist = metric_fn(module.weight).item()\n",
    "                        num_kernels = module.weight.shape[0]\n",
    "                        avg_dist = total_dist / num_kernels if num_kernels > 0 else 0\n",
    "\n",
    "                        print(f\"--- PACA Stats for {name} at Epoch {epoch} ---\")\n",
    "                        print(f\"Num Patterns: {num_patterns}, Avg Pattern Dist: {avg_dist:.4f}\")\n",
    "                        print(\"--------------------------------------------------\\n\")\n",
    "                        break\n",
    "            first_batch = False\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch += 1\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "def validate_resnet(model, testloader, device, loss_func, epoch, *args, **kwargs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_paca_patterns = 0\n",
    "    avg_paca_dist = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.))\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for name, module in model.named_modules():\n",
    "            if \"conv1\" in name and hasattr(module, \"pruning_layer\"):\n",
    "                pruning_layer = module.pruning_layer\n",
    "                if hasattr(pruning_layer, \"metric_fn\") and \"PACAPattern\" in pruning_layer.metric_fn.__class__.__name__:\n",
    "                    metric_fn = pruning_layer.metric_fn\n",
    "                    if metric_fn.dominant_patterns is not None:\n",
    "                        num_paca_patterns = metric_fn.dominant_patterns.shape[0]\n",
    "\n",
    "                    total_dist = metric_fn(module.weight).item()\n",
    "                    num_kernels = module.weight.shape[0]\n",
    "                    avg_paca_dist = total_dist / num_kernels if num_kernels > 0 else 0\n",
    "                    break\n",
    "\n",
    "        ratio = get_layer_keep_ratio(model)\n",
    "        print(f'Accuracy: {100 * correct / total:.2f}%, '\n",
    "              f'Remaining Weights: {ratio * 100:.2f}%, '\n",
    "              f'Num Patterns: {num_paca_patterns}, '\n",
    "              f'Avg Pattern Dist: {avg_paca_dist:.4f}')\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader, val_loader = get_cifar10_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28865b-afdb-4773-be30-717486d9786a",
   "metadata": {},
   "source": [
    "## Create loss function, scheduler and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f88af88-ef7a-4d30-8cff-f39225d5a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850c23a-2abc-4904-9c69-859492b450a8",
   "metadata": {},
   "source": [
    "## Train model\n",
    "Training time. We use the train_compressed_model function from pquant to train. We need to provide some parameters such as training and validation functions, their input parameters, the model and the config file. The function automatically adds pruning layers and replaces activations with a quantized variant, trains the model, and removes the pruning layers after training is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18f2414c-1f4d-4a30-a143-920497e60ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 1.023564100265503\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 0 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 70.08%, Remaining Weights: 96.59%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.9040290713310242\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 197 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 63.14%, Remaining Weights: 96.59%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.9618150591850281\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 394 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 73.10%, Remaining Weights: 96.58%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.9226782321929932\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 591 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 68.42%, Remaining Weights: 96.58%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.8731341361999512\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 788 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 74.76%, Remaining Weights: 96.57%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.8373536467552185\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 985 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 70.21%, Remaining Weights: 96.57%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.934232771396637\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1182 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 76.75%, Remaining Weights: 96.57%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.7584898471832275\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1379 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 72.12%, Remaining Weights: 96.57%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.8126096129417419\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1576 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 77.97%, Remaining Weights: 96.56%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.6571393609046936\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1773 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 75.92%, Remaining Weights: 96.56%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.7207622528076172\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1970 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 79.20%, Remaining Weights: 96.56%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.7187369465827942\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 2167 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 75.83%, Remaining Weights: 96.56%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.8391613960266113\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 2364 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 80.06%, Remaining Weights: 96.56%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.7094944715499878\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 2561 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 76.99%, Remaining Weights: 96.55%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.7293562293052673\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 2758 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 80.78%, Remaining Weights: 96.55%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.6537322998046875\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 2955 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 79.24%, Remaining Weights: 96.55%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.5786072015762329\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 3152 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 81.20%, Remaining Weights: 96.54%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.6281647086143494\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 3349 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 80.78%, Remaining Weights: 96.54%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.6206632852554321\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 3546 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 82.38%, Remaining Weights: 96.54%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.5568910241127014\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 3743 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 82.85%, Remaining Weights: 96.54%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.5789287090301514\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 3940 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 82.67%, Remaining Weights: 96.53%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.46712976694107056\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 4137 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 82.77%, Remaining Weights: 96.53%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.47319871187210083\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 4334 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 83.30%, Remaining Weights: 96.53%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.49258410930633545\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 4531 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iterative_train\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mInputs to train_resnet we defined previously are:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m          model, trainloader, device, loss_func, epoch, optimizer, scheduler, **kwargs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43miterative_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_resnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvalid_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidate_resnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/train.py:8\u001b[0m, in \u001b[0;36miterative_train\u001b[0;34m(model, config, train_func, valid_func, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mbackend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_impl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_torch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iterative_train_torch\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterative_train_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_impl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iterative_train_tf\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/train_torch.py:37\u001b[0m, in \u001b[0;36miterative_train_torch\u001b[0;34m(model, config, train_func, valid_func, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m train_func(model, epoch\u001b[38;5;241m=\u001b[39mepoch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 37\u001b[0m \u001b[43mvalid_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m post_epoch_functions(model, e, training_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     39\u001b[0m epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 82\u001b[0m, in \u001b[0;36mvalidate_resnet\u001b[0;34m(model, testloader, device, loss_func, epoch, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     81\u001b[0m inputs \u001b[38;5;241m=\u001b[39m quantizer(inputs, k\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.\u001b[39m), i\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m), f\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m7.\u001b[39m))\n\u001b[0;32m---> 82\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     84\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/compressed_layers_torch.py:161\u001b[0m, in \u001b[0;36mCompressedLayerConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 161\u001b[0m     weight, bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune_and_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpruning_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwanda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpruning_layer\u001b[38;5;241m.\u001b[39mcollect_input(x, weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/compressed_layers_torch.py:119\u001b[0m, in \u001b[0;36mCompressedLayerBase.prune_and_quantize\u001b[0;34m(self, weight, bias)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     weight, bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantize(weight, bias)\n\u001b[0;32m--> 119\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weight, bias\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/compressed_layers_torch.py:110\u001b[0m, in \u001b[0;36mCompressedLayerBase.prune\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprune\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_pruning:\n\u001b[0;32m--> 110\u001b[0m         weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpruning_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weight\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/layers/layer.py:936\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/backend/torch/layer.py:41\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/ops/operation.py:58\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     54\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     55\u001b[0m         call_fn,\n\u001b[1;32m     56\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     57\u001b[0m     )\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:513\u001b[0m, in \u001b[0;36mMDMM.call\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    511\u001b[0m         weight \u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_hard_mask(weight)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstraint_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weight\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/layers/layer.py:936\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/backend/torch/layer.py:41\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/ops/operation.py:58\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     54\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     55\u001b[0m         call_fn,\n\u001b[1;32m     56\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     57\u001b[0m     )\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:61\u001b[0m, in \u001b[0;36mConstraint.call\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight):\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculates the penalty from a given infeasibility measure.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     raw_infeasibility \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_infeasibility\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     infeasibility \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe_infeasibility(raw_infeasibility)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_grad_:\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:108\u001b[0m, in \u001b[0;36mEqualityConstraint.get_infeasibility\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_infeasibility\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight):\n\u001b[0;32m--> 108\u001b[0m     metric_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     infeasibility \u001b[38;5;241m=\u001b[39m metric_value \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_value\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# return ops.abs(infeasibility)\u001b[39;00m\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:409\u001b[0m, in \u001b[0;36mPACAPatternMetric.__call__\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_dominant_patterns(weight)\n\u001b[0;32m--> 409\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pattern_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m min_distances \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mmin(distances, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39msum(min_distances)\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:386\u001b[0m, in \u001b[0;36mPACAPatternMetric._pattern_distances\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m    384\u001b[0m     distances \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39msum(ops\u001b[38;5;241m.\u001b[39mabs(dom_patterns_exp \u001b[38;5;241m-\u001b[39m w_patterns_exp), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalued_hamming\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 386\u001b[0m     abs_diff \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdom_patterns_exp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw_patterns_exp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m     distances \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39msum(abs_diff \u001b[38;5;241m*\u001b[39m ops\u001b[38;5;241m.\u001b[39mabs(w_kernels_exp), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/ops/numpy.py:185\u001b[0m, in \u001b[0;36mabs\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.ops.abs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.ops.numpy.abs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mabs\u001b[39m(x):\n\u001b[1;32m    184\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Shorthand for `keras.ops.absolute`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mabsolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/ops/numpy.py:175\u001b[0m, in \u001b[0;36mabsolute\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Absolute()\u001b[38;5;241m.\u001b[39msymbolic_call(x)\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabsolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/backend/torch/numpy.py:249\u001b[0m, in \u001b[0;36mabsolute\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m standardize_dtype(x\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pquant import iterative_train\n",
    "\"\"\"\n",
    "Inputs to train_resnet we defined previously are:\n",
    "          model, trainloader, device, loss_func, epoch, optimizer, scheduler, **kwargs\n",
    "\"\"\"\n",
    "\n",
    "trained_model = iterative_train(model = model, \n",
    "                                config = config, \n",
    "                                train_func = train_resnet, \n",
    "                                valid_func = validate_resnet, \n",
    "                                trainloader = train_loader, \n",
    "                                testloader = val_loader, \n",
    "                                device = device, \n",
    "                                loss_func = loss_function,\n",
    "                                optimizer = optimizer, \n",
    "                                scheduler = scheduler\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cdc72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_conv_patterns(model, 'layer1.0.conv1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pquant-gpu-env]",
   "language": "python",
   "name": "conda-env-pquant-gpu-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
