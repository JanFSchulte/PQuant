{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5676e100-c255-4871-b167-01a788309112",
   "metadata": {},
   "source": [
    "## In this tutorial we create a CNN and dataloaders, and train / prune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27197caf-85a2-48b7-af76-a5ff943408ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\" # Needs to be set, some pruning layers as well as the quantizers are Keras\n",
    "import keras\n",
    "keras.config.set_backend(\"torch\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "keras.backend.set_image_data_format(\"channels_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e520e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet_paca_pruned.pth\n",
      "pquant\n",
      "data\n",
      "smartpixels\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(\"/home/das214/PQuant/mdmm_dev/src\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for f in os.listdir(os.getcwd()):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea5a763-a029-495d-a03a-390048d749f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd71c9-86b2-4911-aa71-bd18b1e75aa1",
   "metadata": {},
   "source": [
    "## Add pruning and quantization\n",
    "Begin prunning with MDMM pruning with Unstructured Sparsity metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec145f1-502c-4fd0-84ed-e87b84a27374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "batch_size": 128,
       "cosine_tmax": 200,
       "gamma": 0.1,
       "l2_decay": 0.0001,
       "label_smoothing": 0,
       "lr": 0.001,
       "lr_schedule": "multistep",
       "milestones": [
        75,
        120
       ],
       "momentum": 0.9,
       "optimizer": "sgd",
       "plot_frequency": 100,
       "pruning_parameters": {
        "constraint_type": "Equality",
        "damping": 1,
        "disable_pruning_for_layers": [
         null
        ],
        "enable_pruning": true,
        "epsilon": 0.001,
        "l0_mode": "coarse",
        "metric_type": "UnstructuredSparsity",
        "pruning_method": "mdmm",
        "rf": 1,
        "scale": 50,
        "target_sparsity": 0.9,
        "target_value": 0,
        "use_grad": false
       },
       "quantization_parameters": {
        "default_fractional_bits": 7,
        "default_integer_bits": 0,
        "enable_quantization": false,
        "hgq_gamma": 0.0003,
        "hgq_heterogeneous": true,
        "layer_specific": [],
        "use_high_granularity_quantization": false,
        "use_real_tanh": false,
        "use_symmetric_quantization": false
       },
       "training_parameters": {
        "epochs": 100,
        "fine_tuning_epochs": 30,
        "pretraining_epochs": 0,
        "pruning_first": false,
        "rewind": "never",
        "rounds": 1,
        "save_weights_epoch": -1
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pquant import get_default_config\n",
    "from IPython.display import JSON\n",
    "\n",
    "pruning_method = \"mdmm\"\n",
    "config = get_default_config(pruning_method)\n",
    "JSON(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ef3115-2f3d-43e1-a199-4a19d667f796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): CompressedLayerConv2d(\n",
       "    (pruning_layer): <MDMM name=mdmm, built=True>\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_1, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_2, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_3, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_4, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_5, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_6, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_7, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_8, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_9, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_10, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_11, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_12, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_13, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_14, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_15, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_16, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_17, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_18, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_19, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): CompressedLayerLinear(\n",
       "    (pruning_layer): <MDMM name=mdmm_20, built=True>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace layers with compressed layers\n",
    "from pquant import add_compression_layers\n",
    "input_shape = (256,3,32,32)\n",
    "model = add_compression_layers(model, config, input_shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82dd0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from pquant import get_layer_keep_ratio, get_model_losses\n",
    "from quantizers.fixed_point.fixed_point_ops import get_fixed_quantizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_cifar10_data(batch_size):\n",
    "    normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), \n",
    "                                          transforms.ToTensor(), normalize])\n",
    "    test_transform = transforms.Compose([transforms.ToTensor(), normalize])  \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "    valset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=test_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Set up input quantizer\n",
    "quantizer = get_fixed_quantizer(overflow_mode=\"SAT\")\n",
    "\n",
    "def train_resnet(model, trainloader, device, loss_func,\n",
    "                 epoch, optimizer, scheduler, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    One epoch of training with a live ETA/throughput bar.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(trainloader,\n",
    "              desc=f\"Train ‖ Epoch {epoch}\",\n",
    "              total=len(trainloader),\n",
    "              unit=\"batch\",\n",
    "              dynamic_ncols=True) as pbar:\n",
    "\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.))\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)              # cleaner gradient reset\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            losses = get_model_losses(model, torch.tensor(0.).to(device))\n",
    "            loss += losses\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f} \")\n",
    "        \n",
    "    # ----- Diagnostics on Last mini-batch -----\n",
    "    print(f\"Loss={loss_func(outputs, labels).item():.4f} | Reg={loss.item() - loss_func(outputs, labels).item():.4f}\")\n",
    "\n",
    "def validate_resnet(model, testloader, device, loss_func, epoch, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Validation with progress bar and accuracy summary.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(testloader,\n",
    "                  desc=f\"Val   ‖ Epoch {epoch}\",\n",
    "                  total=len(testloader),\n",
    "                  unit=\"batch\",\n",
    "                  dynamic_ncols=True) as pbar:\n",
    "\n",
    "            for inputs, labels in pbar:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.))\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "                running_acc = 100. * correct / total\n",
    "                pbar.set_postfix(acc=f\"{running_acc:.2f}%\")\n",
    "\n",
    "    ratio = get_layer_keep_ratio(model)\n",
    "    print(f\"Accuracy: {correct/total*100:.2f}% | Remaining weights: {ratio*100:.2f}% \\n\")\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader, val_loader = get_cifar10_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cff4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e20af1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 0: 100%|██████████| 196/196 [00:09<00:00, 20.31batch/s, loss=12.3711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.5315 | Reg=10.8397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 0: 100%|██████████| 196/196 [00:06<00:00, 32.16batch/s, acc=48.28%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.28% | Remaining weights: 96.48% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 1: 100%|██████████| 196/196 [00:09<00:00, 21.10batch/s, loss=23.3255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.3736 | Reg=21.9519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 1: 100%|██████████| 196/196 [00:05<00:00, 32.95batch/s, acc=46.14%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.14% | Remaining weights: 95.72% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 2: 100%|██████████| 196/196 [00:09<00:00, 21.28batch/s, loss=23.9611]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1378 | Reg=22.8233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 2: 100%|██████████| 196/196 [00:05<00:00, 34.31batch/s, acc=58.07%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.07% | Remaining weights: 94.27% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 3: 100%|██████████| 196/196 [00:09<00:00, 21.52batch/s, loss=25.7768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.3645 | Reg=24.4123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 3: 100%|██████████| 196/196 [00:05<00:00, 34.19batch/s, acc=43.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 43.41% | Remaining weights: 93.11% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 4: 100%|██████████| 196/196 [00:08<00:00, 21.82batch/s, loss=21.7134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1757 | Reg=20.5377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 4: 100%|██████████| 196/196 [00:05<00:00, 33.97batch/s, acc=62.14%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.14% | Remaining weights: 90.60% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 5: 100%|██████████| 196/196 [00:08<00:00, 22.39batch/s, loss=23.2727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.4143 | Reg=21.8584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 5: 100%|██████████| 196/196 [00:04<00:00, 40.26batch/s, acc=43.86%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 43.86% | Remaining weights: 89.46% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 6: 100%|██████████| 196/196 [00:07<00:00, 24.87batch/s, loss=18.9606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1539 | Reg=17.8066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 6: 100%|██████████| 196/196 [00:04<00:00, 40.46batch/s, acc=64.24%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.24% | Remaining weights: 86.23% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 7: 100%|██████████| 196/196 [00:07<00:00, 24.67batch/s, loss=20.3729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.5067 | Reg=18.8662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 7: 100%|██████████| 196/196 [00:05<00:00, 38.87batch/s, acc=48.93%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.93% | Remaining weights: 85.22% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 8: 100%|██████████| 196/196 [00:07<00:00, 24.62batch/s, loss=16.8002]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2161 | Reg=15.5840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 8: 100%|██████████| 196/196 [00:05<00:00, 37.55batch/s, acc=63.87%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.87% | Remaining weights: 81.56% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 9: 100%|██████████| 196/196 [00:08<00:00, 24.44batch/s, loss=17.9427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1397 | Reg=16.8030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 9: 100%|██████████| 196/196 [00:04<00:00, 39.69batch/s, acc=48.01%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.01% | Remaining weights: 80.87% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 10: 100%|██████████| 196/196 [00:07<00:00, 24.62batch/s, loss=14.7392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0476 | Reg=13.6917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 10: 100%|██████████| 196/196 [00:04<00:00, 39.94batch/s, acc=63.57%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.57% | Remaining weights: 76.66% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 11: 100%|██████████| 196/196 [00:07<00:00, 24.86batch/s, loss=16.0190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.3045 | Reg=14.7145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 11: 100%|██████████| 196/196 [00:04<00:00, 39.69batch/s, acc=42.76%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 42.76% | Remaining weights: 76.09% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 12: 100%|██████████| 196/196 [00:07<00:00, 24.87batch/s, loss=13.0328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9559 | Reg=12.0768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 12: 100%|██████████| 196/196 [00:04<00:00, 39.98batch/s, acc=62.49%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.49% | Remaining weights: 71.77% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 13: 100%|██████████| 196/196 [00:07<00:00, 24.58batch/s, loss=14.1266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2391 | Reg=12.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 13: 100%|██████████| 196/196 [00:04<00:00, 39.42batch/s, acc=34.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 34.41% | Remaining weights: 71.29% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 14: 100%|██████████| 196/196 [00:07<00:00, 24.62batch/s, loss=11.5139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8892 | Reg=10.6248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 14: 100%|██████████| 196/196 [00:04<00:00, 39.24batch/s, acc=62.03%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.03% | Remaining weights: 67.00% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 15: 100%|██████████| 196/196 [00:07<00:00, 24.79batch/s, loss=12.3061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0909 | Reg=11.2152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 15: 100%|██████████| 196/196 [00:04<00:00, 39.38batch/s, acc=50.66%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.66% | Remaining weights: 66.50% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 16: 100%|██████████| 196/196 [00:07<00:00, 24.87batch/s, loss=10.3932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0415 | Reg=9.3517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 16: 100%|██████████| 196/196 [00:04<00:00, 39.31batch/s, acc=61.42%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.42% | Remaining weights: 62.37% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 17: 100%|██████████| 196/196 [00:08<00:00, 24.31batch/s, loss=10.9900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.3023 | Reg=9.6877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 17: 100%|██████████| 196/196 [00:04<00:00, 39.44batch/s, acc=51.86%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.86% | Remaining weights: 61.77% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 18: 100%|██████████| 196/196 [00:07<00:00, 24.53batch/s, loss=9.4185] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1045 | Reg=8.3140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 18: 100%|██████████| 196/196 [00:05<00:00, 38.93batch/s, acc=58.09%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.09% | Remaining weights: 58.08% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 19: 100%|██████████| 196/196 [00:07<00:00, 25.00batch/s, loss=9.4961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1256 | Reg=8.3705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 19: 100%|██████████| 196/196 [00:05<00:00, 37.83batch/s, acc=55.23%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.23% | Remaining weights: 57.28% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 20: 100%|██████████| 196/196 [00:07<00:00, 25.49batch/s, loss=8.3266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0060 | Reg=7.3207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 20: 100%|██████████| 196/196 [00:05<00:00, 38.95batch/s, acc=61.26%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.26% | Remaining weights: 53.98% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 21: 100%|██████████| 196/196 [00:07<00:00, 25.34batch/s, loss=8.0770]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8683 | Reg=7.2087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 21: 100%|██████████| 196/196 [00:05<00:00, 39.02batch/s, acc=59.75%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.75% | Remaining weights: 52.99% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 22: 100%|██████████| 196/196 [00:07<00:00, 25.26batch/s, loss=7.5430]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0895 | Reg=6.4535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 22: 100%|██████████| 196/196 [00:04<00:00, 39.22batch/s, acc=62.87%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.87% | Remaining weights: 50.19% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 23: 100%|██████████| 196/196 [00:07<00:00, 25.75batch/s, loss=7.4162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2231 | Reg=6.1931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 23: 100%|██████████| 196/196 [00:05<00:00, 39.19batch/s, acc=56.37%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.37% | Remaining weights: 49.00% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 24: 100%|██████████| 196/196 [00:07<00:00, 25.42batch/s, loss=6.9230]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1302 | Reg=5.7929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 24: 100%|██████████| 196/196 [00:04<00:00, 39.52batch/s, acc=55.55%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.55% | Remaining weights: 46.84% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 25: 100%|██████████| 196/196 [00:07<00:00, 25.77batch/s, loss=6.3547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0166 | Reg=5.3381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 25: 100%|██████████| 196/196 [00:05<00:00, 38.97batch/s, acc=55.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.50% | Remaining weights: 45.35% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 26: 100%|██████████| 196/196 [00:07<00:00, 25.63batch/s, loss=6.1669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0644 | Reg=5.1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 26: 100%|██████████| 196/196 [00:05<00:00, 39.03batch/s, acc=61.42%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.42% | Remaining weights: 43.63% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 27: 100%|██████████| 196/196 [00:07<00:00, 25.65batch/s, loss=5.6282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0677 | Reg=4.5604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 27: 100%|██████████| 196/196 [00:04<00:00, 39.39batch/s, acc=65.11%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.11% | Remaining weights: 42.05% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 28: 100%|██████████| 196/196 [00:07<00:00, 25.60batch/s, loss=5.5488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0025 | Reg=4.5463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 28: 100%|██████████| 196/196 [00:05<00:00, 38.93batch/s, acc=50.94%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.94% | Remaining weights: 40.85% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 29: 100%|██████████| 196/196 [00:07<00:00, 25.50batch/s, loss=4.8334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9353 | Reg=3.8982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 29: 100%|██████████| 196/196 [00:04<00:00, 39.26batch/s, acc=67.15%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.15% | Remaining weights: 39.12% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 30: 100%|██████████| 196/196 [00:07<00:00, 25.48batch/s, loss=5.3500]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2216 | Reg=4.1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 30: 100%|██████████| 196/196 [00:04<00:00, 39.26batch/s, acc=58.32%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.32% | Remaining weights: 38.34% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 31: 100%|██████████| 196/196 [00:07<00:00, 25.62batch/s, loss=4.2509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8894 | Reg=3.3616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 31: 100%|██████████| 196/196 [00:04<00:00, 39.47batch/s, acc=68.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.50% | Remaining weights: 36.54% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 32: 100%|██████████| 196/196 [00:07<00:00, 25.18batch/s, loss=4.7005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9911 | Reg=3.7094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 32: 100%|██████████| 196/196 [00:05<00:00, 38.96batch/s, acc=60.83%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.83% | Remaining weights: 36.06% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 33: 100%|██████████| 196/196 [00:07<00:00, 25.69batch/s, loss=3.6832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7748 | Reg=2.9084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 33: 100%|██████████| 196/196 [00:04<00:00, 39.47batch/s, acc=69.45%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.45% | Remaining weights: 34.27% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 34: 100%|██████████| 196/196 [00:07<00:00, 25.52batch/s, loss=4.5153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1430 | Reg=3.3723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 34: 100%|██████████| 196/196 [00:05<00:00, 39.16batch/s, acc=53.67%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.67% | Remaining weights: 34.08% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 35: 100%|██████████| 196/196 [00:07<00:00, 25.66batch/s, loss=3.5008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9587 | Reg=2.5421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 35: 100%|██████████| 196/196 [00:04<00:00, 39.53batch/s, acc=71.44%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.44% | Remaining weights: 32.26% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 36: 100%|██████████| 196/196 [00:07<00:00, 25.62batch/s, loss=4.1073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0538 | Reg=3.0535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 36: 100%|██████████| 196/196 [00:04<00:00, 39.57batch/s, acc=49.74%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.74% | Remaining weights: 32.27% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 37: 100%|██████████| 196/196 [00:07<00:00, 25.66batch/s, loss=3.2805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0420 | Reg=2.2385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 37: 100%|██████████| 196/196 [00:04<00:00, 39.23batch/s, acc=73.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.61% | Remaining weights: 30.49% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 38: 100%|██████████| 196/196 [00:07<00:00, 25.60batch/s, loss=4.0255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2032 | Reg=2.8223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 38: 100%|██████████| 196/196 [00:04<00:00, 39.23batch/s, acc=56.20%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.20% | Remaining weights: 30.72% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 39: 100%|██████████| 196/196 [00:07<00:00, 25.28batch/s, loss=2.8208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8486 | Reg=1.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 39: 100%|██████████| 196/196 [00:05<00:00, 39.04batch/s, acc=74.22%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.22% | Remaining weights: 28.87% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 40: 100%|██████████| 196/196 [00:07<00:00, 25.58batch/s, loss=4.0172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.3868 | Reg=2.6304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 40: 100%|██████████| 196/196 [00:04<00:00, 39.35batch/s, acc=54.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.82% | Remaining weights: 29.23% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 41: 100%|██████████| 196/196 [00:07<00:00, 25.60batch/s, loss=2.6603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8944 | Reg=1.7659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 41: 100%|██████████| 196/196 [00:04<00:00, 39.38batch/s, acc=75.22%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.22% | Remaining weights: 27.44% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 42: 100%|██████████| 196/196 [00:07<00:00, 25.53batch/s, loss=3.8163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.3257 | Reg=2.4906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 42: 100%|██████████| 196/196 [00:04<00:00, 39.51batch/s, acc=45.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.00% | Remaining weights: 28.08% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 43: 100%|██████████| 196/196 [00:07<00:00, 25.54batch/s, loss=2.5792]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9756 | Reg=1.6036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 43: 100%|██████████| 196/196 [00:04<00:00, 39.40batch/s, acc=75.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.39% | Remaining weights: 26.26% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 44: 100%|██████████| 196/196 [00:07<00:00, 25.61batch/s, loss=3.3132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9867 | Reg=2.3265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 44: 100%|██████████| 196/196 [00:05<00:00, 38.86batch/s, acc=54.66%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.66% | Remaining weights: 26.83% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 45: 100%|██████████| 196/196 [00:07<00:00, 25.32batch/s, loss=2.1731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7084 | Reg=1.4648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 45: 100%|██████████| 196/196 [00:05<00:00, 38.68batch/s, acc=75.73%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.73% | Remaining weights: 25.11% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 46: 100%|██████████| 196/196 [00:07<00:00, 25.12batch/s, loss=3.3457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2294 | Reg=2.1163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 46: 100%|██████████| 196/196 [00:04<00:00, 39.40batch/s, acc=56.36%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.36% | Remaining weights: 25.67% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 47: 100%|██████████| 196/196 [00:07<00:00, 25.48batch/s, loss=2.3468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0066 | Reg=1.3402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 47: 100%|██████████| 196/196 [00:04<00:00, 39.27batch/s, acc=75.92%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.92% | Remaining weights: 24.13% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 48: 100%|██████████| 196/196 [00:07<00:00, 25.72batch/s, loss=3.0690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1361 | Reg=1.9330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 48: 100%|██████████| 196/196 [00:04<00:00, 39.49batch/s, acc=60.45%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.45% | Remaining weights: 24.70% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 49: 100%|██████████| 196/196 [00:07<00:00, 25.50batch/s, loss=2.1377]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9134 | Reg=1.2243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 49: 100%|██████████| 196/196 [00:04<00:00, 39.49batch/s, acc=76.01%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.01% | Remaining weights: 23.23% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 50: 100%|██████████| 196/196 [00:07<00:00, 25.68batch/s, loss=3.0355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0793 | Reg=1.9562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 50: 100%|██████████| 196/196 [00:05<00:00, 39.09batch/s, acc=54.14%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.14% | Remaining weights: 24.01% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 51: 100%|██████████| 196/196 [00:07<00:00, 25.59batch/s, loss=1.9004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7790 | Reg=1.1214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 51: 100%|██████████| 196/196 [00:04<00:00, 39.32batch/s, acc=75.73%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.73% | Remaining weights: 22.41% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 52: 100%|██████████| 196/196 [00:07<00:00, 25.60batch/s, loss=2.7970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9969 | Reg=1.8001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 52: 100%|██████████| 196/196 [00:04<00:00, 39.55batch/s, acc=33.01%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.01% | Remaining weights: 23.16% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 53: 100%|██████████| 196/196 [00:07<00:00, 25.16batch/s, loss=1.6678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6226 | Reg=1.0452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 53: 100%|██████████| 196/196 [00:04<00:00, 39.46batch/s, acc=76.38%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.38% | Remaining weights: 21.68% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 54: 100%|██████████| 196/196 [00:07<00:00, 25.61batch/s, loss=3.0375]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.3362 | Reg=1.7012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 54: 100%|██████████| 196/196 [00:05<00:00, 39.04batch/s, acc=55.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.00% | Remaining weights: 22.35% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 55: 100%|██████████| 196/196 [00:07<00:00, 25.35batch/s, loss=1.9666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9728 | Reg=0.9938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 55: 100%|██████████| 196/196 [00:04<00:00, 39.31batch/s, acc=75.95%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.95% | Remaining weights: 21.05% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 56: 100%|██████████| 196/196 [00:07<00:00, 25.65batch/s, loss=2.8769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1980 | Reg=1.6789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 56: 100%|██████████| 196/196 [00:05<00:00, 39.14batch/s, acc=60.55%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.55% | Remaining weights: 21.75% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 57: 100%|██████████| 196/196 [00:07<00:00, 25.16batch/s, loss=1.7794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8270 | Reg=0.9524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 57: 100%|██████████| 196/196 [00:04<00:00, 39.39batch/s, acc=75.49%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.49% | Remaining weights: 20.52% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 58: 100%|██████████| 196/196 [00:07<00:00, 25.50batch/s, loss=2.4303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9356 | Reg=1.4947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 58: 100%|██████████| 196/196 [00:05<00:00, 39.09batch/s, acc=47.25%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.25% | Remaining weights: 21.04% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 59: 100%|██████████| 196/196 [00:07<00:00, 25.40batch/s, loss=1.7343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8143 | Reg=0.9200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 59: 100%|██████████| 196/196 [00:05<00:00, 38.93batch/s, acc=76.10%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.10% | Remaining weights: 20.04% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 60: 100%|██████████| 196/196 [00:07<00:00, 25.61batch/s, loss=2.6576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2350 | Reg=1.4226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 60: 100%|██████████| 196/196 [00:05<00:00, 38.76batch/s, acc=60.73%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.73% | Remaining weights: 20.47% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 61: 100%|██████████| 196/196 [00:07<00:00, 25.76batch/s, loss=1.6189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7111 | Reg=0.9079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 61: 100%|██████████| 196/196 [00:05<00:00, 38.94batch/s, acc=73.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.54% | Remaining weights: 19.59% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 62: 100%|██████████| 196/196 [00:07<00:00, 25.72batch/s, loss=2.2867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9375 | Reg=1.3492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 62: 100%|██████████| 196/196 [00:04<00:00, 39.60batch/s, acc=56.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.00% | Remaining weights: 19.90% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 63: 100%|██████████| 196/196 [00:07<00:00, 25.30batch/s, loss=1.8254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9396 | Reg=0.8858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 63: 100%|██████████| 196/196 [00:05<00:00, 38.94batch/s, acc=73.90%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.90% | Remaining weights: 19.22% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 64: 100%|██████████| 196/196 [00:07<00:00, 25.38batch/s, loss=1.8890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7313 | Reg=1.1577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 64: 100%|██████████| 196/196 [00:05<00:00, 38.87batch/s, acc=56.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.96% | Remaining weights: 19.28% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 65: 100%|██████████| 196/196 [00:07<00:00, 25.55batch/s, loss=1.6493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7507 | Reg=0.8986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 65: 100%|██████████| 196/196 [00:04<00:00, 39.48batch/s, acc=71.89%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.89% | Remaining weights: 18.86% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 66: 100%|██████████| 196/196 [00:07<00:00, 25.54batch/s, loss=1.9503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8596 | Reg=1.0907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 66: 100%|██████████| 196/196 [00:04<00:00, 39.41batch/s, acc=61.23%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.23% | Remaining weights: 18.77% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 67: 100%|██████████| 196/196 [00:07<00:00, 25.72batch/s, loss=1.8401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9266 | Reg=0.9135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 67: 100%|██████████| 196/196 [00:04<00:00, 39.22batch/s, acc=70.76%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.76% | Remaining weights: 18.56% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 68: 100%|██████████| 196/196 [00:07<00:00, 25.44batch/s, loss=1.9459]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9855 | Reg=0.9604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 68: 100%|██████████| 196/196 [00:05<00:00, 39.12batch/s, acc=61.59%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.59% | Remaining weights: 18.26% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 69: 100%|██████████| 196/196 [00:07<00:00, 25.64batch/s, loss=1.7525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8029 | Reg=0.9496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 69: 100%|██████████| 196/196 [00:04<00:00, 39.25batch/s, acc=70.02%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.02% | Remaining weights: 18.32% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 70: 100%|██████████| 196/196 [00:07<00:00, 25.69batch/s, loss=1.6303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7647 | Reg=0.8657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 70: 100%|██████████| 196/196 [00:05<00:00, 38.71batch/s, acc=64.45%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.45% | Remaining weights: 17.83% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 71: 100%|██████████| 196/196 [00:07<00:00, 25.56batch/s, loss=1.7934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8188 | Reg=0.9746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 71: 100%|██████████| 196/196 [00:05<00:00, 39.12batch/s, acc=68.03%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.03% | Remaining weights: 18.11% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 72: 100%|██████████| 196/196 [00:07<00:00, 25.56batch/s, loss=1.5105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6901 | Reg=0.8205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 72: 100%|██████████| 196/196 [00:05<00:00, 38.77batch/s, acc=66.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.54% | Remaining weights: 17.49% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 73: 100%|██████████| 196/196 [00:07<00:00, 25.59batch/s, loss=1.8670]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9289 | Reg=0.9381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 73: 100%|██████████| 196/196 [00:05<00:00, 38.88batch/s, acc=70.12%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.12% | Remaining weights: 17.85% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 74: 100%|██████████| 196/196 [00:07<00:00, 25.62batch/s, loss=1.6527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9345 | Reg=0.7182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 74: 100%|██████████| 196/196 [00:05<00:00, 39.19batch/s, acc=69.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.54% | Remaining weights: 17.15% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 75: 100%|██████████| 196/196 [00:07<00:00, 25.61batch/s, loss=1.9807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9524 | Reg=1.0283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 75: 100%|██████████| 196/196 [00:04<00:00, 39.40batch/s, acc=61.77%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.77% | Remaining weights: 17.72% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 76: 100%|██████████| 196/196 [00:07<00:00, 25.54batch/s, loss=1.6791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0374 | Reg=0.6417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 76: 100%|██████████| 196/196 [00:05<00:00, 38.90batch/s, acc=73.70%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.70% | Remaining weights: 16.89% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 77: 100%|██████████| 196/196 [00:07<00:00, 25.25batch/s, loss=1.6904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7184 | Reg=0.9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 77: 100%|██████████| 196/196 [00:05<00:00, 38.95batch/s, acc=63.11%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.11% | Remaining weights: 17.54% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 78: 100%|██████████| 196/196 [00:07<00:00, 25.09batch/s, loss=1.3669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7645 | Reg=0.6024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 78: 100%|██████████| 196/196 [00:05<00:00, 38.52batch/s, acc=74.19%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.19% | Remaining weights: 16.68% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 79: 100%|██████████| 196/196 [00:07<00:00, 25.34batch/s, loss=1.9931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9241 | Reg=1.0691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 79: 100%|██████████| 196/196 [00:05<00:00, 39.07batch/s, acc=62.92%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.92% | Remaining weights: 17.45% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 80: 100%|██████████| 196/196 [00:07<00:00, 25.22batch/s, loss=1.2535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6702 | Reg=0.5833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 80: 100%|██████████| 196/196 [00:05<00:00, 38.72batch/s, acc=74.52%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.52% | Remaining weights: 16.51% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 81: 100%|██████████| 196/196 [00:07<00:00, 25.42batch/s, loss=1.9065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8107 | Reg=1.0957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 81: 100%|██████████| 196/196 [00:05<00:00, 38.61batch/s, acc=65.98%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.98% | Remaining weights: 17.34% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 82: 100%|██████████| 196/196 [00:07<00:00, 25.08batch/s, loss=1.3546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7923 | Reg=0.5623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 82: 100%|██████████| 196/196 [00:05<00:00, 38.90batch/s, acc=77.51%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.51% | Remaining weights: 16.34% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 83: 100%|██████████| 196/196 [00:07<00:00, 25.53batch/s, loss=1.9645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8573 | Reg=1.1072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 83: 100%|██████████| 196/196 [00:05<00:00, 38.99batch/s, acc=60.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.50% | Remaining weights: 17.17% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 84: 100%|██████████| 196/196 [00:07<00:00, 25.27batch/s, loss=1.3099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7563 | Reg=0.5536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 84: 100%|██████████| 196/196 [00:05<00:00, 38.98batch/s, acc=78.68%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.68% | Remaining weights: 16.21% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 85: 100%|██████████| 196/196 [00:07<00:00, 24.74batch/s, loss=2.1117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0017 | Reg=1.1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 85: 100%|██████████| 196/196 [00:05<00:00, 38.72batch/s, acc=65.76%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.76% | Remaining weights: 17.11% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 86: 100%|██████████| 196/196 [00:07<00:00, 25.51batch/s, loss=1.4531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9090 | Reg=0.5441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 86: 100%|██████████| 196/196 [00:05<00:00, 38.70batch/s, acc=79.28%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.28% | Remaining weights: 16.13% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 87: 100%|██████████| 196/196 [00:07<00:00, 25.43batch/s, loss=2.0282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8898 | Reg=1.1384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 87: 100%|██████████| 196/196 [00:05<00:00, 38.41batch/s, acc=67.89%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.89% | Remaining weights: 16.96% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 88: 100%|██████████| 196/196 [00:07<00:00, 25.39batch/s, loss=1.2547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7247 | Reg=0.5300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 88: 100%|██████████| 196/196 [00:05<00:00, 38.96batch/s, acc=79.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.39% | Remaining weights: 15.98% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 89: 100%|██████████| 196/196 [00:07<00:00, 25.34batch/s, loss=2.1614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9922 | Reg=1.1693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 89: 100%|██████████| 196/196 [00:05<00:00, 39.17batch/s, acc=57.37%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.37% | Remaining weights: 16.85% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 90: 100%|██████████| 196/196 [00:07<00:00, 25.59batch/s, loss=1.3149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7832 | Reg=0.5317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 90: 100%|██████████| 196/196 [00:05<00:00, 38.57batch/s, acc=79.88%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.88% | Remaining weights: 15.87% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 91: 100%|██████████| 196/196 [00:07<00:00, 25.36batch/s, loss=2.0209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8193 | Reg=1.2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 91: 100%|██████████| 196/196 [00:05<00:00, 39.17batch/s, acc=54.40%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.40% | Remaining weights: 16.76% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 92: 100%|██████████| 196/196 [00:07<00:00, 25.03batch/s, loss=1.0481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5109 | Reg=0.5372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 92: 100%|██████████| 196/196 [00:05<00:00, 38.64batch/s, acc=79.97%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.97% | Remaining weights: 15.75% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 93: 100%|██████████| 196/196 [00:07<00:00, 25.37batch/s, loss=1.9360]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7719 | Reg=1.1642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 93: 100%|██████████| 196/196 [00:05<00:00, 38.25batch/s, acc=62.81%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.81% | Remaining weights: 16.58% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 94: 100%|██████████| 196/196 [00:07<00:00, 25.44batch/s, loss=1.3176]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7846 | Reg=0.5330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 94: 100%|██████████| 196/196 [00:05<00:00, 38.91batch/s, acc=79.83%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.83% | Remaining weights: 15.61% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 95: 100%|██████████| 196/196 [00:07<00:00, 25.59batch/s, loss=2.2552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0659 | Reg=1.1893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 95: 100%|██████████| 196/196 [00:05<00:00, 38.98batch/s, acc=59.78%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.78% | Remaining weights: 16.47% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 96: 100%|██████████| 196/196 [00:07<00:00, 24.92batch/s, loss=1.3409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8115 | Reg=0.5294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 96: 100%|██████████| 196/196 [00:05<00:00, 39.14batch/s, acc=79.94%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.94% | Remaining weights: 15.49% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 97: 100%|██████████| 196/196 [00:07<00:00, 25.15batch/s, loss=2.0277]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8054 | Reg=1.2223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 97: 100%|██████████| 196/196 [00:05<00:00, 38.68batch/s, acc=62.60%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.60% | Remaining weights: 16.40% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 98: 100%|██████████| 196/196 [00:07<00:00, 25.21batch/s, loss=1.2667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7387 | Reg=0.5281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 98: 100%|██████████| 196/196 [00:05<00:00, 38.58batch/s, acc=79.95%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.95% | Remaining weights: 15.42% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 99: 100%|██████████| 196/196 [00:07<00:00, 25.34batch/s, loss=1.9353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8159 | Reg=1.1194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 99: 100%|██████████| 196/196 [00:05<00:00, 38.92batch/s, acc=63.76%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.76% | Remaining weights: 16.18% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 100: 100%|██████████| 196/196 [00:03<00:00, 53.34batch/s, loss=0.6153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6153 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 100: 100%|██████████| 196/196 [00:02<00:00, 83.02batch/s, acc=80.94%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.94% | Remaining weights: 14.00% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 101: 100%|██████████| 196/196 [00:03<00:00, 52.86batch/s, loss=0.6754]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6754 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 101: 100%|██████████| 196/196 [00:02<00:00, 82.72batch/s, acc=76.11%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.11% | Remaining weights: 13.13% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 102: 100%|██████████| 196/196 [00:03<00:00, 53.00batch/s, loss=0.6781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6781 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 102: 100%|██████████| 196/196 [00:02<00:00, 80.85batch/s, acc=82.83%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.83% | Remaining weights: 12.51% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 103: 100%|██████████| 196/196 [00:03<00:00, 53.54batch/s, loss=0.7286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7286 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 103: 100%|██████████| 196/196 [00:02<00:00, 80.98batch/s, acc=78.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.61% | Remaining weights: 12.22% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 104: 100%|██████████| 196/196 [00:03<00:00, 53.66batch/s, loss=0.7422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7422 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 104: 100%|██████████| 196/196 [00:02<00:00, 81.15batch/s, acc=83.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.96% | Remaining weights: 11.84% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 105: 100%|██████████| 196/196 [00:03<00:00, 53.34batch/s, loss=0.5348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5348 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 105: 100%|██████████| 196/196 [00:02<00:00, 80.63batch/s, acc=80.90%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.90% | Remaining weights: 11.69% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 106: 100%|██████████| 196/196 [00:03<00:00, 52.75batch/s, loss=0.5076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5076 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 106: 100%|██████████| 196/196 [00:02<00:00, 82.07batch/s, acc=84.81%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.81% | Remaining weights: 11.42% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 107: 100%|██████████| 196/196 [00:03<00:00, 53.01batch/s, loss=0.7548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7548 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 107: 100%|██████████| 196/196 [00:02<00:00, 80.03batch/s, acc=83.22%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.22% | Remaining weights: 11.33% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 108: 100%|██████████| 196/196 [00:03<00:00, 53.14batch/s, loss=0.4868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4868 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 108: 100%|██████████| 196/196 [00:02<00:00, 82.04batch/s, acc=85.58%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.58% | Remaining weights: 11.11% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 109: 100%|██████████| 196/196 [00:03<00:00, 53.03batch/s, loss=0.5707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5707 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 109: 100%|██████████| 196/196 [00:02<00:00, 82.36batch/s, acc=83.15%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.15% | Remaining weights: 11.05% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 110: 100%|██████████| 196/196 [00:03<00:00, 53.10batch/s, loss=0.6220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6220 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 110: 100%|██████████| 196/196 [00:02<00:00, 81.83batch/s, acc=86.26%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.26% | Remaining weights: 10.86% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 111: 100%|██████████| 196/196 [00:03<00:00, 52.86batch/s, loss=0.6977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6977 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 111: 100%|██████████| 196/196 [00:02<00:00, 81.47batch/s, acc=84.88%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.88% | Remaining weights: 10.82% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 112: 100%|██████████| 196/196 [00:03<00:00, 53.06batch/s, loss=0.6014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6014 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 112: 100%|██████████| 196/196 [00:02<00:00, 81.33batch/s, acc=86.49%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.49% | Remaining weights: 10.65% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 113: 100%|██████████| 196/196 [00:03<00:00, 52.89batch/s, loss=0.5381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5381 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 113: 100%|██████████| 196/196 [00:02<00:00, 82.52batch/s, acc=83.59%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.59% | Remaining weights: 10.62% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 114: 100%|██████████| 196/196 [00:03<00:00, 53.35batch/s, loss=0.4010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4010 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 114: 100%|██████████| 196/196 [00:02<00:00, 81.22batch/s, acc=87.09%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.09% | Remaining weights: 10.48% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 115: 100%|██████████| 196/196 [00:03<00:00, 53.10batch/s, loss=0.3746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3746 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 115: 100%|██████████| 196/196 [00:02<00:00, 81.06batch/s, acc=85.88%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.88% | Remaining weights: 10.46% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 116: 100%|██████████| 196/196 [00:03<00:00, 53.71batch/s, loss=0.3654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3654 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 116: 100%|██████████| 196/196 [00:02<00:00, 82.31batch/s, acc=86.92%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.92% | Remaining weights: 10.33% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 117: 100%|██████████| 196/196 [00:03<00:00, 53.46batch/s, loss=0.4842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4842 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 117: 100%|██████████| 196/196 [00:02<00:00, 82.41batch/s, acc=87.36%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.36% | Remaining weights: 10.31% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 118: 100%|██████████| 196/196 [00:03<00:00, 52.56batch/s, loss=0.5739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5739 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 118: 100%|██████████| 196/196 [00:02<00:00, 82.04batch/s, acc=87.67%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.67% | Remaining weights: 10.19% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 119: 100%|██████████| 196/196 [00:03<00:00, 53.29batch/s, loss=0.4417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4417 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 119: 100%|██████████| 196/196 [00:02<00:00, 81.92batch/s, acc=87.60%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.60% | Remaining weights: 10.18% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 120: 100%|██████████| 196/196 [00:03<00:00, 53.72batch/s, loss=0.5640]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5640 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 120: 100%|██████████| 196/196 [00:02<00:00, 81.19batch/s, acc=88.03%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.03% | Remaining weights: 10.07% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 121: 100%|██████████| 196/196 [00:03<00:00, 52.69batch/s, loss=0.5458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5458 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 121: 100%|██████████| 196/196 [00:02<00:00, 81.76batch/s, acc=88.61%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.61% | Remaining weights: 10.06% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 122: 100%|██████████| 196/196 [00:03<00:00, 52.97batch/s, loss=0.6599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6599 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 122: 100%|██████████| 196/196 [00:02<00:00, 81.82batch/s, acc=88.03%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.03% | Remaining weights: 9.96% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 123: 100%|██████████| 196/196 [00:03<00:00, 52.86batch/s, loss=0.4634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4634 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 123: 100%|██████████| 196/196 [00:02<00:00, 81.50batch/s, acc=88.63%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.63% | Remaining weights: 9.95% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 124: 100%|██████████| 196/196 [00:03<00:00, 53.21batch/s, loss=0.3574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3574 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 124: 100%|██████████| 196/196 [00:02<00:00, 81.68batch/s, acc=88.13%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.13% | Remaining weights: 9.86% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 125: 100%|██████████| 196/196 [00:03<00:00, 52.71batch/s, loss=0.5050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5050 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 125: 100%|██████████| 196/196 [00:02<00:00, 82.60batch/s, acc=89.40%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.40% | Remaining weights: 9.85% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 126: 100%|██████████| 196/196 [00:03<00:00, 53.00batch/s, loss=0.2705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.2705 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 126: 100%|██████████| 196/196 [00:02<00:00, 79.59batch/s, acc=88.44%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.44% | Remaining weights: 9.77% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 127: 100%|██████████| 196/196 [00:03<00:00, 53.30batch/s, loss=0.4513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4513 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 127: 100%|██████████| 196/196 [00:02<00:00, 81.57batch/s, acc=89.63%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.63% | Remaining weights: 9.76% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 128: 100%|██████████| 196/196 [00:03<00:00, 52.60batch/s, loss=0.3757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3757 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 128: 100%|██████████| 196/196 [00:02<00:00, 80.98batch/s, acc=88.33%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.33% | Remaining weights: 9.69% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 129: 100%|██████████| 196/196 [00:03<00:00, 53.14batch/s, loss=0.5548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5548 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 129: 100%|██████████| 196/196 [00:02<00:00, 82.60batch/s, acc=90.30%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.30% | Remaining weights: 9.68% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pquant import iterative_train\n",
    "\"\"\"\n",
    "Inputs to train_resnet we defined previously are:\n",
    "          model, trainloader, device, loss_func, epoch, optimizer, scheduler, **kwargs\n",
    "\"\"\"\n",
    "\n",
    "trained_model = iterative_train(model = model, \n",
    "                                config = config, \n",
    "                                train_func = train_resnet, \n",
    "                                valid_func = validate_resnet, \n",
    "                                trainloader = train_loader, \n",
    "                                testloader = val_loader, \n",
    "                                device = device, \n",
    "                                loss_func = loss_function,\n",
    "                                optimizer = optimizer, \n",
    "                                scheduler = scheduler\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd70fed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_203626/1596906618.py:29: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax[0].set_yticklabels(new_ytick)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pquant import remove_pruning_from_model\n",
    "import matplotlib.pyplot as plt\n",
    "# Remove compression layers, leaves Quantized activations in place\n",
    "model = remove_pruning_from_model(trained_model, config)\n",
    "\n",
    "# Plot remaining weights\n",
    "names = []\n",
    "remaining = []\n",
    "total_w = []\n",
    "nonzeros = []\n",
    "for n, m in trained_model.named_modules():\n",
    "    if isinstance(m, (torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Linear)):\n",
    "        names.append(n)\n",
    "        nonzero = np.count_nonzero(m.weight.detach().cpu())\n",
    "        remaining_pct = nonzero / m.weight.numel()\n",
    "        remaining.append(remaining_pct)\n",
    "        total_w.append(m.weight.numel())\n",
    "        nonzeros.append(nonzero)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].bar(range(len(names)), remaining)\n",
    "ax[0].set_xticks(range(len(names)))\n",
    "ax[0].set_xticklabels(names)\n",
    "ax[0].tick_params(axis='x', labelrotation=270)\n",
    "new_ytick = []\n",
    "for i in ax[0].get_yticklabels():\n",
    "    ytick = f\"{float(i.get_text()) * 100:.2f}%\"\n",
    "    new_ytick.append(ytick)\n",
    "ax[0].set_yticklabels(new_ytick)\n",
    "ax[0].title.set_text(\"Remaining weights per layer\")\n",
    "\n",
    "ax[1].bar(range(len(nonzeros)), total_w, color=\"lightcoral\", label=\"total weights\")\n",
    "ax[1].bar(range(len(nonzeros)), nonzeros, color=\"steelblue\", label=\"nonzero weights\")\n",
    "ax[1].set_xticks(range(len(names)))\n",
    "ax[1].set_xticklabels(names)\n",
    "ax[1].tick_params(axis='x', labelrotation=270)\n",
    "ax[1].title.set_text(\"Weights per layer\")\n",
    "ax[1].legend()\n",
    "ax[1].set_yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19fe7f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_conv_patterns(model):\n",
    "    \"\"\"\n",
    "    Extracts unique binary patterns and their counts from convolutional layers\n",
    "    of a given model in a backend-agnostic way.\n",
    "\n",
    "    Args:\n",
    "        model: A Keras or PyTorch model.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are the names of convolutional layers and\n",
    "        values are another dictionary containing:\n",
    "        - 'patterns': A NumPy array of the unique binary patterns.\n",
    "        - 'counts': A NumPy array with the count for each unique pattern.\n",
    "        - 'kernel_shape': The original (height, width) of the kernel.\n",
    "    \"\"\"\n",
    "    patterns_by_layer = {}\n",
    "    backend = keras.backend.backend()\n",
    "\n",
    "    # Iterate through model layers/modules\n",
    "    if backend == \"torch\":\n",
    "        modules = model.named_modules()\n",
    "    else: # TensorFlow\n",
    "        modules = [(layer.name, layer) for layer in model.layers]\n",
    "\n",
    "    for name, module in modules:\n",
    "        is_conv_layer = False\n",
    "        if backend == \"torch\" and isinstance(module, (keras.layers.Conv2D, torch.nn.Conv2d)):\n",
    "            weight = module.weight.detach()\n",
    "            is_conv_layer = len(weight.shape) == 4\n",
    "        elif backend == \"tensorflow\" and isinstance(module, keras.layers.Conv2D):\n",
    "            weight = module.kernel\n",
    "            is_conv_layer = True\n",
    "\n",
    "        if is_conv_layer:\n",
    "            all_patterns_flat = keras.ops.reshape(weight, (weight.shape[0], -1))\n",
    "            all_patterns_binary = keras.ops.cast(all_patterns_flat > 0, dtype=\"float32\")\n",
    "\n",
    "            num_bits = all_patterns_binary.shape[1]\n",
    "            if num_bits == 0:\n",
    "                continue\n",
    "\n",
    "            powers_of_2 = keras.ops.power(2.0, keras.ops.arange(num_bits, dtype=\"float32\"))\n",
    "            hashes = keras.ops.sum(all_patterns_binary * powers_of_2, axis=1)\n",
    "\n",
    "            unique_hashes, counts = np.unique(keras.ops.convert_to_numpy(hashes), return_counts=True)\n",
    "            unique_patterns_binary = ((unique_hashes[:, None] & (1 << np.arange(num_bits))) > 0).astype(float)\n",
    "            \n",
    "            sort_indices = np.argsort(counts)[::-1]\n",
    "            patterns_by_layer[name] = {\n",
    "                'patterns': unique_patterns_binary[sort_indices],\n",
    "                'counts': counts[sort_indices],\n",
    "                'kernel_shape': (weight.shape[2], weight.shape[3])\n",
    "            }\n",
    "            print(f\"Processed layer '{name}', found {len(counts)} unique patterns.\")\n",
    "\n",
    "    return patterns_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae1039e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def visualize_conv_patterns(model, layer_name, max_patterns_to_show=16):\n",
    "    \"\"\"\n",
    "    Visualizes the dominant binary patterns for a specific convolutional layer.\n",
    "    \"\"\"\n",
    "    target_module = None\n",
    "    for name, module in model.named_modules():\n",
    "        if name == layer_name and hasattr(module, \"pruning_layer\"):\n",
    "            pruning_layer = module.pruning_layer\n",
    "            if hasattr(pruning_layer, \"metric_fn\") and \"PACAPattern\" in pruning_layer.metric_fn.__class__.__name__:\n",
    "                target_module = module\n",
    "                break\n",
    "    \n",
    "    if target_module is None:\n",
    "        print(f\"Error: Could not find a PACA-pruned layer named '{layer_name}'.\")\n",
    "        return\n",
    "        \n",
    "    metric_fn = target_module.pruning_layer.metric_fn\n",
    "    # Ensure dominant patterns are selected based on the final weights\n",
    "    metric_fn._select_dominant_patterns(target_module.weight)\n",
    "    \n",
    "    patterns = metric_fn.dominant_patterns\n",
    "    if patterns is None or patterns.shape[0] == 0:\n",
    "        print(f\"No dominant patterns found for layer '{layer_name}'.\")\n",
    "        return\n",
    "        \n",
    "    # Convert to NumPy for plotting\n",
    "    patterns_np = keras.ops.convert_to_numpy(patterns)\n",
    "    \n",
    "    # Get original kernel shape from the weight tensor\n",
    "    kernel_h, kernel_w = target_module.weight.shape[2], target_module.weight.shape[3]\n",
    "    \n",
    "    num_patterns = min(patterns_np.shape[0], max_patterns_to_show)\n",
    "    \n",
    "    # Create a subplot grid for the patterns\n",
    "    cols = math.ceil(math.sqrt(num_patterns))\n",
    "    rows = math.ceil(num_patterns / cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "    if isinstance(axes, plt.Axes):\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    \n",
    "    fig.suptitle(f\"Dominant Patterns for Layer: {layer_name} (found {patterns_np.shape[0]})\", fontsize=16)\n",
    "    \n",
    "    for i in range(num_patterns):\n",
    "        pattern_2d = patterns_np[i].reshape(kernel_h, kernel_w)\n",
    "        print(pattern_2d.shape)\n",
    "        print(pattern_2d)\n",
    "        axes[i].imshow(pattern_2d, cmap='binary', vmin=0, vmax=1)\n",
    "        axes[i].set_title(f\"Pattern {i+1}\")\n",
    "        axes[i].set_xticks([])\n",
    "        axes[i].set_yticks([])\n",
    "        \n",
    "    # Hide any unused subplots\n",
    "    for j in range(num_patterns, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "        \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251515c3-00ac-4110-b8d8-a8c9100f6e6b",
   "metadata": {},
   "source": [
    "## Add PACA prunning\n",
    "#### After pruning we will have multiple patterns, so we force all of them to have a lower num,ber of dominant patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5898e23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "batch_size": 128,
       "cosine_tmax": 200,
       "gamma": 0.1,
       "l2_decay": 0.0001,
       "label_smoothing": 0,
       "lr": 0.001,
       "lr_schedule": "multistep",
       "milestones": [
        75,
        120
       ],
       "momentum": 0.9,
       "optimizer": "sgd",
       "plot_frequency": 100,
       "pruning_parameters": {
        "beta": 0.75,
        "damping": 1,
        "disable_pruning_for_layers": [
         null
        ],
        "distance_metric": "valued_hamming",
        "enable_pruning": true,
        "epsilon": 0.001,
        "metric_type": "PACAPatternSparsity",
        "num_patterns_to_keep": 16,
        "pruning_method": "mdmm",
        "scale": 50,
        "use_grad": false
       },
       "quantization_parameters": {
        "default_fractional_bits": 7,
        "default_integer_bits": 0,
        "enable_quantization": false,
        "hgq_gamma": 0.0003,
        "hgq_heterogeneous": true,
        "layer_specific": [],
        "use_high_granularity_quantization": false,
        "use_real_tanh": false,
        "use_symmetric_quantization": false
       },
       "training_parameters": {
        "epochs": 200,
        "fine_tuning_epochs": 30,
        "pretraining_epochs": 0,
        "pruning_first": false,
        "rewind": "never",
        "rounds": 1,
        "save_weights_epoch": -1
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml \n",
    "\n",
    "with open(\"pquant/configs/config_mdmm_paca.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "JSON(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8da0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE_PATH = 'resnet_paca_pruned.pth'\n",
    "# torch.save(trained_model.state_dict(), SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e857a26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): CompressedLayerConv2d(\n",
       "    (pruning_layer): <MDMM name=mdmm_21, built=True>\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_22, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_23, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_24, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_25, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_26, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_27, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_28, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_29, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_30, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_31, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_32, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_33, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_34, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_35, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_36, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_37, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_38, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_39, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_40, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): CompressedLayerLinear(\n",
       "    (pruning_layer): <MDMM name=mdmm_41, built=True>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pquant import add_compression_layers\n",
    "input_shape = (256,3,32,32)\n",
    "model = add_compression_layers(model.to(device), config, input_shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de0c3b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "(3, 3)\n",
      "[[1. 1. 0.]\n",
      " [0. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "(3, 3)\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "(3, 3)\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 1. 1.]]\n",
      "(3, 3)\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]]\n",
      "(3, 3)\n",
      "[[0. 1. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 1. 1.]]\n",
      "(3, 3)\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 1. 1.]]\n",
      "(3, 3)\n",
      "[[0. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 1. 1.]]\n",
      "(3, 3)\n",
      "[[1. 1. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]]\n",
      "(3, 3)\n",
      "[[0. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 1. 1.]]\n",
      "(3, 3)\n",
      "[[1. 1. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]]\n",
      "(3, 3)\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]]\n",
      "(3, 3)\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]]\n",
      "(3, 3)\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]]\n",
      "(3, 3)\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [0. 1. 1.]]\n",
      "(3, 3)\n",
      "[[1. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 1. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_conv_patterns(model, 'layer1.0.conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90c24bff-9937-4670-8cff-022ddc7a0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(model, trainloader, device, loss_func, epoch, optimizer, scheduler, *args, **kwargs):\n",
    "    first_batch = True\n",
    "    for data in trainloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.)) # 8 bits input quantization\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        losses = get_model_losses(model, torch.tensor(0.).to(device))\n",
    "        loss += losses\n",
    "        loss.backward()\n",
    "\n",
    "        if first_batch:\n",
    "            print(\"\\n ---- Checking Loss values ----\")\n",
    "            print(\"Loss:\", loss_func(outputs, labels).item())\n",
    "            print(\"Model Losses:\", losses.item())\n",
    "            print(\"--------------------------------------------------\")\n",
    "\n",
    "            for name, module in model.named_modules():\n",
    "                if \"conv1\" in name and hasattr(module, \"pruning_layer\"):\n",
    "                    pruning_layer = module.pruning_layer\n",
    "                    if hasattr(pruning_layer, \"metric_fn\") and \"PACAPattern\" in pruning_layer.metric_fn.__class__.__name__:\n",
    "                        metric_fn = pruning_layer.metric_fn\n",
    "                        num_patterns = 0\n",
    "                        if metric_fn.dominant_patterns is not None:\n",
    "                            num_patterns = metric_fn.dominant_patterns.shape[0]\n",
    "\n",
    "                        total_dist = metric_fn(module.weight).item()\n",
    "                        num_kernels = module.weight.shape[0]\n",
    "                        avg_dist = total_dist / num_kernels if num_kernels > 0 else 0\n",
    "\n",
    "                        print(f\"--- PACA Stats for {name} at Epoch {epoch} ---\")\n",
    "                        print(f\"Num Patterns: {num_patterns}, Avg Pattern Dist: {avg_dist:.4f}\")\n",
    "                        print(\"--------------------------------------------------\\n\")\n",
    "                        break\n",
    "            first_batch = False\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch += 1\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "def validate_resnet(model, testloader, device, loss_func, epoch, *args, **kwargs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_paca_patterns = 0\n",
    "    avg_paca_dist = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.))\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for name, module in model.named_modules():\n",
    "            if \"conv1\" in name and hasattr(module, \"pruning_layer\"):\n",
    "                pruning_layer = module.pruning_layer\n",
    "                if hasattr(pruning_layer, \"metric_fn\") and \"PACAPattern\" in pruning_layer.metric_fn.__class__.__name__:\n",
    "                    metric_fn = pruning_layer.metric_fn\n",
    "                    if metric_fn.dominant_patterns is not None:\n",
    "                        num_paca_patterns = metric_fn.dominant_patterns.shape[0]\n",
    "\n",
    "                    total_dist = metric_fn(module.weight).item()\n",
    "                    num_kernels = module.weight.shape[0]\n",
    "                    avg_paca_dist = total_dist / num_kernels if num_kernels > 0 else 0\n",
    "                    break\n",
    "\n",
    "        ratio = get_layer_keep_ratio(model)\n",
    "        print(f'Accuracy: {100 * correct / total:.2f}%, '\n",
    "              f'Remaining Weights: {ratio * 100:.2f}%, '\n",
    "              f'Num Patterns: {num_paca_patterns}, '\n",
    "              f'Avg Pattern Dist: {avg_paca_dist:.4f}')\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader, val_loader = get_cifar10_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28865b-afdb-4773-be30-717486d9786a",
   "metadata": {},
   "source": [
    "## Create loss function, scheduler and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f88af88-ef7a-4d30-8cff-f39225d5a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850c23a-2abc-4904-9c69-859492b450a8",
   "metadata": {},
   "source": [
    "## Train model\n",
    "Training time. We use the train_compressed_model function from pquant to train. We need to provide some parameters such as training and validation functions, their input parameters, the model and the config file. The function automatically adds pruning layers and replaces activations with a quantized variant, trains the model, and removes the pruning layers after training is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18f2414c-1f4d-4a30-a143-920497e60ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.3925909399986267\n",
      "Model Losses: 94639.1484375\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 0 ---\n",
      "Num Patterns: 16, Avg Pattern Dist: 0.1093\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 10.00%, Remaining Weights: 100.00%, Num Patterns: 1, Avg Pattern Dist: nan\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: nan\n",
      "Model Losses: nan\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 197 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: nan\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 10.00%, Remaining Weights: 100.00%, Num Patterns: 1, Avg Pattern Dist: nan\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: nan\n",
      "Model Losses: nan\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 394 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: nan\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 10.00%, Remaining Weights: 100.00%, Num Patterns: 1, Avg Pattern Dist: nan\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: nan\n",
      "Model Losses: nan\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 591 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: nan\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 10.00%, Remaining Weights: 100.00%, Num Patterns: 1, Avg Pattern Dist: nan\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: nan\n",
      "Model Losses: nan\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1773 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: nan\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 10.00%, Remaining Weights: 100.00%, Num Patterns: 1, Avg Pattern Dist: nan\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: nan\n",
      "Model Losses: nan\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1970 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: nan\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 10.00%, Remaining Weights: 100.00%, Num Patterns: 1, Avg Pattern Dist: nan\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: nan\n",
      "Model Losses: nan\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 2167 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: nan\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iterative_train\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mInputs to train_resnet we defined previously are:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m          model, trainloader, device, loss_func, epoch, optimizer, scheduler, **kwargs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43miterative_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_resnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvalid_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidate_resnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/train.py:8\u001b[0m, in \u001b[0;36miterative_train\u001b[0;34m(model, config, train_func, valid_func, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mbackend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_impl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_torch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iterative_train_torch\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterative_train_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_impl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iterative_train_tf\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/train_torch.py:35\u001b[0m, in \u001b[0;36miterative_train_torch\u001b[0;34m(model, config, train_func, valid_func, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     save_weights_functions(model)\n\u001b[1;32m     34\u001b[0m pre_epoch_functions(model, e, training_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 35\u001b[0m \u001b[43mtrain_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     37\u001b[0m valid_func(model, epoch\u001b[38;5;241m=\u001b[39mepoch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m, in \u001b[0;36mtrain_resnet\u001b[0;34m(model, trainloader, device, loss_func, epoch, optimizer, scheduler, *args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m quantizer(inputs, k\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.\u001b[39m), i\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m), f\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m7.\u001b[39m)) \u001b[38;5;66;03m# 8 bits input quantization\u001b[39;00m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(outputs, labels)\n\u001b[1;32m     10\u001b[0m losses \u001b[38;5;241m=\u001b[39m get_model_losses(model, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torchvision/models/resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torchvision/models/resnet.py:96\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m---> 96\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/compressed_layers_torch.py:161\u001b[0m, in \u001b[0;36mCompressedLayerConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 161\u001b[0m     weight, bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune_and_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpruning_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwanda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpruning_layer\u001b[38;5;241m.\u001b[39mcollect_input(x, weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/compressed_layers_torch.py:119\u001b[0m, in \u001b[0;36mCompressedLayerBase.prune_and_quantize\u001b[0;34m(self, weight, bias)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     weight, bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantize(weight, bias)\n\u001b[0;32m--> 119\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weight, bias\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/compressed_layers_torch.py:110\u001b[0m, in \u001b[0;36mCompressedLayerBase.prune\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprune\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_pruning:\n\u001b[0;32m--> 110\u001b[0m         weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpruning_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weight\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/layers/layer.py:936\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/backend/torch/layer.py:41\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/ops/operation.py:58\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     54\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     55\u001b[0m         call_fn,\n\u001b[1;32m     56\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     57\u001b[0m     )\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:516\u001b[0m, in \u001b[0;36mMDMM.call\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    514\u001b[0m         weight \u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_hard_mask(weight)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstraint_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weight\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/layers/layer.py:936\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/backend/torch/layer.py:41\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/ops/operation.py:58\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     54\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     55\u001b[0m         call_fn,\n\u001b[1;32m     56\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     57\u001b[0m     )\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:61\u001b[0m, in \u001b[0;36mConstraint.call\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight):\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculates the penalty from a given infeasibility measure.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     raw_infeasibility \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_infeasibility\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     infeasibility \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe_infeasibility(raw_infeasibility)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_grad_:\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:108\u001b[0m, in \u001b[0;36mEqualityConstraint.get_infeasibility\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_infeasibility\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight):\n\u001b[0;32m--> 108\u001b[0m     metric_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     infeasibility \u001b[38;5;241m=\u001b[39m metric_value \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_value\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# return ops.abs(infeasibility)\u001b[39;00m\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:411\u001b[0m, in \u001b[0;36mPACAPatternMetric.__call__\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weight\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# For non-conv layers, return zero loss.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_dominant_patterns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pattern_distances(weight)\n\u001b[1;32m    413\u001b[0m min_distances \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mmin(distances, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:362\u001b[0m, in \u001b[0;36mPACAPatternMetric._select_dominant_patterns\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m    359\u001b[0m sorted_unique_patterns \u001b[38;5;241m=\u001b[39m unique_patterns[sorted_indices_pdf]\n\u001b[1;32m    361\u001b[0m cdf \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcumsum(sorted_pdf)\n\u001b[0;32m--> 362\u001b[0m indices_where_cdf_exceeds_beta \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mshape(indices_where_cdf_exceeds_beta)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    364\u001b[0m     n_beta \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mshape(cdf)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/ops/numpy.py:6183\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(condition, x1, x2)\u001b[0m\n\u001b[1;32m   6181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((condition, x1, x2)):\n\u001b[1;32m   6182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Where()\u001b[38;5;241m.\u001b[39msymbolic_call(condition, x1, x2)\n\u001b[0;32m-> 6183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/backend/torch/numpy.py:1605\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(condition, x1, x2)\u001b[0m\n\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mwhere(condition, x1, x2)\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pquant import iterative_train\n",
    "\"\"\"\n",
    "Inputs to train_resnet we defined previously are:\n",
    "          model, trainloader, device, loss_func, epoch, optimizer, scheduler, **kwargs\n",
    "\"\"\"\n",
    "\n",
    "trained_model_paca = iterative_train(model = model, \n",
    "                                config = config, \n",
    "                                train_func = train_resnet, \n",
    "                                valid_func = validate_resnet, \n",
    "                                trainloader = train_loader, \n",
    "                                testloader = val_loader, \n",
    "                                device = device, \n",
    "                                loss_func = loss_function,\n",
    "                                optimizer = optimizer, \n",
    "                                scheduler = scheduler\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30cdc72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_conv_patterns(model, 'layer1.0.conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f651848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pquant-gpu-env]",
   "language": "python",
   "name": "conda-env-pquant-gpu-env-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
