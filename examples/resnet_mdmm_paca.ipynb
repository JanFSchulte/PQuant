{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5676e100-c255-4871-b167-01a788309112",
   "metadata": {},
   "source": [
    "## In this tutorial we create a CNN and dataloaders, and train / prune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27197caf-85a2-48b7-af76-a5ff943408ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\" # Needs to be set, some pruning layers as well as the quantizers are Keras\n",
    "import keras\n",
    "keras.config.set_backend(\"torch\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "keras.backend.set_image_data_format(\"channels_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e520e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pquant\n",
      "data\n",
      "smartpixels\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.chdir(\"/home/das214/PQuant/mdmm_dev/src\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for f in os.listdir(os.getcwd()):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea5a763-a029-495d-a03a-390048d749f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd71c9-86b2-4911-aa71-bd18b1e75aa1",
   "metadata": {},
   "source": [
    "## Add pruning and quantization\n",
    "Begin prunning with MDMM pruning with Unstructured Sparsity metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ec145f1-502c-4fd0-84ed-e87b84a27374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "batch_size": 128,
       "cosine_tmax": 200,
       "gamma": 0.1,
       "l2_decay": 0.0001,
       "label_smoothing": 0,
       "lr": 0.001,
       "lr_schedule": "multistep",
       "milestones": [
        75,
        120
       ],
       "momentum": 0.9,
       "optimizer": "sgd",
       "plot_frequency": 100,
       "pruning_parameters": {
        "constraint_type": "Equality",
        "damping": 1,
        "disable_pruning_for_layers": [
         null
        ],
        "enable_pruning": true,
        "epsilon": 0.001,
        "l0_mode": "coarse",
        "metric_type": "UnstructuredSparsity",
        "pruning_method": "mdmm",
        "rf": 1,
        "scale": 50,
        "target_sparsity": 0.9,
        "target_value": 0,
        "use_grad": false
       },
       "quantization_parameters": {
        "default_fractional_bits": 7,
        "default_integer_bits": 0,
        "enable_quantization": false,
        "hgq_gamma": 0.0003,
        "hgq_heterogeneous": true,
        "layer_specific": [],
        "use_high_granularity_quantization": false,
        "use_real_tanh": false,
        "use_symmetric_quantization": false
       },
       "training_parameters": {
        "epochs": 100,
        "fine_tuning_epochs": 30,
        "pretraining_epochs": 0,
        "pruning_first": false,
        "rewind": "never",
        "rounds": 1,
        "save_weights_epoch": -1
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pquant import get_default_config\n",
    "from IPython.display import JSON\n",
    "\n",
    "pruning_method = \"mdmm\"\n",
    "config = get_default_config(pruning_method)\n",
    "JSON(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ef3115-2f3d-43e1-a199-4a19d667f796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): CompressedLayerConv2d(\n",
       "    (pruning_layer): <MDMM name=mdmm, built=True>\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_1, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_2, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_3, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_4, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_5, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_6, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_7, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_8, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_9, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_10, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_11, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_12, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_13, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_14, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_15, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_16, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): CompressedLayerConv2d(\n",
       "          (pruning_layer): <MDMM name=mdmm_17, built=True>\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_18, built=True>\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): CompressedLayerConv2d(\n",
       "        (pruning_layer): <MDMM name=mdmm_19, built=True>\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): CompressedLayerLinear(\n",
       "    (pruning_layer): <MDMM name=mdmm_20, built=True>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace layers with compressed layers\n",
    "from pquant import add_compression_layers\n",
    "input_shape = (256,3,32,32)\n",
    "model = add_compression_layers(model, config, input_shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82dd0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from pquant import get_layer_keep_ratio, get_model_losses\n",
    "from quantizers.fixed_point.fixed_point_ops import get_fixed_quantizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_cifar10_data(batch_size):\n",
    "    normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), \n",
    "                                          transforms.ToTensor(), normalize])\n",
    "    test_transform = transforms.Compose([transforms.ToTensor(), normalize])  \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "    valset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=test_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Set up input quantizer\n",
    "quantizer = get_fixed_quantizer(overflow_mode=\"SAT\")\n",
    "\n",
    "def train_resnet(model, trainloader, device, loss_func,\n",
    "                 epoch, optimizer, scheduler, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    One epoch of training with a live ETA/throughput bar.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(trainloader,\n",
    "              desc=f\"Train ‖ Epoch {epoch}\",\n",
    "              total=len(trainloader),\n",
    "              unit=\"batch\",\n",
    "              dynamic_ncols=True) as pbar:\n",
    "\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.))\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)              # cleaner gradient reset\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            losses = get_model_losses(model, torch.tensor(0.).to(device))\n",
    "            loss += losses\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f} \")\n",
    "        \n",
    "    # ----- Diagnostics on Last mini-batch -----\n",
    "    print(f\"Loss={loss_func(outputs, labels).item():.4f} | Reg={loss.item() - loss_func(outputs, labels).item():.4f}\")\n",
    "\n",
    "def validate_resnet(model, testloader, device, loss_func, epoch, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Validation with progress bar and accuracy summary.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(testloader,\n",
    "                  desc=f\"Val   ‖ Epoch {epoch}\",\n",
    "                  total=len(testloader),\n",
    "                  unit=\"batch\",\n",
    "                  dynamic_ncols=True) as pbar:\n",
    "\n",
    "            for inputs, labels in pbar:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.))\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "                running_acc = 100. * correct / total\n",
    "                pbar.set_postfix(acc=f\"{running_acc:.2f}%\")\n",
    "\n",
    "    ratio = get_layer_keep_ratio(model)\n",
    "    print(f\"Accuracy: {correct/total*100:.2f}% | Remaining weights: {ratio*100:.2f}% \\n\")\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader, val_loader = get_cifar10_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cff4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e20af1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 0: 100%|██████████| 196/196 [00:09<00:00, 21.57batch/s, loss=12.5569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.5907 | Reg=10.9662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 0: 100%|██████████| 196/196 [00:05<00:00, 34.39batch/s, acc=48.69%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.69% | Remaining weights: 96.48% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 1: 100%|██████████| 196/196 [00:08<00:00, 22.19batch/s, loss=23.6727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.6253 | Reg=22.0474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 1: 100%|██████████| 196/196 [00:05<00:00, 33.27batch/s, acc=46.40%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.40% | Remaining weights: 95.73% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 2: 100%|██████████| 196/196 [00:08<00:00, 21.79batch/s, loss=24.1359]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2486 | Reg=22.8873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 2: 100%|██████████| 196/196 [00:05<00:00, 32.75batch/s, acc=58.98%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.98% | Remaining weights: 94.28% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 3: 100%|██████████| 196/196 [00:09<00:00, 21.33batch/s, loss=25.6743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2423 | Reg=24.4321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 3: 100%|██████████| 196/196 [00:05<00:00, 33.91batch/s, acc=49.23%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.23% | Remaining weights: 93.10% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 4: 100%|██████████| 196/196 [00:08<00:00, 22.19batch/s, loss=21.7558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1369 | Reg=20.6189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 4: 100%|██████████| 196/196 [00:05<00:00, 33.59batch/s, acc=63.22%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.22% | Remaining weights: 90.61% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 5: 100%|██████████| 196/196 [00:08<00:00, 22.19batch/s, loss=23.3111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.3556 | Reg=21.9555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 5: 100%|██████████| 196/196 [00:05<00:00, 33.84batch/s, acc=47.32%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.32% | Remaining weights: 89.48% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 6: 100%|██████████| 196/196 [00:08<00:00, 22.20batch/s, loss=18.8451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0086 | Reg=17.8365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 6: 100%|██████████| 196/196 [00:05<00:00, 33.19batch/s, acc=65.21%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.21% | Remaining weights: 86.23% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 7: 100%|██████████| 196/196 [00:08<00:00, 22.30batch/s, loss=20.7098]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.5671 | Reg=19.1427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 7: 100%|██████████| 196/196 [00:05<00:00, 32.79batch/s, acc=46.60%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 46.60% | Remaining weights: 85.32% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 8: 100%|██████████| 196/196 [00:08<00:00, 22.64batch/s, loss=16.6514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0684 | Reg=15.5830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 8: 100%|██████████| 196/196 [00:06<00:00, 31.80batch/s, acc=65.24%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.24% | Remaining weights: 81.54% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 9: 100%|██████████| 196/196 [00:08<00:00, 22.31batch/s, loss=18.1131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.4194 | Reg=16.6937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 9: 100%|██████████| 196/196 [00:06<00:00, 32.59batch/s, acc=34.01%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 34.01% | Remaining weights: 80.79% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 10: 100%|██████████| 196/196 [00:08<00:00, 22.40batch/s, loss=14.8065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0989 | Reg=13.7076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 10: 100%|██████████| 196/196 [00:06<00:00, 32.09batch/s, acc=63.04%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.04% | Remaining weights: 76.69% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 11: 100%|██████████| 196/196 [00:08<00:00, 21.94batch/s, loss=16.1760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.4800 | Reg=14.6960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 11: 100%|██████████| 196/196 [00:06<00:00, 31.47batch/s, acc=45.40%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 45.40% | Remaining weights: 76.08% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 12: 100%|██████████| 196/196 [00:08<00:00, 22.70batch/s, loss=13.2821]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1915 | Reg=12.0905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 12: 100%|██████████| 196/196 [00:06<00:00, 32.55batch/s, acc=64.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.54% | Remaining weights: 71.83% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 13: 100%|██████████| 196/196 [00:08<00:00, 22.77batch/s, loss=14.4506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.6422 | Reg=12.8083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 13: 100%|██████████| 196/196 [00:05<00:00, 32.96batch/s, acc=44.43%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 44.43% | Remaining weights: 71.26% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 14: 100%|██████████| 196/196 [00:08<00:00, 22.63batch/s, loss=11.7791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1684 | Reg=10.6107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 14: 100%|██████████| 196/196 [00:05<00:00, 33.20batch/s, acc=64.63%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.63% | Remaining weights: 67.00% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 15: 100%|██████████| 196/196 [00:08<00:00, 22.07batch/s, loss=12.2021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9230 | Reg=11.2791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 15: 100%|██████████| 196/196 [00:05<00:00, 33.40batch/s, acc=47.33%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.33% | Remaining weights: 66.57% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 16: 100%|██████████| 196/196 [00:08<00:00, 22.41batch/s, loss=10.6216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2424 | Reg=9.3792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 16: 100%|██████████| 196/196 [00:05<00:00, 34.13batch/s, acc=60.74%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.74% | Remaining weights: 62.43% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 17: 100%|██████████| 196/196 [00:08<00:00, 22.16batch/s, loss=11.0581]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.4215 | Reg=9.6366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 17: 100%|██████████| 196/196 [00:05<00:00, 34.30batch/s, acc=49.07%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.07% | Remaining weights: 61.78% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 18: 100%|██████████| 196/196 [00:08<00:00, 22.19batch/s, loss=9.4311] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1772 | Reg=8.2539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 18: 100%|██████████| 196/196 [00:05<00:00, 34.15batch/s, acc=58.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.54% | Remaining weights: 58.03% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 19: 100%|██████████| 196/196 [00:08<00:00, 22.22batch/s, loss=9.3311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0582 | Reg=8.2729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 19: 100%|██████████| 196/196 [00:05<00:00, 34.44batch/s, acc=57.51%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.51% | Remaining weights: 57.18% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 20: 100%|██████████| 196/196 [00:08<00:00, 22.29batch/s, loss=8.3926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0736 | Reg=7.3190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 20: 100%|██████████| 196/196 [00:05<00:00, 34.44batch/s, acc=63.55%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.55% | Remaining weights: 53.96% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 21: 100%|██████████| 196/196 [00:08<00:00, 22.15batch/s, loss=8.3117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0907 | Reg=7.2211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 21: 100%|██████████| 196/196 [00:05<00:00, 34.42batch/s, acc=50.77%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.77% | Remaining weights: 52.97% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 22: 100%|██████████| 196/196 [00:08<00:00, 22.52batch/s, loss=7.4952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0336 | Reg=6.4616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 22: 100%|██████████| 196/196 [00:05<00:00, 34.59batch/s, acc=62.86%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.86% | Remaining weights: 50.20% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 23: 100%|██████████| 196/196 [00:08<00:00, 22.47batch/s, loss=7.1120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9123 | Reg=6.1998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 23: 100%|██████████| 196/196 [00:05<00:00, 33.94batch/s, acc=61.68%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.68% | Remaining weights: 48.97% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 24: 100%|██████████| 196/196 [00:08<00:00, 21.89batch/s, loss=6.8565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1453 | Reg=5.7112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 24: 100%|██████████| 196/196 [00:05<00:00, 34.01batch/s, acc=61.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.50% | Remaining weights: 46.70% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 25: 100%|██████████| 196/196 [00:08<00:00, 22.40batch/s, loss=6.3544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0213 | Reg=5.3331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 25: 100%|██████████| 196/196 [00:05<00:00, 33.62batch/s, acc=56.23%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.23% | Remaining weights: 45.34% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 26: 100%|██████████| 196/196 [00:08<00:00, 22.66batch/s, loss=6.2207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0859 | Reg=5.1348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 26: 100%|██████████| 196/196 [00:05<00:00, 33.01batch/s, acc=58.40%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.40% | Remaining weights: 43.69% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 27: 100%|██████████| 196/196 [00:08<00:00, 22.56batch/s, loss=5.5656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0317 | Reg=4.5339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 27: 100%|██████████| 196/196 [00:06<00:00, 32.44batch/s, acc=67.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.00% | Remaining weights: 42.02% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 28: 100%|██████████| 196/196 [00:08<00:00, 22.78batch/s, loss=5.4303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9366 | Reg=4.4937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 28: 100%|██████████| 196/196 [00:05<00:00, 32.72batch/s, acc=52.65%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.65% | Remaining weights: 40.77% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 29: 100%|██████████| 196/196 [00:08<00:00, 22.60batch/s, loss=4.8491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9606 | Reg=3.8884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 29: 100%|██████████| 196/196 [00:05<00:00, 32.75batch/s, acc=64.94%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.94% | Remaining weights: 39.11% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 30: 100%|██████████| 196/196 [00:08<00:00, 22.49batch/s, loss=5.2010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1005 | Reg=4.1006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 30: 100%|██████████| 196/196 [00:05<00:00, 32.98batch/s, acc=56.80%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.80% | Remaining weights: 38.32% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 31: 100%|██████████| 196/196 [00:08<00:00, 22.50batch/s, loss=4.2932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9594 | Reg=3.3338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 31: 100%|██████████| 196/196 [00:05<00:00, 33.29batch/s, acc=70.87%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.87% | Remaining weights: 36.53% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 32: 100%|██████████| 196/196 [00:08<00:00, 22.40batch/s, loss=4.6615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9903 | Reg=3.6712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 32: 100%|██████████| 196/196 [00:05<00:00, 33.43batch/s, acc=55.49%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.49% | Remaining weights: 36.04% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 33: 100%|██████████| 196/196 [00:08<00:00, 22.54batch/s, loss=3.9340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0267 | Reg=2.9073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 33: 100%|██████████| 196/196 [00:05<00:00, 32.91batch/s, acc=70.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.47% | Remaining weights: 34.28% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 34: 100%|██████████| 196/196 [00:08<00:00, 22.36batch/s, loss=4.6199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.3322 | Reg=3.2876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 34: 100%|██████████| 196/196 [00:05<00:00, 33.86batch/s, acc=61.24%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.24% | Remaining weights: 33.98% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 35: 100%|██████████| 196/196 [00:08<00:00, 22.17batch/s, loss=3.4862]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9595 | Reg=2.5267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 35: 100%|██████████| 196/196 [00:05<00:00, 33.46batch/s, acc=71.93%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.93% | Remaining weights: 32.26% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 36: 100%|██████████| 196/196 [00:08<00:00, 22.32batch/s, loss=4.1828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0956 | Reg=3.0872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 36: 100%|██████████| 196/196 [00:05<00:00, 33.15batch/s, acc=50.88%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.88% | Remaining weights: 32.33% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 37: 100%|██████████| 196/196 [00:08<00:00, 22.23batch/s, loss=2.9709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7458 | Reg=2.2250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 37: 100%|██████████| 196/196 [00:06<00:00, 32.62batch/s, acc=73.48%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.48% | Remaining weights: 30.51% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 38: 100%|██████████| 196/196 [00:08<00:00, 22.55batch/s, loss=3.8004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0098 | Reg=2.7906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 38: 100%|██████████| 196/196 [00:05<00:00, 33.03batch/s, acc=59.91%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.91% | Remaining weights: 30.66% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 39: 100%|██████████| 196/196 [00:08<00:00, 22.67batch/s, loss=2.8207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8570 | Reg=1.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 39: 100%|██████████| 196/196 [00:05<00:00, 33.35batch/s, acc=74.76%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.76% | Remaining weights: 28.87% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 40: 100%|██████████| 196/196 [00:08<00:00, 22.31batch/s, loss=3.5816]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0123 | Reg=2.5694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 40: 100%|██████████| 196/196 [00:05<00:00, 33.18batch/s, acc=56.18%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.18% | Remaining weights: 29.24% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 41: 100%|██████████| 196/196 [00:08<00:00, 22.52batch/s, loss=2.7980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0271 | Reg=1.7709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 41: 100%|██████████| 196/196 [00:05<00:00, 33.61batch/s, acc=76.05%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.05% | Remaining weights: 27.52% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 42: 100%|██████████| 196/196 [00:08<00:00, 22.41batch/s, loss=3.5468]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0777 | Reg=2.4691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 42: 100%|██████████| 196/196 [00:05<00:00, 33.37batch/s, acc=48.86%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 48.86% | Remaining weights: 28.09% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 43: 100%|██████████| 196/196 [00:08<00:00, 22.31batch/s, loss=2.4236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8201 | Reg=1.6036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 43: 100%|██████████| 196/196 [00:05<00:00, 33.65batch/s, acc=75.91%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.91% | Remaining weights: 26.31% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 44: 100%|██████████| 196/196 [00:08<00:00, 22.54batch/s, loss=3.3656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0814 | Reg=2.2842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 44: 100%|██████████| 196/196 [00:05<00:00, 33.30batch/s, acc=59.11%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.11% | Remaining weights: 26.89% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 45: 100%|██████████| 196/196 [00:08<00:00, 22.42batch/s, loss=2.4307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9865 | Reg=1.4442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 45: 100%|██████████| 196/196 [00:05<00:00, 33.76batch/s, acc=75.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.85% | Remaining weights: 25.16% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 46: 100%|██████████| 196/196 [00:08<00:00, 22.47batch/s, loss=3.2215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1123 | Reg=2.1092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 46: 100%|██████████| 196/196 [00:05<00:00, 33.39batch/s, acc=56.76%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56.76% | Remaining weights: 25.76% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 47: 100%|██████████| 196/196 [00:08<00:00, 22.75batch/s, loss=2.0210]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6959 | Reg=1.3251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 47: 100%|██████████| 196/196 [00:05<00:00, 33.34batch/s, acc=76.34%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.34% | Remaining weights: 24.17% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 48: 100%|██████████| 196/196 [00:08<00:00, 22.39batch/s, loss=2.8264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7930 | Reg=2.0334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 48: 100%|██████████| 196/196 [00:05<00:00, 33.21batch/s, acc=50.36%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.36% | Remaining weights: 24.95% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 49: 100%|██████████| 196/196 [00:08<00:00, 22.55batch/s, loss=2.0268]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8074 | Reg=1.2194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 49: 100%|██████████| 196/196 [00:05<00:00, 32.93batch/s, acc=76.62%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.62% | Remaining weights: 23.29% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 50: 100%|██████████| 196/196 [00:08<00:00, 22.01batch/s, loss=2.9517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0690 | Reg=1.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 50: 100%|██████████| 196/196 [00:05<00:00, 33.66batch/s, acc=52.14%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 52.14% | Remaining weights: 24.00% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 51: 100%|██████████| 196/196 [00:08<00:00, 22.14batch/s, loss=2.0516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9250 | Reg=1.1266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 51: 100%|██████████| 196/196 [00:05<00:00, 33.41batch/s, acc=76.93%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.93% | Remaining weights: 22.54% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 52: 100%|██████████| 196/196 [00:08<00:00, 21.91batch/s, loss=3.0847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2794 | Reg=1.8053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 52: 100%|██████████| 196/196 [00:05<00:00, 33.14batch/s, acc=59.37%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.37% | Remaining weights: 23.32% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 53: 100%|██████████| 196/196 [00:08<00:00, 22.07batch/s, loss=1.7152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6638 | Reg=1.0514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 53: 100%|██████████| 196/196 [00:05<00:00, 34.42batch/s, acc=76.95%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.95% | Remaining weights: 21.83% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 54: 100%|██████████| 196/196 [00:08<00:00, 21.99batch/s, loss=2.6087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9326 | Reg=1.6761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 54: 100%|██████████| 196/196 [00:05<00:00, 33.97batch/s, acc=57.81%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.81% | Remaining weights: 22.55% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 55: 100%|██████████| 196/196 [00:08<00:00, 21.85batch/s, loss=1.8267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8346 | Reg=0.9921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 55: 100%|██████████| 196/196 [00:05<00:00, 33.87batch/s, acc=76.37%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.37% | Remaining weights: 21.20% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 56: 100%|██████████| 196/196 [00:08<00:00, 22.16batch/s, loss=2.7221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1089 | Reg=1.6132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 56: 100%|██████████| 196/196 [00:05<00:00, 33.51batch/s, acc=58.40%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.40% | Remaining weights: 21.87% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 57: 100%|██████████| 196/196 [00:08<00:00, 21.79batch/s, loss=1.7224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7738 | Reg=0.9486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 57: 100%|██████████| 196/196 [00:05<00:00, 33.17batch/s, acc=76.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.06% | Remaining weights: 20.64% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 58: 100%|██████████| 196/196 [00:08<00:00, 22.03batch/s, loss=2.5730]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0545 | Reg=1.5185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 58: 100%|██████████| 196/196 [00:05<00:00, 34.21batch/s, acc=61.92%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.92% | Remaining weights: 21.21% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 59: 100%|██████████| 196/196 [00:08<00:00, 21.87batch/s, loss=1.7344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8336 | Reg=0.9008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 59: 100%|██████████| 196/196 [00:05<00:00, 34.05batch/s, acc=75.90%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.90% | Remaining weights: 20.13% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 60: 100%|██████████| 196/196 [00:08<00:00, 22.14batch/s, loss=2.1426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8096 | Reg=1.3330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 60: 100%|██████████| 196/196 [00:05<00:00, 34.25batch/s, acc=64.20%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.20% | Remaining weights: 20.46% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 61: 100%|██████████| 196/196 [00:08<00:00, 22.20batch/s, loss=1.6669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7773 | Reg=0.8896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 61: 100%|██████████| 196/196 [00:05<00:00, 33.88batch/s, acc=76.04%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.04% | Remaining weights: 19.67% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 62: 100%|██████████| 196/196 [00:08<00:00, 21.90batch/s, loss=2.3786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0781 | Reg=1.3006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 62: 100%|██████████| 196/196 [00:05<00:00, 34.58batch/s, acc=62.70%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.70% | Remaining weights: 19.94% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 63: 100%|██████████| 196/196 [00:08<00:00, 22.17batch/s, loss=1.7186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8472 | Reg=0.8713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 63: 100%|██████████| 196/196 [00:05<00:00, 33.74batch/s, acc=75.12%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.12% | Remaining weights: 19.30% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 64: 100%|██████████| 196/196 [00:08<00:00, 21.92batch/s, loss=1.8186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6483 | Reg=1.1704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 64: 100%|██████████| 196/196 [00:05<00:00, 33.43batch/s, acc=65.03%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.03% | Remaining weights: 19.37% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 65: 100%|██████████| 196/196 [00:08<00:00, 21.94batch/s, loss=1.6949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8148 | Reg=0.8800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 65: 100%|██████████| 196/196 [00:05<00:00, 34.24batch/s, acc=73.16%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.16% | Remaining weights: 18.97% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 66: 100%|██████████| 196/196 [00:08<00:00, 22.08batch/s, loss=1.7684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7264 | Reg=1.0420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 66: 100%|██████████| 196/196 [00:05<00:00, 34.16batch/s, acc=60.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.39% | Remaining weights: 18.83% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 67: 100%|██████████| 196/196 [00:09<00:00, 21.58batch/s, loss=1.8772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9986 | Reg=0.8785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 67: 100%|██████████| 196/196 [00:05<00:00, 34.78batch/s, acc=73.15%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.15% | Remaining weights: 18.71% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 68: 100%|██████████| 196/196 [00:08<00:00, 21.93batch/s, loss=1.7490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7728 | Reg=0.9762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 68: 100%|██████████| 196/196 [00:05<00:00, 32.73batch/s, acc=66.81%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.81% | Remaining weights: 18.43% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 69: 100%|██████████| 196/196 [00:08<00:00, 22.22batch/s, loss=1.7018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7817 | Reg=0.9201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 69: 100%|██████████| 196/196 [00:05<00:00, 33.41batch/s, acc=70.87%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.87% | Remaining weights: 18.44% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 70: 100%|██████████| 196/196 [00:09<00:00, 21.62batch/s, loss=1.7932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9157 | Reg=0.8775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 70: 100%|██████████| 196/196 [00:05<00:00, 33.60batch/s, acc=72.14%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.14% | Remaining weights: 18.00% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 71: 100%|██████████| 196/196 [00:09<00:00, 21.41batch/s, loss=1.6483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7306 | Reg=0.9177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 71: 100%|██████████| 196/196 [00:05<00:00, 33.62batch/s, acc=72.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.85% | Remaining weights: 18.24% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 72: 100%|██████████| 196/196 [00:09<00:00, 21.68batch/s, loss=1.4236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6019 | Reg=0.8218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 72: 100%|██████████| 196/196 [00:05<00:00, 33.05batch/s, acc=72.34%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.34% | Remaining weights: 17.66% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 73: 100%|██████████| 196/196 [00:09<00:00, 21.71batch/s, loss=1.7652]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8196 | Reg=0.9457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 73: 100%|██████████| 196/196 [00:05<00:00, 33.67batch/s, acc=73.33%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.33% | Remaining weights: 18.05% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 74: 100%|██████████| 196/196 [00:09<00:00, 21.71batch/s, loss=1.3982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6983 | Reg=0.6999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 74: 100%|██████████| 196/196 [00:05<00:00, 34.29batch/s, acc=71.88%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.88% | Remaining weights: 17.30% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 75: 100%|██████████| 196/196 [00:09<00:00, 21.48batch/s, loss=1.8827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8910 | Reg=0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 75: 100%|██████████| 196/196 [00:05<00:00, 34.05batch/s, acc=70.08%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.08% | Remaining weights: 17.90% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 76: 100%|██████████| 196/196 [00:09<00:00, 21.77batch/s, loss=1.4403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7861 | Reg=0.6542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 76: 100%|██████████| 196/196 [00:05<00:00, 33.23batch/s, acc=72.68%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.68% | Remaining weights: 17.04% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 77: 100%|██████████| 196/196 [00:09<00:00, 21.75batch/s, loss=1.9697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9656 | Reg=1.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 77: 100%|██████████| 196/196 [00:05<00:00, 33.25batch/s, acc=68.33%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.33% | Remaining weights: 17.72% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 78: 100%|██████████| 196/196 [00:09<00:00, 21.37batch/s, loss=1.4737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8678 | Reg=0.6059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 78: 100%|██████████| 196/196 [00:05<00:00, 33.93batch/s, acc=75.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.54% | Remaining weights: 16.80% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 79: 100%|██████████| 196/196 [00:08<00:00, 21.78batch/s, loss=2.2365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.1580 | Reg=1.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 79: 100%|██████████| 196/196 [00:05<00:00, 33.54batch/s, acc=67.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.06% | Remaining weights: 17.59% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 80: 100%|██████████| 196/196 [00:09<00:00, 21.33batch/s, loss=1.7976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.2114 | Reg=0.5863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 80: 100%|██████████| 196/196 [00:05<00:00, 33.11batch/s, acc=76.58%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.58% | Remaining weights: 16.63% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 81: 100%|██████████| 196/196 [00:08<00:00, 21.79batch/s, loss=2.0168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9576 | Reg=1.0592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 81: 100%|██████████| 196/196 [00:05<00:00, 33.90batch/s, acc=66.81%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.81% | Remaining weights: 17.48% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 82: 100%|██████████| 196/196 [00:08<00:00, 21.86batch/s, loss=1.3086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7541 | Reg=0.5544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 82: 100%|██████████| 196/196 [00:05<00:00, 34.22batch/s, acc=78.07%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.07% | Remaining weights: 16.50% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 83: 100%|██████████| 196/196 [00:08<00:00, 22.02batch/s, loss=1.9213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8088 | Reg=1.1126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 83: 100%|██████████| 196/196 [00:05<00:00, 34.38batch/s, acc=64.29%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.29% | Remaining weights: 17.41% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 84: 100%|██████████| 196/196 [00:08<00:00, 21.88batch/s, loss=1.2656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7118 | Reg=0.5538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 84: 100%|██████████| 196/196 [00:05<00:00, 34.55batch/s, acc=78.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.82% | Remaining weights: 16.36% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 85: 100%|██████████| 196/196 [00:08<00:00, 22.22batch/s, loss=2.1934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0898 | Reg=1.1036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 85: 100%|██████████| 196/196 [00:05<00:00, 34.09batch/s, acc=63.74%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.74% | Remaining weights: 17.24% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 86: 100%|██████████| 196/196 [00:08<00:00, 22.00batch/s, loss=1.0818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5437 | Reg=0.5382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 86: 100%|██████████| 196/196 [00:05<00:00, 34.04batch/s, acc=78.51%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.51% | Remaining weights: 16.23% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 87: 100%|██████████| 196/196 [00:08<00:00, 22.10batch/s, loss=1.9287]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7819 | Reg=1.1468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 87: 100%|██████████| 196/196 [00:05<00:00, 33.97batch/s, acc=63.45%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.45% | Remaining weights: 17.13% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 88: 100%|██████████| 196/196 [00:08<00:00, 22.36batch/s, loss=1.2912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7660 | Reg=0.5252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 88: 100%|██████████| 196/196 [00:05<00:00, 33.63batch/s, acc=80.08%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.08% | Remaining weights: 16.07% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 89: 100%|██████████| 196/196 [00:08<00:00, 22.06batch/s, loss=2.1742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=1.0397 | Reg=1.1345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 89: 100%|██████████| 196/196 [00:05<00:00, 33.64batch/s, acc=64.07%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.07% | Remaining weights: 17.01% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 90: 100%|██████████| 196/196 [00:08<00:00, 22.20batch/s, loss=1.1990]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6615 | Reg=0.5375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 90: 100%|██████████| 196/196 [00:05<00:00, 33.04batch/s, acc=80.40%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.40% | Remaining weights: 16.00% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 91: 100%|██████████| 196/196 [00:08<00:00, 22.05batch/s, loss=2.1473]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9355 | Reg=1.2118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 91: 100%|██████████| 196/196 [00:05<00:00, 32.98batch/s, acc=64.06%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.06% | Remaining weights: 17.00% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 92: 100%|██████████| 196/196 [00:08<00:00, 22.13batch/s, loss=1.2941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7472 | Reg=0.5469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 92: 100%|██████████| 196/196 [00:05<00:00, 33.06batch/s, acc=80.66%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.66% | Remaining weights: 15.89% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 93: 100%|██████████| 196/196 [00:08<00:00, 21.95batch/s, loss=2.0157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8476 | Reg=1.1680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 93: 100%|██████████| 196/196 [00:05<00:00, 33.03batch/s, acc=61.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.17% | Remaining weights: 16.80% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 94: 100%|██████████| 196/196 [00:08<00:00, 22.27batch/s, loss=1.2623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7274 | Reg=0.5349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 94: 100%|██████████| 196/196 [00:05<00:00, 33.29batch/s, acc=81.02%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.02% | Remaining weights: 15.75% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 95: 100%|██████████| 196/196 [00:08<00:00, 22.14batch/s, loss=1.8916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7252 | Reg=1.1664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 95: 100%|██████████| 196/196 [00:05<00:00, 33.22batch/s, acc=64.24%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.24% | Remaining weights: 16.65% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 96: 100%|██████████| 196/196 [00:08<00:00, 22.19batch/s, loss=1.1470]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6110 | Reg=0.5360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 96: 100%|██████████| 196/196 [00:05<00:00, 33.32batch/s, acc=81.20%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.20% | Remaining weights: 15.63% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 97: 100%|██████████| 196/196 [00:08<00:00, 22.40batch/s, loss=2.0011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.8599 | Reg=1.1411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 97: 100%|██████████| 196/196 [00:06<00:00, 32.17batch/s, acc=61.23%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 61.23% | Remaining weights: 16.51% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 98: 100%|██████████| 196/196 [00:08<00:00, 22.47batch/s, loss=1.2416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7180 | Reg=0.5236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 98: 100%|██████████| 196/196 [00:06<00:00, 32.65batch/s, acc=80.87%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.87% | Remaining weights: 15.53% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 99: 100%|██████████| 196/196 [00:08<00:00, 22.26batch/s, loss=2.1220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.9857 | Reg=1.1363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 99: 100%|██████████| 196/196 [00:05<00:00, 32.81batch/s, acc=60.95%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.95% | Remaining weights: 16.36% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 100: 100%|██████████| 196/196 [00:04<00:00, 45.89batch/s, loss=0.7177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7177 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 100: 100%|██████████| 196/196 [00:02<00:00, 83.84batch/s, acc=81.53%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.53% | Remaining weights: 14.06% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 101: 100%|██████████| 196/196 [00:03<00:00, 49.05batch/s, loss=0.5238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5238 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 101: 100%|██████████| 196/196 [00:02<00:00, 82.40batch/s, acc=75.52%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.52% | Remaining weights: 13.18% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 102: 100%|██████████| 196/196 [00:04<00:00, 47.67batch/s, loss=0.4518]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4518 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 102: 100%|██████████| 196/196 [00:02<00:00, 83.20batch/s, acc=83.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.50% | Remaining weights: 12.52% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 103: 100%|██████████| 196/196 [00:04<00:00, 47.03batch/s, loss=0.7561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7561 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 103: 100%|██████████| 196/196 [00:02<00:00, 83.30batch/s, acc=79.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.50% | Remaining weights: 12.24% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 104: 100%|██████████| 196/196 [00:04<00:00, 45.80batch/s, loss=0.4369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4369 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 104: 100%|██████████| 196/196 [00:02<00:00, 83.13batch/s, acc=84.48%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.48% | Remaining weights: 11.86% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 105: 100%|██████████| 196/196 [00:04<00:00, 44.51batch/s, loss=0.7852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.7852 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 105: 100%|██████████| 196/196 [00:02<00:00, 84.53batch/s, acc=81.81%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.81% | Remaining weights: 11.70% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 106: 100%|██████████| 196/196 [00:04<00:00, 44.53batch/s, loss=0.5636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5636 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 106: 100%|██████████| 196/196 [00:02<00:00, 82.50batch/s, acc=85.40%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.40% | Remaining weights: 11.43% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 107: 100%|██████████| 196/196 [00:04<00:00, 43.73batch/s, loss=0.4918]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4918 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 107: 100%|██████████| 196/196 [00:02<00:00, 82.50batch/s, acc=82.89%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.89% | Remaining weights: 11.33% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 108: 100%|██████████| 196/196 [00:04<00:00, 45.22batch/s, loss=0.6184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6184 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 108: 100%|██████████| 196/196 [00:02<00:00, 82.82batch/s, acc=86.11%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.11% | Remaining weights: 11.10% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 109: 100%|██████████| 196/196 [00:04<00:00, 46.41batch/s, loss=0.6930]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6930 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 109: 100%|██████████| 196/196 [00:02<00:00, 83.35batch/s, acc=83.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.47% | Remaining weights: 11.04% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 110: 100%|██████████| 196/196 [00:04<00:00, 48.75batch/s, loss=0.3826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3826 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 110: 100%|██████████| 196/196 [00:02<00:00, 84.22batch/s, acc=86.89%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.89% | Remaining weights: 10.85% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 111: 100%|██████████| 196/196 [00:04<00:00, 47.40batch/s, loss=0.3612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3612 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 111: 100%|██████████| 196/196 [00:02<00:00, 83.01batch/s, acc=83.22%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.22% | Remaining weights: 10.80% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 112: 100%|██████████| 196/196 [00:04<00:00, 47.47batch/s, loss=0.4414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4414 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 112: 100%|██████████| 196/196 [00:02<00:00, 84.40batch/s, acc=87.30%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.30% | Remaining weights: 10.64% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 113: 100%|██████████| 196/196 [00:04<00:00, 46.33batch/s, loss=0.5283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5283 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 113: 100%|██████████| 196/196 [00:02<00:00, 82.64batch/s, acc=86.22%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.22% | Remaining weights: 10.61% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 114: 100%|██████████| 196/196 [00:04<00:00, 45.64batch/s, loss=0.4284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4284 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 114: 100%|██████████| 196/196 [00:02<00:00, 83.65batch/s, acc=87.73%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.73% | Remaining weights: 10.47% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 115: 100%|██████████| 196/196 [00:04<00:00, 43.90batch/s, loss=0.6980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6980 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 115: 100%|██████████| 196/196 [00:02<00:00, 84.37batch/s, acc=86.42%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.42% | Remaining weights: 10.44% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 116: 100%|██████████| 196/196 [00:04<00:00, 44.81batch/s, loss=0.4576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4576 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 116: 100%|██████████| 196/196 [00:02<00:00, 82.03batch/s, acc=87.59%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.59% | Remaining weights: 10.31% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 117: 100%|██████████| 196/196 [00:04<00:00, 46.36batch/s, loss=0.5080]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5080 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 117: 100%|██████████| 196/196 [00:02<00:00, 83.06batch/s, acc=87.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.47% | Remaining weights: 10.29% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 118: 100%|██████████| 196/196 [00:04<00:00, 47.18batch/s, loss=0.5208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5208 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 118: 100%|██████████| 196/196 [00:02<00:00, 83.66batch/s, acc=88.15%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.15% | Remaining weights: 10.17% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 119: 100%|██████████| 196/196 [00:04<00:00, 47.41batch/s, loss=0.6247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.6247 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 119: 100%|██████████| 196/196 [00:02<00:00, 83.84batch/s, acc=88.21%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.21% | Remaining weights: 10.16% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 120: 100%|██████████| 196/196 [00:04<00:00, 47.38batch/s, loss=0.5645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.5645 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 120: 100%|██████████| 196/196 [00:02<00:00, 82.19batch/s, acc=88.67%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.67% | Remaining weights: 10.05% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 121: 100%|██████████| 196/196 [00:04<00:00, 45.80batch/s, loss=0.3327]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3327 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 121: 100%|██████████| 196/196 [00:02<00:00, 82.07batch/s, acc=89.05%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.05% | Remaining weights: 10.04% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 122: 100%|██████████| 196/196 [00:04<00:00, 45.52batch/s, loss=0.4151]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4151 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 122: 100%|██████████| 196/196 [00:02<00:00, 82.21batch/s, acc=88.43%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.43% | Remaining weights: 9.94% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 123: 100%|██████████| 196/196 [00:04<00:00, 44.65batch/s, loss=0.3943]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3943 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 123: 100%|██████████| 196/196 [00:02<00:00, 82.59batch/s, acc=89.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.41% | Remaining weights: 9.93% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 124: 100%|██████████| 196/196 [00:04<00:00, 44.47batch/s, loss=0.4102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4102 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 124: 100%|██████████| 196/196 [00:02<00:00, 84.07batch/s, acc=89.13%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.13% | Remaining weights: 9.83% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 125: 100%|██████████| 196/196 [00:04<00:00, 43.88batch/s, loss=0.4357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4357 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 125: 100%|██████████| 196/196 [00:02<00:00, 82.08batch/s, acc=90.10%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.10% | Remaining weights: 9.83% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 126: 100%|██████████| 196/196 [00:04<00:00, 46.06batch/s, loss=0.4119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4119 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 126: 100%|██████████| 196/196 [00:02<00:00, 84.30batch/s, acc=88.79%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.79% | Remaining weights: 9.74% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 127: 100%|██████████| 196/196 [00:04<00:00, 47.68batch/s, loss=0.3381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.3381 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 127: 100%|██████████| 196/196 [00:02<00:00, 82.99batch/s, acc=90.42%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.42% | Remaining weights: 9.73% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 128: 100%|██████████| 196/196 [00:04<00:00, 48.11batch/s, loss=0.4037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4037 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 128: 100%|██████████| 196/196 [00:02<00:00, 83.60batch/s, acc=88.56%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.56% | Remaining weights: 9.65% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train ‖ Epoch 129: 100%|██████████| 196/196 [00:04<00:00, 48.28batch/s, loss=0.4099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=0.4099 | Reg=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val   ‖ Epoch 129: 100%|██████████| 196/196 [00:02<00:00, 82.82batch/s, acc=91.24%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.24% | Remaining weights: 9.64% \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pquant import iterative_train\n",
    "\"\"\"\n",
    "Inputs to train_resnet we defined previously are:\n",
    "          model, trainloader, device, loss_func, epoch, optimizer, scheduler, **kwargs\n",
    "\"\"\"\n",
    "\n",
    "trained_model = iterative_train(model = model, \n",
    "                                config = config, \n",
    "                                train_func = train_resnet, \n",
    "                                valid_func = validate_resnet, \n",
    "                                trainloader = train_loader, \n",
    "                                testloader = val_loader, \n",
    "                                device = device, \n",
    "                                loss_func = loss_function,\n",
    "                                optimizer = optimizer, \n",
    "                                scheduler = scheduler\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd70fed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1104558/1596906618.py:29: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax[0].set_yticklabels(new_ytick)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pquant import remove_pruning_from_model\n",
    "import matplotlib.pyplot as plt\n",
    "# Remove compression layers, leaves Quantized activations in place\n",
    "model = remove_pruning_from_model(trained_model, config)\n",
    "\n",
    "# Plot remaining weights\n",
    "names = []\n",
    "remaining = []\n",
    "total_w = []\n",
    "nonzeros = []\n",
    "for n, m in trained_model.named_modules():\n",
    "    if isinstance(m, (torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Linear)):\n",
    "        names.append(n)\n",
    "        nonzero = np.count_nonzero(m.weight.detach().cpu())\n",
    "        remaining_pct = nonzero / m.weight.numel()\n",
    "        remaining.append(remaining_pct)\n",
    "        total_w.append(m.weight.numel())\n",
    "        nonzeros.append(nonzero)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].bar(range(len(names)), remaining)\n",
    "ax[0].set_xticks(range(len(names)))\n",
    "ax[0].set_xticklabels(names)\n",
    "ax[0].tick_params(axis='x', labelrotation=270)\n",
    "new_ytick = []\n",
    "for i in ax[0].get_yticklabels():\n",
    "    ytick = f\"{float(i.get_text()) * 100:.2f}%\"\n",
    "    new_ytick.append(ytick)\n",
    "ax[0].set_yticklabels(new_ytick)\n",
    "ax[0].title.set_text(\"Remaining weights per layer\")\n",
    "\n",
    "ax[1].bar(range(len(nonzeros)), total_w, color=\"lightcoral\", label=\"total weights\")\n",
    "ax[1].bar(range(len(nonzeros)), nonzeros, color=\"steelblue\", label=\"nonzero weights\")\n",
    "ax[1].set_xticks(range(len(names)))\n",
    "ax[1].set_xticklabels(names)\n",
    "ax[1].tick_params(axis='x', labelrotation=270)\n",
    "ax[1].title.set_text(\"Weights per layer\")\n",
    "ax[1].legend()\n",
    "ax[1].set_yscale(\"log\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae1039e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not find a PACA-pruned layer named 'layer1.0.conv1'.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def visualize_conv_patterns(model, layer_name, max_patterns_to_show=16):\n",
    "    \"\"\"\n",
    "    Visualizes the dominant binary patterns for a specific convolutional layer.\n",
    "    \"\"\"\n",
    "    target_module = None\n",
    "    for name, module in model.named_modules():\n",
    "        if name == layer_name and hasattr(module, \"pruning_layer\"):\n",
    "            pruning_layer = module.pruning_layer\n",
    "            if hasattr(pruning_layer, \"metric_fn\") and \"PACAPattern\" in pruning_layer.metric_fn.__class__.__name__:\n",
    "                target_module = module\n",
    "                break\n",
    "    \n",
    "    if target_module is None:\n",
    "        print(f\"Error: Could not find a PACA-pruned layer named '{layer_name}'.\")\n",
    "        return\n",
    "        \n",
    "    metric_fn = target_module.pruning_layer.metric_fn\n",
    "    # Ensure dominant patterns are selected based on the final weights\n",
    "    metric_fn._select_dominant_patterns(target_module.weight)\n",
    "    \n",
    "    patterns = metric_fn.dominant_patterns\n",
    "    if patterns is None or patterns.shape[0] == 0:\n",
    "        print(f\"No dominant patterns found for layer '{layer_name}'.\")\n",
    "        return\n",
    "        \n",
    "    # Convert to NumPy for plotting\n",
    "    patterns_np = keras.ops.convert_to_numpy(patterns)\n",
    "    \n",
    "    # Get original kernel shape from the weight tensor\n",
    "    kernel_h, kernel_w = target_module.weight.shape[2], target_module.weight.shape[3]\n",
    "    \n",
    "    num_patterns = min(patterns_np.shape[0], max_patterns_to_show)\n",
    "    \n",
    "    # Create a subplot grid for the patterns\n",
    "    cols = math.ceil(math.sqrt(num_patterns))\n",
    "    rows = math.ceil(num_patterns / cols)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n",
    "    if isinstance(axes, plt.Axes):\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    \n",
    "    fig.suptitle(f\"Dominant Patterns for Layer: {layer_name} (found {patterns_np.shape[0]})\", fontsize=16)\n",
    "    \n",
    "    for i in range(num_patterns):\n",
    "        pattern_2d = patterns_np[i].reshape(kernel_h, kernel_w)\n",
    "        print(pattern_2d.shape)\n",
    "        print(pattern_2d)\n",
    "        axes[i].imshow(pattern_2d, cmap='binary', vmin=0, vmax=1)\n",
    "        axes[i].set_title(f\"Pattern {i+1}\")\n",
    "        axes[i].set_xticks([])\n",
    "        axes[i].set_yticks([])\n",
    "        \n",
    "    # Hide any unused subplots\n",
    "    for j in range(num_patterns, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "        \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# --- How to use it in your notebook after training is finished ---\n",
    "visualize_conv_patterns(model, 'layer1.0.conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251515c3-00ac-4110-b8d8-a8c9100f6e6b",
   "metadata": {},
   "source": [
    "## Add PACA prunning\n",
    "#### After pruning we will have multiple patterns, so we force all of them to have a lower num,ber of dominant patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5898e23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "batch_size": 128,
       "cosine_tmax": 200,
       "gamma": 0.1,
       "l2_decay": 0.0001,
       "label_smoothing": 0,
       "lr": 0.001,
       "lr_schedule": "multistep",
       "milestones": [
        75,
        120
       ],
       "momentum": 0.9,
       "optimizer": "sgd",
       "plot_frequency": 100,
       "pruning_parameters": {
        "beta": 0.75,
        "damping": 1,
        "disable_pruning_for_layers": [
         null
        ],
        "distance_metric": "valued_hamming",
        "enable_pruning": true,
        "epsilon": 0.001,
        "metric_type": "PACAPatternSparsity",
        "num_patterns_to_keep": 16,
        "pruning_method": "mdmm",
        "scale": 50,
        "use_grad": false
       },
       "quantization_parameters": {
        "default_fractional_bits": 7,
        "default_integer_bits": 0,
        "enable_quantization": false,
        "hgq_gamma": 0.0003,
        "hgq_heterogeneous": true,
        "layer_specific": [],
        "use_high_granularity_quantization": false,
        "use_real_tanh": false,
        "use_symmetric_quantization": false
       },
       "training_parameters": {
        "epochs": 200,
        "fine_tuning_epochs": 30,
        "pretraining_epochs": 0,
        "pruning_first": false,
        "rewind": "never",
        "rounds": 1,
        "save_weights_epoch": -1
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml \n",
    "\n",
    "with open(\"pquant/configs/config_mdmm_paca.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "JSON(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pquant                       # make sure the top-level module is in scope\n",
    "\n",
    "importlib.reload(pquant)            # re-executes pquant/__init__.py and submodules it imports\n",
    "\n",
    "# Re-import the symbol(s) you need, or access them via the namespace\n",
    "from pquant import add_compression_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e857a26e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper_CUDA__native_batch_norm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_compression_layers\n\u001b[1;32m      2\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43madd_compression_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/compressed_layers.py:25\u001b[0m, in \u001b[0;36madd_compression_layers\u001b[0;34m(model, config, input_shape)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mbackend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_impl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressed_layers_torch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m         add_compression_layers_torch,\n\u001b[1;32m     23\u001b[0m     )\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43madd_compression_layers_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_impl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressed_layers_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_compression_layers_tf\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/compressed_layers_torch.py:215\u001b[0m, in \u001b[0;36madd_compression_layers_torch\u001b[0;34m(model, config, input_shape)\u001b[0m\n\u001b[1;32m    213\u001b[0m model \u001b[38;5;241m=\u001b[39m disable_pruning_from_layers(model, config)\n\u001b[1;32m    214\u001b[0m model \u001b[38;5;241m=\u001b[39m add_layer_specific_quantization_to_model(model, config)\n\u001b[0;32m--> 215\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torchvision/models/resnet.py:269\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m--> 269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m    271\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    186\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/functional.py:2822\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2820\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2823\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2830\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper_CUDA__native_batch_norm)"
     ]
    }
   ],
   "source": [
    "from pquant import add_compression_layers\n",
    "input_shape = (256,3,32,32)\n",
    "model = add_compression_layers(model, config, input_shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c24bff-9937-4670-8cff-022ddc7a0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(model, trainloader, device, loss_func, epoch, optimizer, scheduler, *args, **kwargs):\n",
    "    first_batch = True\n",
    "    for data in trainloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.)) # 8 bits input quantization\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        losses = get_model_losses(model, torch.tensor(0.).to(device))\n",
    "        loss += losses\n",
    "        loss.backward()\n",
    "\n",
    "        if first_batch:\n",
    "            print(\"\\n ---- Checking Loss values ----\")\n",
    "            print(\"Loss:\", loss_func(outputs, labels).item())\n",
    "            print(\"Model Losses:\", losses.item())\n",
    "            print(\"--------------------------------------------------\")\n",
    "\n",
    "            for name, module in model.named_modules():\n",
    "                if \"conv1\" in name and hasattr(module, \"pruning_layer\"):\n",
    "                    pruning_layer = module.pruning_layer\n",
    "                    if hasattr(pruning_layer, \"metric_fn\") and \"PACAPattern\" in pruning_layer.metric_fn.__class__.__name__:\n",
    "                        metric_fn = pruning_layer.metric_fn\n",
    "                        num_patterns = 0\n",
    "                        if metric_fn.dominant_patterns is not None:\n",
    "                            num_patterns = metric_fn.dominant_patterns.shape[0]\n",
    "\n",
    "                        total_dist = metric_fn(module.weight).item()\n",
    "                        num_kernels = module.weight.shape[0]\n",
    "                        avg_dist = total_dist / num_kernels if num_kernels > 0 else 0\n",
    "\n",
    "                        print(f\"--- PACA Stats for {name} at Epoch {epoch} ---\")\n",
    "                        print(f\"Num Patterns: {num_patterns}, Avg Pattern Dist: {avg_dist:.4f}\")\n",
    "                        print(\"--------------------------------------------------\\n\")\n",
    "                        break\n",
    "            first_batch = False\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch += 1\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "def validate_resnet(model, testloader, device, loss_func, epoch, *args, **kwargs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_paca_patterns = 0\n",
    "    avg_paca_dist = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.))\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for name, module in model.named_modules():\n",
    "            if \"conv1\" in name and hasattr(module, \"pruning_layer\"):\n",
    "                pruning_layer = module.pruning_layer\n",
    "                if hasattr(pruning_layer, \"metric_fn\") and \"PACAPattern\" in pruning_layer.metric_fn.__class__.__name__:\n",
    "                    metric_fn = pruning_layer.metric_fn\n",
    "                    if metric_fn.dominant_patterns is not None:\n",
    "                        num_paca_patterns = metric_fn.dominant_patterns.shape[0]\n",
    "\n",
    "                    total_dist = metric_fn(module.weight).item()\n",
    "                    num_kernels = module.weight.shape[0]\n",
    "                    avg_paca_dist = total_dist / num_kernels if num_kernels > 0 else 0\n",
    "                    break\n",
    "\n",
    "        ratio = get_layer_keep_ratio(model)\n",
    "        print(f'Accuracy: {100 * correct / total:.2f}%, '\n",
    "              f'Remaining Weights: {ratio * 100:.2f}%, '\n",
    "              f'Num Patterns: {num_paca_patterns}, '\n",
    "              f'Avg Pattern Dist: {avg_paca_dist:.4f}')\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader, val_loader = get_cifar10_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28865b-afdb-4773-be30-717486d9786a",
   "metadata": {},
   "source": [
    "## Create loss function, scheduler and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88af88-ef7a-4d30-8cff-f39225d5a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850c23a-2abc-4904-9c69-859492b450a8",
   "metadata": {},
   "source": [
    "## Train model\n",
    "Training time. We use the train_compressed_model function from pquant to train. We need to provide some parameters such as training and validation functions, their input parameters, the model and the config file. The function automatically adds pruning layers and replaces activations with a quantized variant, trains the model, and removes the pruning layers after training is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2414c-1f4d-4a30-a143-920497e60ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 1.023564100265503\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 0 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 70.08%, Remaining Weights: 96.59%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.9040290713310242\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 197 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 63.14%, Remaining Weights: 96.59%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.9618150591850281\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 394 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 73.10%, Remaining Weights: 96.58%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.9226782321929932\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 591 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 68.42%, Remaining Weights: 96.58%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.8731341361999512\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 788 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 74.76%, Remaining Weights: 96.57%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.8373536467552185\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 985 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 70.21%, Remaining Weights: 96.57%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.934232771396637\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1182 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 76.75%, Remaining Weights: 96.57%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.7584898471832275\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1379 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 72.12%, Remaining Weights: 96.57%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.8126096129417419\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1576 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 77.97%, Remaining Weights: 96.56%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.6571393609046936\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1773 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 75.92%, Remaining Weights: 96.56%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.7207622528076172\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 1970 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 79.20%, Remaining Weights: 96.56%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.7187369465827942\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 2167 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 75.83%, Remaining Weights: 96.56%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.8391613960266113\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 2364 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 80.06%, Remaining Weights: 96.56%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.7094944715499878\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 2561 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 76.99%, Remaining Weights: 96.55%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.7293562293052673\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 2758 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 80.78%, Remaining Weights: 96.55%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.6537322998046875\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 2955 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 79.24%, Remaining Weights: 96.55%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.5786072015762329\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 3152 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 81.20%, Remaining Weights: 96.54%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.6281647086143494\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 3349 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 80.78%, Remaining Weights: 96.54%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.6206632852554321\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 3546 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 82.38%, Remaining Weights: 96.54%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.5568910241127014\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 3743 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 82.85%, Remaining Weights: 96.54%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.5789287090301514\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 3940 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 82.67%, Remaining Weights: 96.53%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.46712976694107056\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 4137 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 82.77%, Remaining Weights: 96.53%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.47319871187210083\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 4334 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "Accuracy: 83.30%, Remaining Weights: 96.53%, Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "\n",
      " ---- Checking Loss values ----\n",
      "Loss: 0.49258410930633545\n",
      "Model Losses: 0.0\n",
      "--------------------------------------------------\n",
      "--- PACA Stats for conv1 at Epoch 4531 ---\n",
      "Num Patterns: 1, Avg Pattern Dist: 0.0000\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iterative_train\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mInputs to train_resnet we defined previously are:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m          model, trainloader, device, loss_func, epoch, optimizer, scheduler, **kwargs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43miterative_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_resnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvalid_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidate_resnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/train.py:8\u001b[0m, in \u001b[0;36miterative_train\u001b[0;34m(model, config, train_func, valid_func, **kwargs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mbackend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_impl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_torch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iterative_train_torch\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterative_train_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpquant\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_impl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iterative_train_tf\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/train_torch.py:37\u001b[0m, in \u001b[0;36miterative_train_torch\u001b[0;34m(model, config, train_func, valid_func, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m train_func(model, epoch\u001b[38;5;241m=\u001b[39mepoch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 37\u001b[0m \u001b[43mvalid_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m post_epoch_functions(model, e, training_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     39\u001b[0m epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 82\u001b[0m, in \u001b[0;36mvalidate_resnet\u001b[0;34m(model, testloader, device, loss_func, epoch, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     81\u001b[0m inputs \u001b[38;5;241m=\u001b[39m quantizer(inputs, k\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.\u001b[39m), i\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m), f\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m7.\u001b[39m))\n\u001b[0;32m---> 82\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     84\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/compressed_layers_torch.py:161\u001b[0m, in \u001b[0;36mCompressedLayerConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 161\u001b[0m     weight, bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune_and_quantize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpruning_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwanda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpruning_layer\u001b[38;5;241m.\u001b[39mcollect_input(x, weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/compressed_layers_torch.py:119\u001b[0m, in \u001b[0;36mCompressedLayerBase.prune_and_quantize\u001b[0;34m(self, weight, bias)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     weight, bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantize(weight, bias)\n\u001b[0;32m--> 119\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weight, bias\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/core/torch_impl/compressed_layers_torch.py:110\u001b[0m, in \u001b[0;36mCompressedLayerBase.prune\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprune\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_pruning:\n\u001b[0;32m--> 110\u001b[0m         weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpruning_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weight\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/layers/layer.py:936\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/backend/torch/layer.py:41\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/ops/operation.py:58\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     54\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     55\u001b[0m         call_fn,\n\u001b[1;32m     56\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     57\u001b[0m     )\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:513\u001b[0m, in \u001b[0;36mMDMM.call\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    511\u001b[0m         weight \u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_hard_mask(weight)\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstraint_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weight\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/layers/layer.py:936\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    940\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/backend/torch/layer.py:41\u001b[0m, in \u001b[0;36mTorchLayer.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/ops/operation.py:58\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     54\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     55\u001b[0m         call_fn,\n\u001b[1;32m     56\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     57\u001b[0m     )\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:61\u001b[0m, in \u001b[0;36mConstraint.call\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight):\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculates the penalty from a given infeasibility measure.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     raw_infeasibility \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_infeasibility\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     infeasibility \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe_infeasibility(raw_infeasibility)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_grad_:\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:108\u001b[0m, in \u001b[0;36mEqualityConstraint.get_infeasibility\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_infeasibility\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight):\n\u001b[0;32m--> 108\u001b[0m     metric_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     infeasibility \u001b[38;5;241m=\u001b[39m metric_value \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_value\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# return ops.abs(infeasibility)\u001b[39;00m\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:409\u001b[0m, in \u001b[0;36mPACAPatternMetric.__call__\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_dominant_patterns(weight)\n\u001b[0;32m--> 409\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pattern_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m min_distances \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mmin(distances, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39msum(min_distances)\n",
      "File \u001b[0;32m~/PQuant/mdmm_dev/src/pquant/pruning_methods/mdmm.py:386\u001b[0m, in \u001b[0;36mPACAPatternMetric._pattern_distances\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m    384\u001b[0m     distances \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39msum(ops\u001b[38;5;241m.\u001b[39mabs(dom_patterns_exp \u001b[38;5;241m-\u001b[39m w_patterns_exp), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalued_hamming\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 386\u001b[0m     abs_diff \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdom_patterns_exp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw_patterns_exp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m     distances \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39msum(abs_diff \u001b[38;5;241m*\u001b[39m ops\u001b[38;5;241m.\u001b[39mabs(w_kernels_exp), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_metric \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/ops/numpy.py:185\u001b[0m, in \u001b[0;36mabs\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.ops.abs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.ops.numpy.abs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mabs\u001b[39m(x):\n\u001b[1;32m    184\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Shorthand for `keras.ops.absolute`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mabsolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/ops/numpy.py:175\u001b[0m, in \u001b[0;36mabsolute\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Absolute()\u001b[38;5;241m.\u001b[39msymbolic_call(x)\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabsolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/depot/cms/conda_envs/das214/pquant-gpu-env/lib/python3.10/site-packages/keras/src/backend/torch/numpy.py:249\u001b[0m, in \u001b[0;36mabsolute\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m standardize_dtype(x\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pquant import iterative_train\n",
    "\"\"\"\n",
    "Inputs to train_resnet we defined previously are:\n",
    "          model, trainloader, device, loss_func, epoch, optimizer, scheduler, **kwargs\n",
    "\"\"\"\n",
    "\n",
    "trained_model = iterative_train(model = model, \n",
    "                                config = config, \n",
    "                                train_func = train_resnet, \n",
    "                                valid_func = validate_resnet, \n",
    "                                trainloader = train_loader, \n",
    "                                testloader = val_loader, \n",
    "                                device = device, \n",
    "                                loss_func = loss_function,\n",
    "                                optimizer = optimizer, \n",
    "                                scheduler = scheduler\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cdc72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_conv_patterns(model, 'layer1.0.conv1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pquant-gpu-env]",
   "language": "python",
   "name": "conda-env-pquant-gpu-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
